link,pdf_link,title,authors,abstract,submitted
https://arxiv.org/abs/2504.17203,https://arxiv.org/pdf/2504.17203,High-Fidelity And Complex TestDataGeneration For Real-World SQL Code Generation Services,"Shivasankari Kannan, Yeounoh Chung, Amita Gondi, Tristan Swadell, Fatma Ozcan","The demand for high-fidelity test data is paramount in industrial settings where access to production data is largely restricted. Traditional data generation methods often fall short, struggling with low-fidelity and the ability to model complex data structures and semantic relationships that are critical for testing complex SQL code generation services like Natural Language to SQL (NL2SQL). In this paper, we address the critical need for generating syntactically correct and semantically ``meaningful'' mock data for complex schema that includes columns with nested structures that we frequently encounter in Google SQL code generation workloads. We highlight the limitations of existing approaches used in production, particularly their inability to handle large and complex schema, as well as the lack of semantically coherent test data that lead to limited test coverage. We demonstrate that by leveraging LargeLanguageModels (LLMs) and incorporating strategic pre- and post-processing steps, we can generate realistic high-fidelity test data that adheres to complex structural constraints and maintains semantic integrity to the test targets (SQL queries/functions). This approach supports comprehensive testing of complex SQL queries involving joins, aggregations, and even deeply nested subqueries, ensuring robust evaluation of SQL code generation services, like NL2SQL and SQL Code Assistant services. Our results demonstrate the practical utility of an out-of-the-box LLM (\textit{gemini}) based testdatageneration for industrial SQL code generation services where generating realistic test data is essential due to the frequent unavailability of production datasets.","Submitted 23 April, 2025; originally announced April 2025."
https://arxiv.org/abs/2410.16197,https://arxiv.org/pdf/2410.16197,LASER: Script Execution by Autonomous Agents for On-demand Traffic Simulation,"Hao Gao, Jingyue Wang, Wenyang Fang, Jingwei Xu, Yunpeng Huang, Taolue Chen, Xiaoxing Ma","Autonomous Driving Systems (ADS) require diverse and safety-critical traffic scenarios for effective training and testing, but the existing data generation methods struggle to provide flexibility and scalability. We propose LASER, a novel frame-work that leverage largelanguagemodels (LLMs) to conduct traffic simulations based on natural language inputs. The framework operates in two stages: it first generates scripts from user-provided descriptions and then executes them using autonomous agents in real time. Validated in the CARLA simulator, LASER successfully generates complex, on-demand driving scenarios, significantly improving ADS training and testingdatageneration.","Submitted 24 October, 2024; v1 submitted 21 October, 2024; originally announced October 2024."
https://arxiv.org/abs/2401.17626,https://arxiv.org/pdf/2401.17626,Generative AI to Generate TestDataGenerators,"Benoit Baudry, Khashayar Etemadi, Sen Fang, Yogya Gamage, Yi Liu, Yuxin Liu, Martin Monperrus, Javier Ron, Andr√© Silva, Deepika Tiwari","Generating fake data is an essential dimension of modern software testing, as demonstrated by the number and significance of data faking libraries. Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains. In this paper, we assess the ability of generative AI for generating test data in different domains. We design three types of prompts for LargeLanguageModels (LLMs), which perform testdatageneration tasks at different levels of integrability: 1) raw testdatageneration, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries. We evaluate our approach by prompting LLMs to generate test data for 11 domains. The results show that LLMs can successfully generate realistic testdatagenerators in a wide range of domains at all three levels of integrability.","Submitted 14 June, 2024; v1 submitted 31 January, 2024; originally announced January 2024."
