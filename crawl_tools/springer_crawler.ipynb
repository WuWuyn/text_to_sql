{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd52ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DrissionPage import ChromiumPage, ChromiumOptions\n",
    "import time\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59deb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_link(keyword: str, page: ChromiumPage):\n",
    "    try:\n",
    "        # URL tìm kiếm ban đầu\n",
    "        search_url = f'https://link.springer.com/search?new-search=true&query={keyword}'\n",
    "        page.get(search_url)\n",
    "\n",
    "        # Wait for the page to load\n",
    "        time.sleep(2)\n",
    "\n",
    "        links = []\n",
    "        cnt = 1\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # Wait for the page to load\n",
    "                time.sleep(5)\n",
    "\n",
    "                # Tìm tất cả các thẻ <a> chứa liên kết bài báo\n",
    "                link_elements = page.eles(\"css:li.app-card-open a.app-card-open__link\")\n",
    "                # Trích xuất và xây dựng danh sách các URL tuyệt đối\n",
    "                for link in link_elements:\n",
    "                    href = link.attr('href')\n",
    "                    if href.startswith('/'):\n",
    "                        full_url = \"https://link.springer.com\" + href\n",
    "                    else:\n",
    "                        full_url = href  # Trường hợp hiếm khi href đã là URL tuyệt đối\n",
    "                    links.append(full_url)\n",
    "\n",
    "                # Check for next button\n",
    "                next_button = page.ele(\"css:a.eds-c-pagination__link[rel='next']\")\n",
    "                if not next_button:\n",
    "                    print(f\"Reached last page for keyword: {keyword}\")\n",
    "                    break\n",
    "                \n",
    "                next_button.click()\n",
    "                cnt += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {cnt}: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching for keyword {keyword}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dea1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def combination_keywords(sets):\n",
    "    \"\"\"Tạo danh sách các chuỗi từ khóa từ một danh sách các bộ từ khóa.\"\"\"\n",
    "    if not sets:\n",
    "        return []\n",
    "    combinations = itertools.product(*sets)\n",
    "    return [' AND '.join(combo) for combo in combinations]\n",
    "\n",
    "def generate_all_combinations(t2sql, security, llm):\n",
    "    \"\"\"Tạo danh sách tất cả các tổ hợp từ khóa theo các trường hợp yêu cầu.\"\"\"\n",
    "    # Định nghĩa các trường hợp cần tạo tổ hợp\n",
    "    cases = [\n",
    "        [t2sql],                    # Chỉ t2sql\n",
    "        [t2sql, security],          # t2sql + security\n",
    "        [t2sql, llm],               # t2sql + llm\n",
    "        [t2sql, security, llm]      # t2sql + security + llm\n",
    "    ]\n",
    "    \n",
    "    # Tạo và hợp nhất tất cả các tổ hợp\n",
    "    all_combinations = []\n",
    "    for case in cases:\n",
    "        all_combinations.extend(combination_keywords(case))\n",
    "    \n",
    "    return all_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9131ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_springer():\n",
    "    # Initialize empty lists to store all results\n",
    "    all_links = []\n",
    "\n",
    "    # option = ChromiumOptions()\n",
    "    # # Uncomment the following lines to run in headless mode or with specific options\n",
    "    # option.headless(on_off=True)  # Run in headless mode\n",
    "    # # Initialize Chromium browser\n",
    "    option = ChromiumOptions()\n",
    "    browser = ChromiumPage()\n",
    "\n",
    "    #Thêm ngoặc chính xác bộ keywords\n",
    "    t2sql = ['\"text-to-sql\"', '\"nl2sql\"', '\"t2sql\"', '\"text2sql\"', '\"natural language to sql\"', \n",
    "             '\"semantic parsing to sql\"', '\"nl to sql\"']\n",
    "    security = ['\"security\"', '\"access control\"', '\"injection\"', '\"prompt injection\"', \n",
    "                '\"defense\"', '\"attack\"', '\"vulnerability\"']\n",
    "    llm = ['\"llm\"', '\"large language model\"']\n",
    "\n",
    "    keywords = generate_all_combinations(t2sql, security, llm)\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        \n",
    "        print(f'{keyword} is proccessed......')\n",
    "\n",
    "        keyword_1 = keyword.strip()\n",
    "        keyword_1 = keyword_1.replace(\" \", \"+\")\n",
    "        links = crawl_link(keyword_1, browser)\n",
    "\n",
    "        # Create DataFrame after collecting all data\n",
    "        partly = pd.DataFrame()\n",
    "        partly['link'] = links\n",
    "\n",
    "        if not partly.empty:\n",
    "            partly.drop_duplicates(subset=['link'], inplace=True)\n",
    "            keyword = keyword.replace('\"', '')\n",
    "            keyword = keyword.replace(' ', '_')\n",
    "            partly.to_csv(f'springer/crawl_by_{keyword}.csv', index=False)\n",
    "\n",
    "        all_links.extend(links)\n",
    "\n",
    "    full = pd.DataFrame()\n",
    "    \n",
    "    full['link'] = all_links\n",
    "    \n",
    "    full.drop_duplicates(subset=['link'], inplace=True)\n",
    "    full.to_csv('springer/all_springer_papers.csv', index=False)\n",
    "\n",
    "    # Close the browser\n",
    "    browser.quit()\n",
    "    print(f\"Total papers collected: {len(full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fb8392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"text-to-sql\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"\n",
      "\"nl2sql\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"\n",
      "\"t2sql\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"\n",
      "\"text2sql\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"\n",
      "\"natural language to sql\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"\n",
      "\"semantic parsing to sql\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"\n",
      "\"nl to sql\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"\n",
      "\"text-to-sql\" AND \"security\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"security\"\n",
      "\"text-to-sql\" AND \"access control\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"access+control\"\n",
      "\"text-to-sql\" AND \"injection\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"injection\"\n",
      "\"text-to-sql\" AND \"prompt injection\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"prompt+injection\"\n",
      "\"text-to-sql\" AND \"defense\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"defense\"\n",
      "\"text-to-sql\" AND \"attack\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"attack\"\n",
      "\"text-to-sql\" AND \"vulnerability\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"vulnerability\"\n",
      "\"nl2sql\" AND \"security\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"security\"\n",
      "\"nl2sql\" AND \"access control\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"access+control\"\n",
      "\"nl2sql\" AND \"injection\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"injection\"\n",
      "\"nl2sql\" AND \"prompt injection\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"prompt+injection\"\n",
      "\"nl2sql\" AND \"defense\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"defense\"\n",
      "\"nl2sql\" AND \"attack\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"attack\"\n",
      "\"nl2sql\" AND \"vulnerability\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"vulnerability\"\n",
      "\"t2sql\" AND \"security\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"security\"\n",
      "\"t2sql\" AND \"access control\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"access+control\"\n",
      "\"t2sql\" AND \"injection\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"injection\"\n",
      "\"t2sql\" AND \"prompt injection\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"prompt+injection\"\n",
      "\"t2sql\" AND \"defense\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"defense\"\n",
      "\"t2sql\" AND \"attack\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"attack\"\n",
      "\"t2sql\" AND \"vulnerability\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"vulnerability\"\n",
      "\"text2sql\" AND \"security\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"security\"\n",
      "\"text2sql\" AND \"access control\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"access+control\"\n",
      "\"text2sql\" AND \"injection\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"injection\"\n",
      "\"text2sql\" AND \"prompt injection\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"prompt+injection\"\n",
      "\"text2sql\" AND \"defense\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"defense\"\n",
      "\"text2sql\" AND \"attack\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"attack\"\n",
      "\"text2sql\" AND \"vulnerability\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"vulnerability\"\n",
      "\"natural language to sql\" AND \"security\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"security\"\n",
      "\"natural language to sql\" AND \"access control\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"access+control\"\n",
      "\"natural language to sql\" AND \"injection\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"injection\"\n",
      "\"natural language to sql\" AND \"prompt injection\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"prompt+injection\"\n",
      "\"natural language to sql\" AND \"defense\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"defense\"\n",
      "\"natural language to sql\" AND \"attack\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"attack\"\n",
      "\"natural language to sql\" AND \"vulnerability\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"vulnerability\"\n",
      "\"semantic parsing to sql\" AND \"security\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"security\"\n",
      "\"semantic parsing to sql\" AND \"access control\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"access+control\"\n",
      "\"semantic parsing to sql\" AND \"injection\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"injection\"\n",
      "\"semantic parsing to sql\" AND \"prompt injection\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"prompt+injection\"\n",
      "\"semantic parsing to sql\" AND \"defense\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"defense\"\n",
      "\"semantic parsing to sql\" AND \"attack\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"attack\"\n",
      "\"semantic parsing to sql\" AND \"vulnerability\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"vulnerability\"\n",
      "\"nl to sql\" AND \"security\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"security\"\n",
      "\"nl to sql\" AND \"access control\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"access+control\"\n",
      "\"nl to sql\" AND \"injection\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"injection\"\n",
      "\"nl to sql\" AND \"prompt injection\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"prompt+injection\"\n",
      "\"nl to sql\" AND \"defense\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"defense\"\n",
      "\"nl to sql\" AND \"attack\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"attack\"\n",
      "\"nl to sql\" AND \"vulnerability\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"vulnerability\"\n",
      "\"text-to-sql\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"llm\"\n",
      "\"text-to-sql\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"large+language+model\"\n",
      "\"nl2sql\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"llm\"\n",
      "\"nl2sql\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"large+language+model\"\n",
      "\"t2sql\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"llm\"\n",
      "\"t2sql\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"large+language+model\"\n",
      "\"text2sql\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"llm\"\n",
      "\"text2sql\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"large+language+model\"\n",
      "\"natural language to sql\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"llm\"\n",
      "\"natural language to sql\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"large+language+model\"\n",
      "\"semantic parsing to sql\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"llm\"\n",
      "\"semantic parsing to sql\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"large+language+model\"\n",
      "\"nl to sql\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"llm\"\n",
      "\"nl to sql\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"large+language+model\"\n",
      "\"text-to-sql\" AND \"security\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"security\"+AND+\"llm\"\n",
      "\"text-to-sql\" AND \"security\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"security\"+AND+\"large+language+model\"\n",
      "\"text-to-sql\" AND \"access control\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"access+control\"+AND+\"llm\"\n",
      "\"text-to-sql\" AND \"access control\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"access+control\"+AND+\"large+language+model\"\n",
      "\"text-to-sql\" AND \"injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"injection\"+AND+\"llm\"\n",
      "\"text-to-sql\" AND \"injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"injection\"+AND+\"large+language+model\"\n",
      "\"text-to-sql\" AND \"prompt injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"prompt+injection\"+AND+\"llm\"\n",
      "\"text-to-sql\" AND \"prompt injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"prompt+injection\"+AND+\"large+language+model\"\n",
      "\"text-to-sql\" AND \"defense\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"defense\"+AND+\"llm\"\n",
      "\"text-to-sql\" AND \"defense\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"defense\"+AND+\"large+language+model\"\n",
      "\"text-to-sql\" AND \"attack\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"attack\"+AND+\"llm\"\n",
      "\"text-to-sql\" AND \"attack\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"attack\"+AND+\"large+language+model\"\n",
      "\"text-to-sql\" AND \"vulnerability\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"vulnerability\"+AND+\"llm\"\n",
      "\"text-to-sql\" AND \"vulnerability\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text-to-sql\"+AND+\"vulnerability\"+AND+\"large+language+model\"\n",
      "\"nl2sql\" AND \"security\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"security\"+AND+\"llm\"\n",
      "\"nl2sql\" AND \"security\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"security\"+AND+\"large+language+model\"\n",
      "\"nl2sql\" AND \"access control\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"access+control\"+AND+\"llm\"\n",
      "\"nl2sql\" AND \"access control\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"access+control\"+AND+\"large+language+model\"\n",
      "\"nl2sql\" AND \"injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"injection\"+AND+\"llm\"\n",
      "\"nl2sql\" AND \"injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"injection\"+AND+\"large+language+model\"\n",
      "\"nl2sql\" AND \"prompt injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"prompt+injection\"+AND+\"llm\"\n",
      "\"nl2sql\" AND \"prompt injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"prompt+injection\"+AND+\"large+language+model\"\n",
      "\"nl2sql\" AND \"defense\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"defense\"+AND+\"llm\"\n",
      "\"nl2sql\" AND \"defense\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"defense\"+AND+\"large+language+model\"\n",
      "\"nl2sql\" AND \"attack\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"attack\"+AND+\"llm\"\n",
      "\"nl2sql\" AND \"attack\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"attack\"+AND+\"large+language+model\"\n",
      "\"nl2sql\" AND \"vulnerability\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"vulnerability\"+AND+\"llm\"\n",
      "\"nl2sql\" AND \"vulnerability\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl2sql\"+AND+\"vulnerability\"+AND+\"large+language+model\"\n",
      "\"t2sql\" AND \"security\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"security\"+AND+\"llm\"\n",
      "\"t2sql\" AND \"security\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"security\"+AND+\"large+language+model\"\n",
      "\"t2sql\" AND \"access control\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"access+control\"+AND+\"llm\"\n",
      "\"t2sql\" AND \"access control\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"access+control\"+AND+\"large+language+model\"\n",
      "\"t2sql\" AND \"injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"injection\"+AND+\"llm\"\n",
      "\"t2sql\" AND \"injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"injection\"+AND+\"large+language+model\"\n",
      "\"t2sql\" AND \"prompt injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"prompt+injection\"+AND+\"llm\"\n",
      "\"t2sql\" AND \"prompt injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"prompt+injection\"+AND+\"large+language+model\"\n",
      "\"t2sql\" AND \"defense\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"defense\"+AND+\"llm\"\n",
      "\"t2sql\" AND \"defense\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"defense\"+AND+\"large+language+model\"\n",
      "\"t2sql\" AND \"attack\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"attack\"+AND+\"llm\"\n",
      "\"t2sql\" AND \"attack\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"attack\"+AND+\"large+language+model\"\n",
      "\"t2sql\" AND \"vulnerability\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"vulnerability\"+AND+\"llm\"\n",
      "\"t2sql\" AND \"vulnerability\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"t2sql\"+AND+\"vulnerability\"+AND+\"large+language+model\"\n",
      "\"text2sql\" AND \"security\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"security\"+AND+\"llm\"\n",
      "\"text2sql\" AND \"security\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"security\"+AND+\"large+language+model\"\n",
      "\"text2sql\" AND \"access control\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"access+control\"+AND+\"llm\"\n",
      "\"text2sql\" AND \"access control\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"access+control\"+AND+\"large+language+model\"\n",
      "\"text2sql\" AND \"injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"injection\"+AND+\"llm\"\n",
      "\"text2sql\" AND \"injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"injection\"+AND+\"large+language+model\"\n",
      "\"text2sql\" AND \"prompt injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"prompt+injection\"+AND+\"llm\"\n",
      "\"text2sql\" AND \"prompt injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"prompt+injection\"+AND+\"large+language+model\"\n",
      "\"text2sql\" AND \"defense\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"defense\"+AND+\"llm\"\n",
      "\"text2sql\" AND \"defense\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"defense\"+AND+\"large+language+model\"\n",
      "\"text2sql\" AND \"attack\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"attack\"+AND+\"llm\"\n",
      "\"text2sql\" AND \"attack\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"attack\"+AND+\"large+language+model\"\n",
      "\"text2sql\" AND \"vulnerability\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"vulnerability\"+AND+\"llm\"\n",
      "\"text2sql\" AND \"vulnerability\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"text2sql\"+AND+\"vulnerability\"+AND+\"large+language+model\"\n",
      "\"natural language to sql\" AND \"security\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"security\"+AND+\"llm\"\n",
      "\"natural language to sql\" AND \"security\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"security\"+AND+\"large+language+model\"\n",
      "\"natural language to sql\" AND \"access control\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"access+control\"+AND+\"llm\"\n",
      "\"natural language to sql\" AND \"access control\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"access+control\"+AND+\"large+language+model\"\n",
      "\"natural language to sql\" AND \"injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"injection\"+AND+\"llm\"\n",
      "\"natural language to sql\" AND \"injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"injection\"+AND+\"large+language+model\"\n",
      "\"natural language to sql\" AND \"prompt injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"prompt+injection\"+AND+\"llm\"\n",
      "\"natural language to sql\" AND \"prompt injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"prompt+injection\"+AND+\"large+language+model\"\n",
      "\"natural language to sql\" AND \"defense\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"defense\"+AND+\"llm\"\n",
      "\"natural language to sql\" AND \"defense\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"defense\"+AND+\"large+language+model\"\n",
      "\"natural language to sql\" AND \"attack\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"attack\"+AND+\"llm\"\n",
      "\"natural language to sql\" AND \"attack\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"attack\"+AND+\"large+language+model\"\n",
      "\"natural language to sql\" AND \"vulnerability\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"vulnerability\"+AND+\"llm\"\n",
      "\"natural language to sql\" AND \"vulnerability\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"natural+language+to+sql\"+AND+\"vulnerability\"+AND+\"large+language+model\"\n",
      "\"semantic parsing to sql\" AND \"security\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"security\"+AND+\"llm\"\n",
      "\"semantic parsing to sql\" AND \"security\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"security\"+AND+\"large+language+model\"\n",
      "\"semantic parsing to sql\" AND \"access control\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"access+control\"+AND+\"llm\"\n",
      "\"semantic parsing to sql\" AND \"access control\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"access+control\"+AND+\"large+language+model\"\n",
      "\"semantic parsing to sql\" AND \"injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"injection\"+AND+\"llm\"\n",
      "\"semantic parsing to sql\" AND \"injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"injection\"+AND+\"large+language+model\"\n",
      "\"semantic parsing to sql\" AND \"prompt injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"prompt+injection\"+AND+\"llm\"\n",
      "\"semantic parsing to sql\" AND \"prompt injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"prompt+injection\"+AND+\"large+language+model\"\n",
      "\"semantic parsing to sql\" AND \"defense\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"defense\"+AND+\"llm\"\n",
      "\"semantic parsing to sql\" AND \"defense\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"defense\"+AND+\"large+language+model\"\n",
      "\"semantic parsing to sql\" AND \"attack\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"attack\"+AND+\"llm\"\n",
      "\"semantic parsing to sql\" AND \"attack\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"attack\"+AND+\"large+language+model\"\n",
      "\"semantic parsing to sql\" AND \"vulnerability\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"vulnerability\"+AND+\"llm\"\n",
      "\"semantic parsing to sql\" AND \"vulnerability\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"semantic+parsing+to+sql\"+AND+\"vulnerability\"+AND+\"large+language+model\"\n",
      "\"nl to sql\" AND \"security\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"security\"+AND+\"llm\"\n",
      "\"nl to sql\" AND \"security\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"security\"+AND+\"large+language+model\"\n",
      "\"nl to sql\" AND \"access control\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"access+control\"+AND+\"llm\"\n",
      "\"nl to sql\" AND \"access control\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"access+control\"+AND+\"large+language+model\"\n",
      "\"nl to sql\" AND \"injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"injection\"+AND+\"llm\"\n",
      "\"nl to sql\" AND \"injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"injection\"+AND+\"large+language+model\"\n",
      "\"nl to sql\" AND \"prompt injection\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"prompt+injection\"+AND+\"llm\"\n",
      "\"nl to sql\" AND \"prompt injection\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"prompt+injection\"+AND+\"large+language+model\"\n",
      "\"nl to sql\" AND \"defense\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"defense\"+AND+\"llm\"\n",
      "\"nl to sql\" AND \"defense\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"defense\"+AND+\"large+language+model\"\n",
      "\"nl to sql\" AND \"attack\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"attack\"+AND+\"llm\"\n",
      "\"nl to sql\" AND \"attack\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"attack\"+AND+\"large+language+model\"\n",
      "\"nl to sql\" AND \"vulnerability\" AND \"llm\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"vulnerability\"+AND+\"llm\"\n",
      "\"nl to sql\" AND \"vulnerability\" AND \"large language model\" is proccessed......\n",
      "Reached last page for keyword: \"nl+to+sql\"+AND+\"vulnerability\"+AND+\"large+language+model\"\n",
      "Total papers collected: 228\n"
     ]
    }
   ],
   "source": [
    "crawl_springer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f778f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_detail(page: ChromiumPage, link: str):\n",
    "    \n",
    "    page.get(link)\n",
    "    time.sleep(2)\n",
    "\n",
    "   # Trích xuất link PDF\n",
    "    pdf_link_element = page.ele(\"css:a.c-pdf-download__link\")\n",
    "    pdf_link = pdf_link_element.attr('href') if pdf_link_element else None\n",
    "    if pdf_link and pdf_link.startswith('/'):\n",
    "        pdf_link = \"https://link.springer.com\" + pdf_link\n",
    "    if not pdf_link.lower().endswith(\".pdf\"): \n",
    "        pdf_link = None\n",
    "    print(f\"PDF: {pdf_link}\")\n",
    "\n",
    "    #Trích xuất title\n",
    "    title_element = page.ele(\"css:h1.c-article-title\", timeout=10)\n",
    "    # Extract the text if the element exists, otherwise return None\n",
    "    title = title_element.text if title_element else None\n",
    "    print(f\"Title: {title}\")\n",
    "    \n",
    "    # Trích xuất authors\n",
    "    authors_element = page.ele(\"css:p.c-article-author-affiliation__authors-list\")\n",
    "    authors = authors_element.text if authors_element else None\n",
    "    print(f\"Authors: {authors}\")\n",
    "\n",
    "    # Trích xuất abstract\n",
    "    abstract_element = page.ele(\"css:div.c-article-section__content#Abs1-content\")\n",
    "    abstract = abstract_element.text if abstract_element else None\n",
    "    print(f'Abstract: {abstract}')\n",
    "\n",
    "    # Trích xuất submitted date\n",
    "    submitted_element = page.ele(\"css:span.c-bibliographic-information__value time\")\n",
    "    submitted_date = submitted_element.attr('datetime') if submitted_element else None\n",
    "    print(f\"Submitted_date: {submitted_date}\")\n",
    "\n",
    "    # Trích xuất DOI link\n",
    "    doi_element = page.ele(\"css:span.c-bibliographic-information__value\")\n",
    "    doi_link = None\n",
    "    if doi_element:\n",
    "        for ele in page.eles(\"css:span.c-bibliographic-information__value\"):\n",
    "            text = ele.text\n",
    "            if \"10.1007\" in text:  # Kiểm tra DOI\n",
    "                doi_link = text\n",
    "                break\n",
    "    if doi_link and not doi_link.startswith('http'):\n",
    "        doi_link = \"https://doi.org/\" + doi_link\n",
    "    print(f\"DOI link: {doi_link}\")\n",
    "\n",
    "    return title, authors, pdf_link, abstract, doi_link, submitted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d12c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing link: https://link.springer.com/article/10.1007/s42979-025-03662-6\n",
      "PDF: None\n",
      "Title: LLM-Based Text-to-SQL for Real-World Databases\n",
      "Authors: Eduardo R. Nascimento, Grettel García, Yenier T. Izquierdo, Lucas Feijó, Gustavo M. C. Coelho, Melissa Lemos & Marco A. Casanova\n",
      "Abstract: Text-to-SQL refers to the task defined as “given a relational databaseDand a natural language sentenceSthat describes a question on D, generate an SQL query QoverDthat expressesS”. Several LLM-based text-to-SQL tools, that is, text-to-SQL tools that explore Large Language Models (LLMs), emerged that outperformed previous approaches on well-known benchmarks. This article first shows that the performance of a selected set of LLM-based text-to-SQL tools is, however, significantly less when run on two challenging databases with a large number of tables, columns, and foreign keys. A closer analysis reveals that one of the problems lie in that the relational schema is an inappropriate specification of the database from the point of view of the LLM. The article then introduces database specifications based on LLM-friendly views, that are close to the language of the users’ questions and that eliminate frequently used joins, and LLM-friendly data descriptions of the database values. The article proceeds to show that the use of a set of LLM-friendly views and data samples considerably improves the performance of a text-to-SQL prompt strategy over a real-world database. This result suggests that real-world databases require rethinking how schema specifications should be passed to the LLM to recover state-of-the-art performance.\n",
      "Submitted_date: 2024-10-05\n",
      "DOI link: https://doi.org/10.1007/s42979-025-03662-6\n",
      "Processing link: https://link.springer.com/article/10.1007/s13042-023-02086-z\n",
      "PDF: None\n",
      "Title: ConDA: state-based data augmentation for context-dependent text-to-SQL\n",
      "Authors: Dingzirui Wang, Longxu Dou & Wanxiang Che\n",
      "Abstract: The context-dependent text-to-SQL task has profound real-world implications, as it facilitates users in extracting knowledge from vast databases, which allows users to acquire the information interactively for better accuracy. Unfortunately, current models struggle to address this task effectively due to the scarcity of data led by the high annotation overhead. The most straightforward method for addressing this problem is data augmentation, which aims at scaling up the parsing corpus. However, the naive methods suffer from the low diversity of the augmented data. To address this limitation, we propose the state-based CONtext-dependent text-to-SQL Data Augmentation (ConDA), which generate and filter augmented data based on the dialogue state, which has higher diversity. Experimental results show that ConDA yields performance improvement on all experimental datasets with an average boosting of \\(1.6\\%\\), proving the effectiveness of our method.\n",
      "Submitted_date: 2023-05-22\n",
      "DOI link: https://doi.org/10.1007/s13042-023-02086-z\n",
      "Processing link: https://link.springer.com/article/10.1007/s10664-023-10374-z\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s10664-023-10374-z.pdf\n",
      "Title: Assessing the utility of text-to-SQL approaches for satisfying software developer information needs\n",
      "Authors: Mihaela Tomova, Martin Hofmann, Constantin Hütterer & Patrick Mäder\n",
      "Abstract: Software analytics integrated with complex databases can deliver project intelligence into the hands of software engineering (SE) experts for satisfying their information needs. A new and promising machine learning technique known as text-to-SQL automatically extracts information for users of complex databases without the need to fully understand the database structure nor the accompanying query language. Users pose their request as so-called natural language utterance, i.e., question. Our goal was evaluating the performance and applicability of text-to-SQL approaches on data derived from tools typically used in the workflow of software engineers for satisfying their information needs. We carefully selected and discussed five seminal as well as state-of-the-art text-to-SQL approaches and conducted a comparative assessment using the large-scale, cross-domain Spider dataset and the SE domain-specific SEOSS-Queries dataset. Furthermore, we study via a survey how SE professionals perform in satisfying their information needs and how they perceive text-to-SQL approaches. For the best performing approach, we observe a high accuracy of 94% in query prediction when training specifically on SE data. This accuracy is almost independent of the query’s complexity. At the same time, we observe that SE professionals have substantial deficits in satisfying their information needs directly via SQL queries. Furthermore, SE professionals are open for utilizing text-to-SQL approaches in their daily work, considering them less time-consuming and helpful. We conclude that state-of-the-art text-to-SQL approaches are applicable in SE practice for day-to-day information needs.\n",
      "Submitted_date: 2023-07-25\n",
      "DOI link: https://doi.org/10.1007/s10664-023-10374-z\n",
      "Processing link: https://link.springer.com/article/10.1007/s00778-022-00776-8\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s00778-022-00776-8.pdf\n",
      "Title: A survey on deep learning approaches for text-to-SQL\n",
      "Authors: George Katsogiannis-Meimarakis & Georgia Koutrika\n",
      "Abstract: To bridge the gap between users and data, numerous text-to-SQL systems have been developed that allow users to pose natural language questions over relational databases. Recently, novel text-to-SQL systems are adopting deep learning methods with very promising results. At the same time, several challenges remain open making this area an active and flourishing field of research and development. To make real progress in building text-to-SQL systems, we need to de-mystify what has been done, understand how and when each approach can be used, and, finally, identify the research challenges ahead of us. The purpose of this survey is to present a detailed taxonomy of neural text-to-SQL systems that will enable a deeper study of all the parts of such a system. This taxonomy will allow us to make a better comparison between different approaches, as well as highlight specific challenges in each step of the process, thus enabling researchers to better strategise their quest towards the “holy grail” of database accessibility.\n",
      "Submitted_date: 2022-05-27\n",
      "DOI link: https://doi.org/10.1007/s00778-022-00776-8\n",
      "Processing link: https://link.springer.com/article/10.1007/s00530-023-01201-y\n",
      "PDF: None\n",
      "Title: SV2-SQL: a text-to-SQL transformation mechanism based on BERT models for slot filling, value extraction, and verification\n",
      "Authors: Chih-Yung Chang, Yuan-Lin Liang & Shih-Jung Wu\n",
      "Abstract: Information retrieval from databases is challenging for a non-SQL-domain expert. Some previous studies have provided solutions for translating the natural language to SQL instruction, aiming to access the information in the database directly. However, most solutions are in English Natural Language. In addition, the accuracies of the existing works still need to be improved. This work presents a mechanism called SV2-SQL, based on the pre-trained BERT. The proposed SV2-SQL mainly consists of multiple deep-learning models, including select-where slot filling model (SWSF-model), value extraction model (VE-model), and verification (V-model). The SWSF-model handles the classification tasks for those fields that appear in the “Select” and “Where” clauses, and the VE-model extracts the values for the “Where” clause from the input. The V-model sorts out the unwanted candidates from two previous models and leaves only the ones with the highest possibility. The proposed SV2-SQL also includes an algorithm for the inference process and allows the three models to be cooperative. Experimental results show that the proposed SV2-SQL outperforms the existing studies in terms of precision, accuracy, and recall.\n",
      "Submitted_date: 2023-02-24\n",
      "DOI link: https://doi.org/10.1007/s00530-023-01201-y\n",
      "Processing link: https://link.springer.com/article/10.1007/s13042-023-01898-3\n",
      "PDF: None\n",
      "Title: UniSAr: a unified structure-aware autoregressive language model for text-to-SQL semantic parsing\n",
      "Authors: Longxu Dou, Mingyang Pan, Dingzirui Wang, Wanxiang Che & Dechen Zhan\n",
      "Abstract: Existing text-to-SQL semantic parsers are typically designed for particular settings such as handling queries that span multiple tables, domains, or turns which makes them ineffective when applied to different settings. We present UniSAr (Unified Structure-Aware Autoregressive Language Model), which benefits from directly using an off-the-shelf language model architecture and demonstrates consistently high performance under different settings. Specifically, UniSAr extends existing autoregressive language models to incorporate two non-invasive extensions to make them structure-aware: (1) adding structure mark to encode database schema, conversation context, and their relationships; (2) constrained decoding to decode well-structured SQL for a given database schema. On seven well-known text-to-SQL datasets covering multi-domain, multi-table, and multi-turn, UniSAr demonstrates highly comparable or better performance to the most advanced specifically-designed text-to-SQL models.\n",
      "Submitted_date: 2022-12-19\n",
      "DOI link: https://doi.org/10.1007/s13042-023-01898-3\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-96-5318-8_15\n",
      "PDF: None\n",
      "Title: Chinese Text-to-SQL Parsing Based on Relation-Aware Mechanism\n",
      "Authors: Qixiang He & Hongyu Guo\n",
      "Abstract: When converting natural language questions into SQL queries with semantic parsing models, existing methods struggle to parse various unknown database structures. Encoding the relations within the database and aligning the columns in the database with the keywords in given natural language questions are key challenges for existing text-to-SQL methods to achieve generalization. Furthermore, the majority of research related to text-to-SQL tasks is currently based on English datasets, with very limited methods tailored to Chinese questions. To address these challenges, we propose a text-to-SQL method based on relation-aware self-attention mechanism. It utilizes multilingual BERT for initial word embedding and integrates both local and non-local relations distinctively with line graph. Experiments conducted on the Chinese dataset CSpider demonstrate the effectiveness of the proposed method. It achieves an accuracy of 51.9% in exact matching, at least 5% absolute improvement compared to other existing schema encoding and linking text-to-SQL models.\n",
      "Submitted_date: 2025-05-10\n",
      "DOI link: https://doi.org/10.1007/978-981-96-5318-8_15\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-75872-0_15\n",
      "PDF: None\n",
      "Title: Small, Medium, and Large Language Models for Text-to-SQL\n",
      "Authors: Aiko Oliveira, Eduardo Nascimento, Gustavo Coelho, Lucas Feijó, Yenier Izquierdo, Grettel García, Melissa Lemos & Marco A. Casanova\n",
      "Abstract: This paper investigates how the model size affects the ability of a Generative AI Language Model, or briefly a GLM, to support the text-to-SQL task for databases with large schemas typical of real-world applications. The paper first introduces a text-to-SQL framework that combines a prompt strategy and a Retrieval-Augmented Generation (RAG) technique, leaving as flexibilization points the GLM and the database. Then, it describes a benchmark based on an open-source database featuring a schema much larger than the schemas of most of the databases in familiar text-to-SQL benchmarks. The paper proceeds with experiments to assess the performance of the text-to-SQL framework instantiated with the benchmark database and GLMs of different sizes. The paper concludes with recommendations to help select which GLM size is appropriate for a text-to-SQL scenario, characterized by the difficulty of the expected NL questions and the data privacy requirements, among other characteristics.\n",
      "Submitted_date: 2024-10-21\n",
      "DOI link: https://doi.org/10.1007/978-3-031-75872-0_15\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-7019-3_21\n",
      "PDF: None\n",
      "Title: Context-Dependent Text-to-SQL Generation with Intermediate Representation\n",
      "Authors: Xuesong Gao & Junfeng Zhao\n",
      "Abstract: In recent years, the Text-to-SQL task has become a research hotspot in semantic analysis. Among them, context-dependent Text-to-SQL task has received more and more attention as it meets the needs of actual scenarios. The core of the problem is how to use historical interaction information and database schema to understand the context. Most existing research ignores the structure of SQL queries and introduces low-level information such as variable names and parameters, and the mismatch problem between intents expressed in utterance and the implementation details in SQL still exists. In this paper, SemQL is applied to serve as an intermediate representation between utterance and SQL, meanwhile, the Coarse-to-Fine neural architecture is adopted to decompose decoding process of SemQL into two stages. We validated the performance of our model on SParC and CoSQL datasets, which outperforms the existing ones and achieves excellent results on both datasets.\n",
      "Submitted_date: 2023-11-10\n",
      "DOI link: https://doi.org/10.1007/978-981-99-7019-3_21\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-71167-1_17\n",
      "PDF: None\n",
      "Title: Valid Text-to-SQL Generation with Unification-Based DeepStochLog\n",
      "Authors: Ying Jiao, Luc De Raedt & Giuseppe Marra\n",
      "Abstract: Large language models have been used to translate natural language questions to SQL queries. Without hard constraints on syntax and database schema, they occasionally produce invalid queries that are not executable. These failures limit the usage of these systems in real-life scenarios. We propose a neurosymbolic framework that imposes SQL syntax and schema constraints with unification-based definite clause grammars and thus guarantees the generation of valid queries. Our framework also builds a bi-directional interface to language models to leverage their natural language understanding abilities. The evaluation results on a subset of SQL grammars show that all our output queries are valid. This work is the first step towards extending language models with unification-based grammars. We demonstrate this extension enhances the validity, execution accuracy, and ground truth alignment of the underlying language model by a large margin. Our code is available at https://github.com/ML-KULeuven/deepstochlog-lm.\n",
      "Submitted_date: 2024-09-10\n",
      "DOI link: https://doi.org/10.1007/978-3-031-71167-1_17\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-9434-8_35\n",
      "PDF: None\n",
      "Title: Finetuning LLMs for Text-to-SQL with Two-Stage Progressive Learning\n",
      "Authors: Xiao Ling, Jialin Liu, Jindu Liu, Jianhua Wu & Jie Liu\n",
      "Abstract: With the widespread usage of large language model (LLMs), LLM-based method has become the mainstream approach for Text-to-SQL tasks, achieving leading performance on Text-to-SQL leaderboards. However, generating complex SQL queries correctly has always been a main challenge. Current LLM-based models primarily utilize prompting-based methods on large scale closed-source LLMs (e.g., GPT-4 and ChatGPT), which may cause concerns of usage costs and data privacy. For fine-tuning based methods, it is difficult to generate complex SQL accurately in only one fine-tuning step. Focusing on this, we propose TSP-SQL, a Two-Stage Progressive learning method for Text-to-SQL. TSP-SQL decomposes Text-to-SQL task into two stages: SQL elements generation auxiliary task, and SQL query generation main task. The two tasks are progressively fine-tuned on a single model, effectively reducing the difficulty of SQL generation and improving accuracy. TSP-SQL achieves state-of-the-art performance among open-source fine-tuning based methods on Spider dev set, and surpasses most of the methods based on large scale closed-source LLMs.\n",
      "Submitted_date: 2024-11-01\n",
      "DOI link: https://doi.org/10.1007/978-981-97-9434-8_35\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-68309-1_11\n",
      "PDF: None\n",
      "Title: SQL-to-Schema Enhances Schema Linking in Text-to-SQL\n",
      "Authors: Sun Yang\n",
      "Abstract: Sophisticated Text-to-SQL methods often face errors, such as schema-linking errors, join errors, nested errors, and group-by errors. To mitigate these, it’s crucial to filter out unnecessary tables and columns, focusing the language model on relevant ones. Previous methods have attempted to sort tables and columns based on relevance or directly identify necessary elements, but these approaches suffer from long training times, high costs with GPT-4 tokens, or poor schema linking performance. We propose a two-step schema linking method: first, generate an initial SQL query using the full database schema; then, extract the relevant tables and columns to form a concise schema. This method, tested with Code Llama and GPT-4, shows optimal performance compared to mainstream methods on the Spider dataset, reducing errors and improving efficiency in SQL generation.\n",
      "Submitted_date: 2024-08-18\n",
      "DOI link: https://doi.org/10.1007/978-3-031-68309-1_11\n",
      "Processing link: https://link.springer.com/article/10.1007/s11042-022-12573-0\n",
      "PDF: None\n",
      "Title: Enhance text-to-SQL model performance with information sharing and reweight loss\n",
      "Authors: Chi Wei, Shaobin Huang & Rongsheng Li\n",
      "Abstract: The goal of Text-to-SQL task is to map natural language queries into equivalent structured query languages(NL2SQL). On the WikiSQL dataset, the method used by the state-of-the-art models is to decouple the NL2SQL task into subtasks and then build a dedicated decoder for each subtask. There are some problems in this method, such as the model is too complicated, and the ability to learn the dependency between different subtasks is limited. To solve these problems, this paper innovatively introduces the sharing mechanism of multi-task learning into the NL2SQL task and realizes sharing by letting different subtasks share the same decoder. Firstly, sharing decoders for different subtasks can effectively reduce the complexity of the model, and at the same time, allows different subtasks to share knowledge during the training process so that the model can better learn the dependencies between different subtasks. This paper also designed a re-weighted loss to balance the complexity of the SELECT clause and the WHERE clause. We have evaluated the method in this article on the WikiSQL dataset. The experimental results show that the accuracy of the proposed model is better than state-of-the-art on the WikiSQL without execution guided decoding.\n",
      "Submitted_date: 2021-03-17\n",
      "DOI link: https://doi.org/10.1007/s11042-022-12573-0\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-7022-3_24\n",
      "PDF: None\n",
      "Title: QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing\n",
      "Authors: Linzheng Chai, Jian Yang, Liqun Yang & Zhoujun Li\n",
      "Abstract: Context-dependent Text-to-SQL aims to translate multi-turn natural language questions into SQL queries. Despite various methods have exploited context-dependence information implicitly for contextual SQL parsing, there are few attempts to explicitly address the dependencies between current question and question context. This paper presents QURG, a novel QUestion Rewriting Guided approach to help the models achieve adequate contextual understanding. Specifically, we first train a question rewriting model to complete the current question based on question context, and convert them into a rewriting edit matrix. We further design a two-stream matrix encoder to jointly model the rewriting relations between question and context, and the schema linking relations between natural language and structured schema. Experimental results show that QURG significantly improves the performances on two large-scale context-dependent datasets SParC and CoSQL, especially for hard and long-turn questions.\n",
      "Submitted_date: 2023-11-10\n",
      "DOI link: https://doi.org/10.1007/978-981-99-7022-3_24\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-96-1809-5_41\n",
      "PDF: None\n",
      "Title: HIT-SCIR at CCKS-IJCKG2024: Enhancing Text-to-SQL with Multi-step Pipeline\n",
      "Authors: Dingzirui Wang, Xuanliang Zhang, Keyan Xu & Wanxiang Che\n",
      "Abstract: The text-to-SQL task translates natural language questions into SQL queries, simplifying database access. While large language models (LLMs) have shown strong performance, they often struggle with complex reasoning, such as commonsense and numerical reasoning, required for more challenging SQL generation. We propose a new pipeline that enhances SQL generation by incorporating advanced reasoning skills, alongside techniques like entity linking and self-correction. Tested on the Archer dataset, which requires more complex reasoning, our approach improves performance by \\(23.3\\%\\) over the baseline, demonstrating its effectiveness in handling challenging queries.\n",
      "Submitted_date: 2025-03-09\n",
      "DOI link: https://doi.org/10.1007/978-981-96-1809-5_41\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-0769-4_18\n",
      "PDF: None\n",
      "Title: Solving Text-to-SQL Task Through Machine Translation\n",
      "Authors: Alaka Das\n",
      "Abstract: Technical people use structured Query Language (SQL) to retrieve information from relational databases, whereas novice users lack the expertise to use SQL. For these people, natural language interface to the database (NLIDB) systems is nowadays being developed to make them deal with data comfortably. These NLIDB systems convert a query in natural language (NL) to its corresponding structured query language (SQL). This task is famously known as text-to-SQL task. Most of the works in this area consider this task as a semantic parsing problem or a variation of it and though research in this area is gradually converging to satisfactory outcomes, till date no widely accepted commercial product is available. This paper considers Text-to-SQL task as a machine translation problem and describes a model that uses an open-source neural machine translation toolkit OpenNMT for training and translation and a parallel corpus developed from the Spider dataset. The training accuracy of the model is 99.97% which no existing NLIDB system have achieved so far. This result proves that a more organized dataset may lead to cent percent training accuracy and the model may be used in certain real-life applications to ease human work load. The Python code for parallel corpora generation, training and testing will be available at https://github.com/Ali-Das/Text_to_SQL_using_OpenNMT_with_Spider site.\n",
      "Submitted_date: 2023-06-15\n",
      "DOI link: https://doi.org/10.1007/978-981-99-0769-4_18\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-7022-3_23\n",
      "PDF: None\n",
      "Title: Prompting GPT-3.5 for Text-to-SQL with De-semanticization and Skeleton Retrieval\n",
      "Authors: Chunxi Guo, Zhiliang Tian, Jintao Tang, Pancheng Wang, Zhihua Wen, Kang Yang & Ting Wang\n",
      "Abstract: Text-to-SQL is a task that converts a natural language question into a structured query language (SQL) to retrieve information from a database. Large language models (LLMs) work well in natural language generation tasks, but they are not specifically pre-trained to understand the syntax and semantics of SQL commands. In this paper, we propose an LLM-based framework for Text-to-SQL which retrieves helpful demonstration examples to prompt LLMs. However, questions with different database schemes can vary widely, even if the intentions behind them are similar and the corresponding SQL queries exhibit similarities. Consequently, it becomes crucial to identify the appropriate SQL demonstrations that align with our requirements. We design a de-semanticization mechanism that extracts question skeletons, allowing us to retrieve similar examples based on their structural similarity. We also model the relationships between question tokens and database schema items (i.e., tables and columns) to filter out scheme-related information. Our framework adapts the range of the database schema in prompts to balance length and valuable information. A fallback mechanism allows for a more detailed schema to be provided if the generated SQL query fails. Ours outperforms state-of-the-art models and demonstrates strong generalization ability on three cross-domain Text-to-SQL benchmarks.\n",
      "Submitted_date: 2023-11-10\n",
      "DOI link: https://doi.org/10.1007/978-981-99-7022-3_23\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-19-0098-3_14\n",
      "PDF: None\n",
      "Title: Redefining Text-to-SQL Task as a Machine Translation Problem\n",
      "Authors: Alaka Das\n",
      "Abstract: Natural language interfaces are gaining popularity as an alternative interface for non-technical users. Natural language interface to database (NLIDB) systems have been attracting considerable interest recently that are being developed to accept user’s query in natural language (NL), and then converting this NL query to an SQL query, the SQL query is executed to extract the resultant data from the database. This Text-to-SQL task is a long-standing, open problem, and towards solving the problem, the standard approach that is followed is to implement a sequence-to-sequence model. In this paper, I recast the Text-to-SQL task as a machine translation problem using sequence-to-sequence-style neural network models. To this end, I have introduced a parallel corpus that I have developed using the WikiSQL dataset. Though there are a lot of work done in this area using sequence-to-sequence-style models, most of the state-of-the-art models use semantic parsing or a variation of it. None of these models’ accuracy exceeds 90%. In contrast to it, my model is based on a very simple architecture as it uses an open-source neural machine translation toolkit OpenNMT, that implements a standard SEQ2SEQ model, and though my model’s performance is not better than the said models in predicting on test and development datasets, its training accuracy is higher than any existing NLIDB system to the best of my knowledge.\n",
      "Submitted_date: 2022-06-10\n",
      "DOI link: https://doi.org/10.1007/978-981-19-0098-3_14\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-7184-4_11\n",
      "PDF: None\n",
      "Title: KIS-SQL: Knowledge-Enhanced In-Context Learning and Self-correction of Text-to-SQL\n",
      "Authors: Qiujie Fan\n",
      "Abstract: Text-to-SQL power by large language models enhances the accessibility and user-friendliness of relational databases significantly. Optimizing prompting strategies for improved SQL query accuracy, is one of the key research directions in this field. We propose the KIS-SQL model based on knowledge-enhanced prompt learning. KIS-SQL used Structural Measurement-Based In-Context Learning to predict the syntax structure of SQLs, and integrating this structural knowledge to construct prompts that can better guide large language models in generating SQLs. Meanwhile, Syntax-Semantic Analysis Guided Self-Correction is employed to incorporate syntax and semantic knowledge obtained from external analysis engines into prompts, thereby improving the ability of large language models to correct SQLs. Experiments are conducted on the Spider dataset using Exact-Set-Match accuracy and Execution accuracy as metrics, and the results show that KIS-SQL outperforms baseline models.\n",
      "Submitted_date: 2024-08-21\n",
      "DOI link: https://doi.org/10.1007/978-981-97-7184-4_11\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-5663-6_32\n",
      "PDF: None\n",
      "Title: An Experimental Research of Text-to-SQL for Heterogeneous Data in Large Language Models\n",
      "Authors: Weiwei Yang, Xiaoliang Wang, Bosheng Chen, Yong Liu, Bing Wang, Hui Wang, Xiaoke Wang, Haitao Zhua & Zhehao Wang\n",
      "Abstract: The large language model (LLMs) technology has become a new paradigm for Text-to-SQL task. However, when faced with heterogeneous data, there is still a lack of comprehensive and effective solutions for Text-to-SQL task, especially in terms of high-quality corpus construction methods and low-resource training strategies. To address this challenge, this paper systematically studies Text-to-SQL technology in the field of heterogeneous data and innovatively proposes the HD-SQL method. This method achieves the construction of Text-to-SQL corpora for heterogeneous data that are secure, efficient, and low-cost. Based on this corpus, we further explore supervised fine-tuning strategies for low-resource LLMs on heterogeneous data. Compared to baseline models, significant performance improvements are achieved on the Heterogeneous Data Test set, further validating the practicality and effectiveness of the HD-SQL method.\n",
      "Overall, this paper proposes a feasible solution for Text-to-SQL task with heterogeneous data. We hope that our work can advance research in this field and provide strong technical support for practical applications.\n",
      "Submitted_date: 2024-08-01\n",
      "DOI link: https://doi.org/10.1007/978-981-97-5663-6_32\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-70411-6_41\n",
      "PDF: None\n",
      "Title: A Survey on Text-to-SQL Parsing: From Rule-Based Foundations to Large Language Models\n",
      "Authors: Farida El Boujddaini & Youssef Mejdoub\n",
      "Abstract: Transforming natural language into SQL queries represents a significant challenge in both natural language processing (NLP) and database research fields. The main target of this transformation is to allow users to interact with databases using their natural language. This extends the scope of data analysis to include persons who do not have expertise in computer science or data engineering. In recent years, the implementation of deep learning models has yielded significant results. Following this, the introduction of large language models (LLMs) such as GPT-4 and BERT has lifted the standard for this task, promoting them to new heights of performance and capacity. Our survey provides a detailed review of the evolutionary phases of Text-to-SQL parsing. Initially, we investigate the primary approaches in this field that use rule-based models. Following this, we present the several deep learning models proposed for this task. After that, we introduce several datasets commonly used in training and evaluating the proposed models. In the last section, we examine the role of large language models (LLMs) in revolutionizing Text-to-SQL tasks.\n",
      "Submitted_date: 2024-10-13\n",
      "DOI link: https://doi.org/10.1007/978-3-031-70411-6_41\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-96-1024-2_12\n",
      "PDF: None\n",
      "Title: Enhancing Interaction Graph of Data Schema and Syntactic Structure with Pre-trained Language Model for Text-to-SQL\n",
      "Authors: Wenbin Zhao, Long Zhao, Zixuan Zheng, Haoxin Jin & Bin Gu\n",
      "Abstract: Text-to-SQL generation is an important area of natural language processing. It can help non-specialists interact with databases using natural language, simplify the database query process, improve efficiency, enhance the user experience, etc. Existing work on Text-to-SQL mainly utilizes large-scale pre-trained models to improve the performance of model generation. Despite progress, Text-to-SQL still has some shortcomings in some respects, such as the discrepancy of words in natural languages, inaccurate scheme links, and inadequate domain generalization capabilities. In this paper, we present an SQL generating framework,which enhancing the interaction graph of data schema and syntactic structure with pre-trained language model for Text-to-SQL(SGIS), aimed at improving the domain generalization of models and the ability of models to deal with cross-cutting questions. Specifically, we first introduce a model linking method based on a pre-trained model, extracting input NL question and database scheme relationship structures to solve the scheme linking question between the NL questions in the model and database models. On this basis, the sentence in the input NL question is extracted from the reliant information and integrated into the well-structured chart data to solve the incomplete question of the relationship characteristics embedded in it. At the same time, in order to prevent the over-adaptation of embedded sides during the training process during the optimization process, we use a type-coding method to help the model effectively differentiate between the type of relationship that the sentence depends on when embedding sides, thereby reducing unnecessary entanglement. Numerous experiments have proven that SGIS’s performance on both data sets Spider and Spider-SYN under standard settings is due to all comparative base lines.\n",
      "Submitted_date: 2025-01-24\n",
      "DOI link: https://doi.org/10.1007/978-981-96-1024-2_12\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-89621-7_8\n",
      "PDF: None\n",
      "Title: On the Construction of Text-to-SQL Tools Based on Large Language Models for Real-World Relational Databases\n",
      "Authors: Eduardo Nascimento, Gustavo Coelho, Lucas Feijó, Yenier Izquierdo, Grettel García, Aiko Oliveira, Melissa Lemos & Marco Casanova\n",
      "Abstract: A strategy to construct natural language database interfaces is to use Large Language Models (LLMs) to translate the end-user questions into SQL queries. Such interfaces will be called LLM-based text-to-SQL tools. This article analyses the limitations and proposes solutions to improve the performance of LLM-based text-to-SQL tools for real-world relational databases that have large, complex schemas often expressed in terms different from those adopted by end-users to formulate their questions. The article considers implementations based on Prompt Engineering, including Retrieval-Augmented Generation, and LLM fine-tuning. Finally, it describes experiments that analyze the accuracy of some implementations on two benchmarks built upon databases with large schemas.\n",
      "Submitted_date: 2025-05-21\n",
      "DOI link: https://doi.org/10.1007/978-3-031-89621-7_8\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-44693-1_42\n",
      "PDF: None\n",
      "Title: SeSQL: A High-Quality Large-Scale Session-Level Chinese Text-to-SQL Dataset\n",
      "Authors: Saihao Huang, Zhenghua Li, Zeyang Liu, Chenhui Dou, Fukang Yan & Min Zhang\n",
      "Abstract: As the first session-level Chinese dataset, CHASE contains two separate parts, i.e., 2,003 sessions manually constructed from scratch (CHASE-C), and 3,456 sessions translated from English SParC (CHASE-T). We find the two parts are highly discrepant and incompatible. In this work, we present SeSQL, a high-quality large-scale session-level Chinese text-to-SQL dataset, consisting of 5,028 sessions all manually constructed from scratch. Compared with previous datasets, in order to guarantee data quality, we adopt an iterative annotation workflow to facilitate intense and in-time review of previous-round natural language (NL) questions and SQL queries. Moreover, by completing all context-dependent NL questions, we obtain 27,012 context-independent question/SQL pairs, allowing SeSQL to be used as the largest dataset for single-round text-to-SQL parsing. We conduct benchmark session-level text-to-SQL parsing experiments on SeSQL via employing three competitive session-level parsers, and present detailed analysis.\n",
      "Submitted_date: 2023-10-08\n",
      "DOI link: https://doi.org/10.1007/978-3-031-44693-1_42\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-8076-5_25\n",
      "PDF: None\n",
      "Title: Retrieval-Augmented GPT-3.5-Based Text-to-SQL Framework with Sample-Aware Prompting and Dynamic Revision Chain\n",
      "Authors: Chunxi Guo, Zhiliang Tian, Jintao Tang, Shasha Li, Zhihua Wen, Kaixuan Wang & Ting Wang\n",
      "Abstract: Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases. Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL. However, it faces challenges with strict SQL syntax requirements. Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large. In this paper, we propose a retrieval-augmented prompting method for an LLM-based Text-to-SQL framework, involving sample-aware prompting and a dynamic revision chain. Our approach incorporates sample-aware demonstrations, which include the composition of SQL operators and fine-grained information related to the given question. To retrieve questions sharing similar intents with input questions, we propose two strategies for assisting retrieval. Firstly, we leverage LLMs to simplify the original questions, unifying the syntax and thereby clarifying the users’ intentions. To generate executable and accurate SQLs without human intervention, we design a dynamic revision chain that iteratively adapts fine-grained feedback from the previously generated SQL. Experimental results on three Text-to-SQL benchmarks demonstrate the superiority of our method over strong baseline models.\n",
      "Submitted_date: 2023-11-14\n",
      "DOI link: https://doi.org/10.1007/978-981-99-8076-5_25\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-68309-1_8\n",
      "PDF: None\n",
      "Title: Improving the Accuracy of Text-to-SQL Tools Based on Large Language Models for Real-World Relational Databases\n",
      "Authors: Gustavo M. C. Coelho, Eduardo R. S. Nascimento, Yenier T. Izquierdo, Grettel M. García, Lucas Feijó, Melissa Lemos, Aiko R. de Oliveira & Marco A. Casanova\n",
      "Abstract: Real-world relational databases (RW-RDB) have large, complex schemas often expressed in terms alien to end-users. This scenario is challenging to LLM-based text-to-SQL tools, that is, tools that translate Natural Language (NL) sentences into SQL queries using a Large Language Model (LLM). Indeed, their accuracy on RW-RDBs is considerably less than that reported for well-known synthetic benchmarks. This paper then introduces a technique to improve the accuracy of LLM-based text-to-SQL tools on RW-RDBs using Retrieval-Augmented Generation. The technique consists of two steps. Using the RW-RDB schema, the first step generates a synthetic dataset E of pairs \\((Q_N,Q_S)\\), where \\(Q_N\\) is an NL sentence and \\(Q_S\\) is the corresponding SQL translation. The core contribution of the paper is an algorithm that implements this first step. Given an input NL sentence \\(Q_I\\), the second step retrieves pairs \\((Q_N,Q_S)\\) from E based on the similarity of \\(Q_I\\) and \\(Q_N\\), and prompts such pairs to the LLM to improve accuracy. To argue in favor of the proposed technique, the paper includes experiments with an RW-RDB, which is in production at an Energy company, and a well-known text-to-SQL prompt strategy. It repeats the experiments with Mondial, an openly available database with a large schema. These experiments constitute a second contribution of the paper.\n",
      "Submitted_date: 2024-08-18\n",
      "DOI link: https://doi.org/10.1007/978-3-031-68309-1_8\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-7254-8_53\n",
      "PDF: None\n",
      "Title: GADESQL: Graph Attention Diffusion Enhanced Text-To-SQL with Single and Multi-hop Relations\n",
      "Authors: Qinzhen Cao, Rui Xi, Jie Wu, Xiaowen Nie & Mengshu Hou\n",
      "Abstract: Text-To-SQL is crucial for enabling users without technical expertise to effectively extract important information from databases. The graph-based encoder has been successfully employed in this field. However, existing methods often adopt a node-centric approach, focusing on single-hop edge relations. This approach gives rise to two main issues: 1) failure to differentiate between single-hop and multi-hop relations among nodes; 2) ignoring the valuable multi-hop reasoning information between nodes. To tackle these challenges, we propose a Graph Attention Diffusion Enhanced Text-To-SQL(GADESQL) model that enables multi-hop reasoning among nodes. With GAD, information can propagate efficiently through multi-hop paths, uniquely integrating single-hop and multi-hop relations during the graph iteration process. Furthermore, we employ Semantic Dependency Parsing for natural language analysis, constructing a semantic analysis tree for questions to enhance the effective connection between question tokens and Schema structures. Experiments on the cross-domain dataset Spider demonstrate that our model possesses strong generalization capabilities, achieving certain performance improvements over existing works.\n",
      "Submitted_date: 2023-10-21\n",
      "DOI link: https://doi.org/10.1007/978-981-99-7254-8_53\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-98305-5_26\n",
      "PDF: None\n",
      "Title: Integrating Question Answering and Text-to-SQL in Portuguese\n",
      "Authors: Marcos Menon José & Fábio Gagliardi Cozman\n",
      "Abstract: Deep learning transformers have drastically improved systems that automatically answer questions in natural language. However, different questions demand different answering techniques; here we propose, build and validate an architecture that integrates different modules to answer two distinct kinds of queries. Our architecture takes a free-form natural language text and classifies it to send it either to a Neural Question Answering Reasoner or a Natural Language parser to SQL. We implemented a complete system for the Portuguese language, using some of the main tools available for the language and translating training and testing datasets. Experiments show that our system selects the appropriate answering method with high accuracy (over 99%), thus validating a modular question answering strategy.\n",
      "Submitted_date: 2022-03-16\n",
      "DOI link: https://doi.org/10.1007/978-3-030-98305-5_26\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-44192-9_15\n",
      "PDF: None\n",
      "Title: Towards Robustness of Large Language Models on Text-to-SQL Task: An Adversarial and Cross-Domain Investigation\n",
      "Authors: Weixu Zhang, Yu Wang & Ming Fan\n",
      "Abstract: Recent advances in large language models (LLMs) like ChatGPT have led to impressive results on various natural language processing (NLP) challenges including text-to-SQL task, which aims to automatically generate SQL queries from natural language questions. However, these language models are still subject to vulnerabilities such as adversarial attacks, domain shift and lack of robustness, which can greatly affect their performance and reliability. In this paper, we conduct a comprehensive evaluation of large language models, such as ChatGPT, on their robustness in text-to-SQL tasks. We assess the impact of adversarial and domain generalization perturbations on LLMs using seven datasets, five of which are popular robustness evaluation benchmarks for text-to-SQL tasks and two are synthetic adversarial datasets generated by ChatGPT. Our experiments show that while LLMs exhibit promise as zero-shot text-to-SQL parsers, their performances degrade under adversarial and domain generalization perturbations, with varying degrees of robustness depending on the type and level of perturbations applied. We also explore the impact of usage-related factors such as prompt design on the performance and robustness of LLMs. Our study provides insights into the limitations and potential directions for future research to enhance the performance and robustness of LLMs on text-to-SQL and other NLP tasks.\n",
      "Submitted_date: 2023-09-22\n",
      "DOI link: https://doi.org/10.1007/978-3-031-44192-9_15\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-91699-2_35\n",
      "PDF: None\n",
      "Title: mRAT-SQL+GAP: A Portuguese Text-to-SQL Transformer\n",
      "Authors: Marcelo Archanjo José\n",
      "Abstract: The translation of natural language questions to SQL queries has attracted growing attention, in particular in connection with transformers and similar language models. A large number of techniques are geared towards the English language; in this work, we thus investigated translation to SQL when input questions are given in the Portuguese language. To do so, we properly adapted state-of-the-art tools and resources. We changed the RAT-SQL+GAP system by relying on a multilingual BART model (we report tests with other language models), and we produced a translated version of the Spider dataset. Our experiments expose interesting phenomena that arise when non-English languages are targeted; in particular, it is better to train with original and translated training datasets together, even if a single target language is desired. This multilingual BART model fine-tuned with a double-size training dataset (English and Portuguese) achieved 83% of the baseline, making inferences for the Portuguese test dataset. This investigation can help other researchers to produce results in Machine Learning in a language different from English. Our multilingual ready version of RAT-SQL+GAP and the data are available, open-sourced as mRAT-SQL+GAP at: https://github.com/C4AI/gap-text2sql.\n",
      "Submitted_date: 2021-11-28\n",
      "DOI link: https://doi.org/10.1007/978-3-030-91699-2_35\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-17189-5_1\n",
      "PDF: None\n",
      "Title: Faster and Better Grammar-Based Text-to-SQL Parsing via Clause-Level Parallel Decoding and Alignment Loss\n",
      "Authors: Kun Wu, Lijie Wang & Xinyan Xiao\n",
      "Abstract: As a mainstream approach, grammar-based models have achieved high performance in text-to-SQL parsing task, but suffer from low decoding efficiency since the number of actions for building SQL trees are much larger than the number of tokens in SQL queries. Meanwhile, intuitively it is beneficial from the parsing performance perspective to incorporate alignment information between SQL clauses and question segments. This paper proposes clause-level parallel decoding and alignment loss to enhance two high-performance grammar-based parsers, i.e., RATSQL and LGESQL. Experiments on the Spider dataset show our approach improves the decoding speed of RATSQL and LGESQL by 18.9% and 35.5% respectively, and also achieves consistent improvement in parsing accuracy, especially on complex questions.\n",
      "Submitted_date: 2022-09-24\n",
      "DOI link: https://doi.org/10.1007/978-3-031-17189-5_1\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-92310-5_59\n",
      "PDF: None\n",
      "Title: CMAT: Column-Mask-Augmented Training for Text-to-SQL Parsers\n",
      "Authors: Chen Chang\n",
      "Abstract: When it comes to text-to-SQL tasks, the model needs to learn context-based representations of schema along with natural language utterances. We present a simple and effective method for text-to-SQL tasks, Column-Mask-Augmented Training (CMAT), to make up for the insufficiency of training data. To exploit the synthesized data, we propose the clause prediction (CP) object for multi-task learning, which forces the model to capture contextual features of the schema items. Besides, we add the fuzzy match and subword match to the schema linking strategy in RAT-SQL. As a result, our method significantly increases the recall and F1 value of schema linking and achieves a competitive result with RAT-SQL and GraPPa on Spider.\n",
      "Submitted_date: 2021-12-02\n",
      "DOI link: https://doi.org/10.1007/978-3-030-92310-5_59\n",
      "Processing link: https://link.springer.com/article/10.1007/s40593-025-00460-2\n",
      "PDF: None\n",
      "Title: SQL Autograder: Web-based LLM-powered Autograder for Assessment of SQL Queries\n",
      "Authors: Karan Manikani, Radhika Chapaneri, Dharmik Shetty & Divyata Shah\n",
      "Abstract: Structured query language (SQL) queries are an important aspect of database concepts in the information technology (IT) domain. Evaluation of SQL queries ensures that the learners can understand and apply various SQL concepts correctly. However, this can be a laborious task when carried out manually by course instructors at universities, which often does not scale well. To address these limitations, this study proposes a web-based application, SQL autograder, which can be used by instructors of a university course to evaluate assessments and enhance the quality of education and learning outcomes. We propose a framework that makes use of large language models (LLMs) to assess the correctness of SQL queries submitted by students. We train a variety of open-source LLMs of varying sizes on a diverse dataset of SQL queries, with queries ranging from simple ones that include a single JOIN statement to more complex ones involving multiple SQL features. We implemented and tested our LLM-based framework in real-world educational settings for a university course, which shows promising results in enhancing the learning experience for students by providing instant feedback on areas needing improvement. We tested our application on 88 participants and found that the autograder is 180x faster than the instructor, with an average accuracy of 96.77%. After taking the qualitative feedback from the participants, 97% of them found it to be useful. The proposed framework reduces the workload of instructors by offering a more scalable and consistent evaluation process that enhances the performance of students.\n",
      "Submitted_date: 2025-01-18\n",
      "DOI link: https://doi.org/10.1007/s40593-025-00460-2\n",
      "Processing link: https://link.springer.com/article/10.1007/s00778-024-00837-0\n",
      "PDF: None\n",
      "Title: Speech-to-SQL: toward speech-driven SQL query generation from natural language question\n",
      "Authors: Yuanfeng Song & Raymond Chi-Wing Wong\n",
      "Abstract: Speech-based inputs have been gaining significant momentum with the popularity of smartphones and tablets in our daily lives, since voice is the most popular and efficient way for human–computer interaction. This paper works toward designing more effective speech-based interfaces to query the structured data in relational databases. We first identify a new task named Speech-to-SQL, which aims to understand the information conveyed by human speech and directly translate it into structured query language (SQL) statements. A naive solution to this problem can work in a cascaded manner, that is, an automatic speech recognition component followed by a text-to-SQL component. However, it requires a high-quality ASR system and also suffers from the error compounding problem between the two components, resulting in limited performance. To handle these challenges, we propose a novel end-to-end neural architecture named SpeechSQLNet to directly translate human speech into SQL queries without an external ASR step. SpeechSQLNet has the advantage of making full use of the rich linguistic information presented in speech. To the best of our knowledge, this is the first attempt to directly synthesize SQL based on common natural language questions in spoken form, rather than a natural language-based version of SQL. To validate the effectiveness of the proposed problem and model, we further construct a dataset named SpeechQL, by piggybacking the widely used text-to-SQL datasets. Extensive experimental evaluations on this dataset show that SpeechSQLNet can directly synthesize high-quality SQL queries from human speech, outperforming various competitive counterparts as well as the cascaded methods in terms of exact match accuracies. We expect speech-to-SQL would inspire more research on more effective and efficient human–machine interfaces to lower the barrier of using relational databases.\n",
      "Submitted_date: 2023-01-28\n",
      "DOI link: https://doi.org/10.1007/s00778-024-00837-0\n",
      "Processing link: https://link.springer.com/article/10.1007/s11704-024-40330-z\n",
      "PDF: None\n",
      "Title: A survey of table reasoning with large language models\n",
      "Authors: Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Qingfu Zhu & Wanxiang Che\n",
      "Abstract: Table reasoning aims to generate inference results based on the user requirement and the provided table. Enhancing the table reasoning capability of the model can aid in obtaining information efficiently. Recent advancements have positioned Large Language Models (LLMs) as the predominant method for table reasoning, primarily due to their substantial reduction in annotation costs and superior performance compared to previous methods. However, existing research still lacks a summary of LLM-based table reasoning works. Therefore, questions about which techniques can improve table reasoning performance in the era of LLMs and how to enhance table reasoning abilities in the future, remain largely unexplored. This gap significantly limits progress in research. To answer the above questions and advance table reasoning research with LLMs, we present this survey to analyze existing research, inspiring future work. In this paper, we analyze the mainstream techniques used to improve table reasoning performance in the LLM era. Also, we provide research directions from the improvement of existing methods to inspire future research.\n",
      "Submitted_date: 2024-04-02\n",
      "DOI link: https://doi.org/10.1007/s11704-024-40330-z\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-96-1809-5_40\n",
      "PDF: None\n",
      "Title: Improving SQL Generation with Schema Retrieval and Reaction Mechanism\n",
      "Authors: Yang Shuangtao, Zhang Donghai & Fu Bo\n",
      "Abstract: Text-to-SQL is one of the crucial tasks in natural language processing. We present an innovative approach to the text-to-SQL parsing challenge, leveraging retrieval-driven in-context learning to enhance the performance of large language model (LLMs) in translating natural language queries into SQL queries. We address the critical challenges of schema linking and SQL generation quality by incorporates a schema-similar retrieval module and a Reaction mechanism for error correction. Our schema-similar retrieval module identifies and integrates the most contextually relevant data from the database schema into the LLMs’ input, significantly improving the model's reasoning capabilities. The Reaction mechanism introduces a novel verification process during SQL generation, ensuring syntactical correctness and query accuracy. Our method has been rigorously tested on the Archer, a complex bilingual text-to-SQL dataset known for its demanding reasoning requirements. The results demonstrate a remarkable improvement in query accuracy, achieving 42.56% on the Archer dataset, surpassing both Deepseek and GPT-4o baselines. This advancement not only streamlines the interaction between non-expert users and database access but also contributes to the broader field of natural language processing by pushing the boundaries of LLMs applicability in structured query generation.\n",
      "Submitted_date: 2025-03-09\n",
      "DOI link: https://doi.org/10.1007/978-981-96-1809-5_40\n",
      "Processing link: https://link.springer.com/article/10.1007/s10489-023-05043-z\n",
      "PDF: None\n",
      "Title: NLINQ: A natural language interface for querying network performance\n",
      "Authors: Barun Kumar Saha\n",
      "Abstract: Artificial Intelligence is finding increased applications in communication networks. In particular, the field of text-to-Structured Query Language (SQL) translation has great potential to improve customer experience by allowing the querying of network performance databases using natural language. Such adoption, however, is challenging, in general. On one hand, live production systems may have databases with non-semantic table and column names, which makes natural language parsing and text-to-SQL translation difficult. On the other hand, noisy input texts may lead to the generation of incorrect queries. Moreover, inaccurate transcription of speech input into text may further aggravate the problem. Motivated by these aspects, we investigate the problem of natural language-based querying of network performance databases used by Wireless Mesh Networks (WMNs). In particular, we fine-tune a state-of-the-art model to translate natural language questions into appropriate SQL queries. In order to mitigate the problem of non-semantic names, we generate database views with semantic column names, based on the existing tables. In addition, we make domain-specific corrections in the text in order to help generate accurate queries. We also design the Natural Language Interface for Network Query (NLINQ) prototype for a real-life industrial WMN solution. The results of the performance evaluation indicate that natural language text can be translated into SQL queries with an accuracy of 89.021–92.663%, on average. Moreover, the average turnaround time of NLINQ ranges between 1.263–2.013 seconds. The results indicate that NLINQ is suitable for real-time, interactive querying of network performance databases.\n",
      "Submitted_date: 2023-09-25\n",
      "DOI link: https://doi.org/10.1007/s10489-023-05043-z\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-86957-1_20\n",
      "PDF: None\n",
      "Title: Similar Questions Correspond to Similar SQL Queries: A Case-Based Reasoning Approach for Text-to-SQL Translation\n",
      "Authors: Wei Yu, Xiaoting Guo, Fei Chen, Tao Chang, Mengzhu Wang & Xiaodong Wang\n",
      "Abstract: Based on the natural truth that similar questions correspond to similar SQL queries, a CBR-based approach is proposed to deal with the Text-to-SQL task in this paper. We follow the traditional CBR processes: similarity assessment, case retrieval, and case reuse. First, we introduce a neural classifier in the similarity assessment stage and comprehensively uses classification probability and literal cosine similarity to measure similarity. Then, based on the results of the similarity assessment, our model retrieves a case template. Finally, our model fills the columns and values generated by the Ranker module and Question Answering (QA) module into the solution template. At this point, a SQL query suitable for the new case is generated. We evaluate our models on a large-scale Text-to-SQL dataset—WikiSQL. Experimentally, our model has a competitive performance compared with the baseline and significantly improves the accuracy of the aggregation function prediction.\n",
      "Submitted_date: 2021-09-10\n",
      "DOI link: https://doi.org/10.1007/978-3-030-86957-1_20\n",
      "Processing link: https://link.springer.com/article/10.1007/s11042-023-16987-2\n",
      "PDF: None\n",
      "Title: Translating natural language questions to SQL queries (nested queries)\n",
      "Authors: Sindhuja Swamidorai\n",
      "Abstract: Real world questions are generally complex and need the user to extract information from multiple tables in a database using complex SQL queries like nested queries. Though the overall accuracy in translation of Natural Language queries to SQL queries lies close to 75%, the accuracy of complex queries is still quite less, around 60% in the current state-of-the-art models. In this vein, this study proposes to improve the current IRNet framework for translating natural language queries to nested SQL queries, one type of complex queries. Data oversampling is first used to boost the representation of nested queries in order to achieve this goal. Second, a novel loss function that computes the overall loss while accounting for the complexity of SQL, as measured by the quantity of SELECT columns and keywords in the SQL query. The proposed method exhibited a 5% improvement in prediction of hard and extra hard queries when tested on Spider’s development dataset.\n",
      "Submitted_date: 2023-01-23\n",
      "DOI link: https://doi.org/10.1007/s11042-023-16987-2\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-60029-7_32\n",
      "PDF: None\n",
      "Title: Interpretable Text-to-SQL Generation with Joint Optimization\n",
      "Authors: Mingdong Zhu, Xianfang Wang & Yang Zhang\n",
      "Abstract: The purpose of Text-to-SQL is to obtain the correct answer for a textual question from the database, which can take advantage of advanced database system to provide reliable and efficient response. Existing Text-to-SQL methods generally focus on accuracy by designing complex deep neural network models, and hardly consider interpretability, which is very important for serious applications. To address this, in this paper we propose a novel framework for Interpretable Text-to-SQL Generation (ITSG) with joint optimization, which achieves state-of-the-art accuracy and possesses two-level interpretability at the same time. The framework mainly consists of three layers: a sequence encoder which encodes questions, table headers and significant table contents, an attention-based LSTM layer which generates SQL queries and a reinforcement learning layer which boosts the execution accuracy. Comparing with state-of-the-art methods on benchmark datasets, the experimental results show the effectiveness and interpretability of our ITSG framework.\n",
      "Submitted_date: 2020-09-22\n",
      "DOI link: https://doi.org/10.1007/978-3-030-60029-7_32\n",
      "Processing link: https://link.springer.com/article/10.1007/s11704-024-40763-6\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s11704-024-40763-6.pdf\n",
      "Title: Large language model for table processing: a survey\n",
      "Authors: Weizheng Lu, Jing Zhang, Ju Fan, Yueguo Chen & Xiaoyong Du\n",
      "Abstract: Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, Web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.\n",
      "Submitted_date: 2024-07-26\n",
      "DOI link: https://doi.org/10.1007/s11704-024-40763-6\n",
      "Processing link: https://link.springer.com/article/10.1007/s40031-025-01202-7\n",
      "PDF: None\n",
      "Title: A Spatial Attention-Based Transductive Long Short-Term Memory for Semantic Parsing Trained with a Semi-Supervised Method\n",
      "Authors: Shichao Chen\n",
      "Abstract: Semantic parsing (SP) aims to convert natural language into SQL, which is a complex task. Traditional SP research typically depends on supervised learning, which requires extensive, accurately labeled databases. This study introduces a semi-supervised generative adversarial network (SS-GAN) model, enhanced by a spatial attention-based transductive long-short-term memory (TLSTM), to improve classifier effectiveness. The semi-supervised aspect of SS-GAN is essential in situations where labeled data is limited or unevenly distributed. The TLSTM model applies transductive learning methods, focusing on samples close to the test data to refine the adaptation process and outperform traditional LSTM models. Our methodology includes a regularization tactic to mitigate mode collapse and stabilize SS-GAN training. We guide the generator with feature vectors from the discriminator to generate precise outputs. Furthermore, we integrate a reconstruction loss into SS-GAN’s loss function, forcing the generator to mimic the discriminator’s features, which aligns the outputs more closely with real data configurations. This elevates the effectiveness of SS-GAN and ensures high-quality output. We evaluate the recommended framework utilizing the WikiSQL and SparC databases. Our experimental outcomes highlight the model's superiority over existing models, achieving execution accuracies of 0.902 and 0.844 on the WikiSQL and SparC databases.\n",
      "Submitted_date: 2024-04-17\n",
      "DOI link: https://doi.org/10.1007/s40031-025-01202-7\n",
      "Processing link: https://link.springer.com/article/10.1007/s11042-023-15731-0\n",
      "PDF: None\n",
      "Title: An SQL query generator for cross-domain human language based questions based on NLP model\n",
      "Authors: B. Balaji Naik\n",
      "Abstract: The amount of data generated in the modern world is so great that data lakes are now being used to store data. However, relational databases are currently the primary repository for the world’s data. However, it is very time-consuming for a user to type each query every time, especially the queries that include complex keywords. Our proposed approach uses the interaction history by altering the preceding projected query to improve the generation quality, based on the finding that successive human language queries are frequently lin- guistically dependent, and their equivalent SQL queries overlap. This paper focuses on text-to-SQL conversion for cross-domain datasets. Our approach reuses results produced at the token level and considers SQL statements as sequences. Finally, we evaluate our approach on different datasets like the Sparc, Spider, and CoSQL datasets. It compared our proposed approach with existing famous algorithms like Seq2seq, and added attention and copying to the seq2seq model, SQLNet model, and TypeSQL model in terms of accuracy and F1 score.\n",
      "Submitted_date: 2022-06-27\n",
      "DOI link: https://doi.org/10.1007/s11042-023-15731-0\n",
      "Processing link: https://link.springer.com/chapter/10.1007/979-8-8688-0515-8_6\n",
      "PDF: None\n",
      "Title: Natural Language to SQL\n",
      "Authors: Pere Martra\n",
      "Abstract: In the first chapter of this book, you saw a very naive approach to creating an NL2SQL system. In this project, since it's a bit more serious, you'll create the prompt by following the guidelines in a paper from Ohio University: Shuaichen Chang, Eric Fosler-Lussier (2023). \"How to Prompt LLMs for Text-to-SQL.\" arXiv:2305.11853 [cs.CL].\n",
      "Submitted_date: 2024-09-19\n",
      "DOI link: https://doi.org/10.1007/979-8-8688-0515-8_6\n",
      "Processing link: https://link.springer.com/article/10.1007/s00778-025-00912-0\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s00778-025-00912-0.pdf\n",
      "Title: prompt4vis: prompting large language models with example mining for tabular data visualization\n",
      "Authors: Shuaimin Li, Chen Jason Zhang & Fei Hao\n",
      "Abstract: We are currently in the epoch of Large Language Models (LLMs), which have transformed numerous technological domains within the database community. In this paper, we examine the application of LLMs in text-to-visualization (text-to-vis). The advancement of natural language processing technologies has made natural language interfaces more accessible and intuitive for visualizing tabular data. However, despite utilizing advanced neural network architectures, current methods such as Seq2Vis, ncNet, and RGVisNet for transforming natural language queries into DV commands still underperform, indicating significant room for improvement. In this paper, we introduce Prompt4Vis, a novel framework that leverages LLMs and In-context learning to enhance the generation of data visualizations from natural language. Given that In-context learning’s effectiveness is highly dependent on the selection of examples, it is critical to optimize this aspect. Additionally, encoding the full database schema of a query is not only costly but can also lead to inaccuracies. This framework includes two main components: (1) an example mining module that identifies highly effective examples to enhance In-context learning capabilities for text-to-vis applications, and (2) a schema filtering module designed to streamline database schemas. Comprehensive testing on the NVBench dataset has shown that Prompt4Vis significantly outperforms the current state-of-the-art model, RGVisNet, by approximately 35.9% on development sets and 71.3% on test sets. To the best of our knowledge, Prompt4Vis is the first framework to incorporate In-context learning for enhancing text-to-vis, marking a pioneering step in the domain.\n",
      "Submitted_date: 2024-05-17\n",
      "DOI link: https://doi.org/10.1007/s00778-025-00912-0\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-15928-2_35\n",
      "PDF: None\n",
      "Title: Implementing Vocal Natural Language Interface to Enterprise Resource Planning System\n",
      "Authors: Shengzhe Zhang & Julien Le Duigou\n",
      "Abstract: In order to meet the challenge of industry in economic globalization, enterprises around world have built information systems to effectively manage the production, achieve integration of suppliers and customer and organize all the resources by software. However, the interaction between the system and users are not always intuitive and some of the questions about production cannot easily to be answered in today’s Enterprise Resource Planning (ERP). This paper aims at proposing a vocal natural language interface which adopts text-to-SQL neural networks for an open source ERP system Odoo for purpose of interacting with the complex ERP system easily by voice. The current automatic speech recognition can transform voice into text with reliability. For the next step of text-to-SQL, although there is no large amount of data of natural-language-query and SQL pairs in production, the neural text-to-SQL parser can be trained on less specific databases and data and worked on specific closed domain databases like ERP. Moreover, this cross-domain feature of neural text-to-SQL parser makes it possible to build such an interface. The result showed that such an interface can be applied on actual usage scenario and informs the user of the manufacturing status. The proposed system is now implemented on command line and a graphic interface will make it more intuitive.\n",
      "Submitted_date: 2022-09-25\n",
      "DOI link: https://doi.org/10.1007/978-3-031-15928-2_35\n",
      "Processing link: https://link.springer.com/article/10.1007/s00778-025-00900-4\n",
      "PDF: None\n",
      "Title: Generating highly customizable python code for data processing with large language models\n",
      "Authors: Immanuel Trummer\n",
      "Abstract: CARD (Coding Assistant for Relational Data analysis) generates Python code that processes relational queries on raw data. Users can customize generated code via natural language instructions, e.g., by instructing the system to use specific libraries or produce certain output. Internally, CARD uses large language models such as GPT-4o to synthesize code. CARD automatically constructs prompts describing code generation tasks to the language models. Those prompts contain information on data format, customization requirements, as well as processing plans, generated by CARD’s scenario-specific query planner. CARD automatically tests generated code by comparing its output to the output of a reference SQL engine. In case of inconsistencies, CARD re-generates code with a certain degree of randomization. Furthermore, CARD can automatically generate libraries of code samples for specific customization scenarios in a pre-processing step, leveraging those samples at run time for few-shot learning. The experiments show that CARD generates accurate code in the vast majority of scenarios. Furthermore, current trends in language models are likely to benefit CARD’s performance in the future.\n",
      "Submitted_date: 2024-06-30\n",
      "DOI link: https://doi.org/10.1007/s00778-025-00900-4\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-82938-3_27\n",
      "PDF: None\n",
      "Title: The Role of Large Language Models Within Construction Facilities Lifecycle Data Management\n",
      "Authors: Eugeny Chaplygin & Konstantin Losev\n",
      "Abstract: Introduction: The modern construction industry faces the challenge of managing vast volumes of fragmented data, which complicates its integration and analysis across all stages of the construction lifecycle. The use of artificial intelligence, particularly large language models (LLM), offers a promising approach to address these issues. The relevance of the study lies in the need to optimize data processing workflows using LLM to improve the efficiency of construction project management. Materials and Methods: The study examines three methods for applying LLM: Retrieval-Augmented Generation (RAG) for unstructured data, Text-to-SQL for structured data in relational databases, and Knowledge Map for processing sensitive unstructured data. Results: Testing demonstrated that RAG effectively retrieves information from unstructured sources, Text-to-SQL ensures precise access to data in relational databases, and Knowledge Map minimizes errors when handling hierarchical data structures. These methods complement each other, enabling LLM to work with a wide range of data types and improving the accuracy of the responses. Conclusion: The application of LLM in construction data management opens new opportunities for automation and improving the precision of data processing. Further integration of LLM with advanced technologies, such as digital twins and the Internet of Things, has the potential to create more efficient systems for managing construction projects.\n",
      "Submitted_date: 2025-02-21\n",
      "DOI link: https://doi.org/10.1007/978-3-031-82938-3_27\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-36778-7_48\n",
      "PDF: None\n",
      "Title: Text2SQLNet: Syntax Type-Aware Tree Networks for Text-to-SQL\n",
      "Authors: Youssef Mellah & El Hassane Ettifouri\n",
      "Abstract: Building natural language interfaces to relational databases is an important and challenging problem in natural language processing (NLP), it requires a system that is able to understand natural language questions and generate corresponding SQL queries. In this paper, we present our idea of using type information and database content to better understand rare entities and numbers in natural language questions, in order to improve the model SyntaxSQLNet as the state of the art in Text-to-SQL task. We also present the global architecture and techniques that can be used in the implementation of our Neural Network (NN) model Text2SQLNet, with the integration of our idea that consists of using type information to better understand rare entities and numbers in natural language questions. We can also use the database content to better understand the user query if it is not well-formed. The implementation of this idea can further improve performance in the Text-to-SQL task.\n",
      "Submitted_date: 2019-12-01\n",
      "DOI link: https://doi.org/10.1007/978-3-030-36778-7_48\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-45043-3_4\n",
      "PDF: None\n",
      "Title: Text to Data\n",
      "Authors: Yunyao Li\n",
      "Abstract: This chapter covers text-to-data translation. It begins by providing a historical perspective, briefly covering early relevant work on semantic parsing and meaning representation before moving to newer approaches that utilize word embedding, semantic compositionality, knowledge graph, and large language models. Three key topics are covered in depth: (1) converting sentences to structured form, (2) neural semantic parsing, and (3) recent text-to-SQL models, systems, and benchmarks.\n",
      "Submitted_date: 2023-11-25\n",
      "DOI link: https://doi.org/10.1007/978-3-031-45043-3_4\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-8391-9_10\n",
      "PDF: None\n",
      "Title: CySpider: A Neural Semantic Parsing Corpus with Baseline Models for Property Graphs\n",
      "Authors: Ziyu Zhao, Wei Liu, Tim French & Michael Stewart\n",
      "Abstract: Enterprise knowledge graphs are gaining increasing popularity in industrial applications, with a pressing demand for natural language interfaces to support non-technical end-users. For natural language queries to relational databases, the neural semantic parsing task Text-to-SQL achieves strong performance in translating text inputs to SQL queries. However, very few public corpora are available for the training of neural semantic parsing models that convert textual queries to graph query languages. In this research, we develop a generic SQL2Cypher algorithm that can map a SQL query to a set of Cypher clauses, where Cypher is a query language used by a popular property graph database Neo4j. The converted Cypher statement is then combined with the original natural language query to create a parallel corpus that enables end-to-end training of neural semantic parsing models for Text-to-Cypher. To evaluate the dataset quality, we construct a corresponding graph database to obtain execution accuracy. In addition, the Text-to-Cypher corpus features four transformer-based baseline models. The availability of such corpus and baseline models is critical in developing and benchmarking new machine learning methods in advancing natural language interfaces for fact retrieval from large graph-based knowledge repositories. The source code and dataset are available at github(https://github.com/22842219/SemanticParser4Graph).\n",
      "Submitted_date: 2023-11-27\n",
      "DOI link: https://doi.org/10.1007/978-981-99-8391-9_10\n",
      "Processing link: https://link.springer.com/article/10.1007/s10791-024-09443-8\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s10791-024-09443-8.pdf\n",
      "Title: Leveraging transformers architectures and augmentation for efficient classification of fasteners and natural language searches\n",
      "Authors: Nino Cauli, Marco Murgia & Diego Reforgiato Recupero\n",
      "Abstract: A primary concern in the realm of mechanical engineering is to ensure the efficient and effective data entry of hardware devices. Fasteners are mechanical tools that rigidly connect or affix two surfaces or objects together. They are small and often different fasteners might look similar; it is therefore a long and prone-to-risk procedure to manually analyze them to classify and store their related information. With the widespread diffusion of AI frameworks in several domains, equipment manufacturers started to rely on AI technologies for these heavy tasks. Automatically classifying fasteners by type and extracting metadata from natural language questions are important tasks that fastener manufacturers and suppliers encounter. In this paper, we address these challenges. To address the first task, we introduce an augmentation methodology that starts with a small set of 3D models representing each of the 21 types of fasteners we aim to classify. This methodology efficiently generates multiple 2D images from these models. Next, we train a vision transformer using the collected data to address a single-label multi-class classification task. For the second task, we introduce a prompt-engineering technique designed for conversational agents. This technique leverages in-context knowledge to extract (metadata field, value) pairs from natural language questions. Subsequently, we tackle a question-answering task to the description fields of the extracted fasteners. Our evaluation demonstrates the effectiveness of both approaches, surpassing the baselines we tested.\n",
      "Submitted_date: 2023-09-12\n",
      "DOI link: https://doi.org/10.1007/s10791-024-09443-8\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-45043-3_5\n",
      "PDF: None\n",
      "Title: Evaluation\n",
      "Authors: Yunyao Li\n",
      "Abstract: Suppose you are in charge of administering an enterprise database and are tasked with choosing an NLIDB system to provide a non-technical interface to the users of your database. You check the literature and find hundreds of research papers on the subject. You also check a few leaderboards and find tens of top-performing models. How do you decide on a system? What should you look for in choosing one from many choices? These are some of the questions that are answered in this chapter. This chapter covers the evaluation methodologies for NLIDBs with a focus on text-to-SQL. Datasets and benchmarks are presented and their statistics, in terms of supported query types, are compared. Two major evaluation methods, namely reference-based and human- centric evaluations, are discussed in more detail. Finally, other evaluation metrics are reviewed and some of the top-performing models and their characteristics are presented.\n",
      "Submitted_date: 2023-11-25\n",
      "DOI link: https://doi.org/10.1007/978-3-031-45043-3_5\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-53187-4_61\n",
      "PDF: None\n",
      "Title: Artificial Neural Networks for Text-to-SQL Task: State of the Art\n",
      "Authors: Youssef Mellah, Hassane El Ettifouri, Toumi Bouchentouf & Mohammed Ghaouth Belkasmi\n",
      "Abstract: Databases store a significant amount of data from the world, however, to access this data, users must understand a query language such as SQL. In order to facilitate this task and to make the interaction with databases possible for all the world, researches has recently appeared to approach systems that understand the natural language questions and automatically convert them into SQL queries. The purpose of this article is to provide the state of the art text-to-SQL task in which we present the main models and existing solutions based on Artificial Neural Networks (ANN), precisely on Deep Learning (DL) and Natural Language Processing (NLP). We also specify the experimental settings of each approach, their limits as well as a comparison of the best existing ones.\n",
      "Submitted_date: 2020-08-04\n",
      "DOI link: https://doi.org/10.1007/978-3-030-53187-4_61\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-61007-3_2\n",
      "PDF: None\n",
      "Title: Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies\n",
      "Authors: Alessandro Berti, Humam Kourani, Hannes Häfke, Chiao-Yun Li & Daniel Schuster\n",
      "Abstract: Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results. However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks. This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks? The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.\n",
      "Submitted_date: 2024-05-31\n",
      "DOI link: https://doi.org/10.1007/978-3-031-61007-3_2\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-64576-1_15\n",
      "PDF: None\n",
      "Title: A Survey of Natural Language-Based Editing of Low-Code Applications Using Large Language Models\n",
      "Authors: Simon Cornelius Gorissen & Wolf G. Beckmann\n",
      "Abstract: In recent years, Large Language Models (LLMs) have showcased an impressive ability for natural language (NL) understanding, code generation, and logical reasoning. This provides the potential to significantly speed up development times by integrating this technology into software development workflows. Similarly, Low-Code Development Platforms (LCDPs) are already in use for reducing development effort and lowering the entry barrier to who can become a developer in the first place. This poses the question whether these technologies can be combined in order to enable end-users to edit an application via NL while experienced developers can still work on the same app using a regular LCDP and benefit from its advantages. To asses whether this proposal has been realised yet (and if so, to what extend), a literature survey is necessary. This paper presents such a survey, outlining how LLMs have been used to edit low-code applications, and especially Oracle Application Express (APEX) apps. It identifies an open research gap in this direction.\n",
      "Submitted_date: 2024-07-01\n",
      "DOI link: https://doi.org/10.1007/978-3-031-64576-1_15\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-62624-1_11\n",
      "PDF: None\n",
      "Title: Innovative SQL Query Generator Using an Arabic Language Description\n",
      "Authors: Salma Salah Ounifi, Mohamed Samir Elbuni & Yousef Omran Gdura\n",
      "Abstract: This paper presents the development of an advanced system designed to efficiently process Arabic queries in today’s data-driven landscape. By using the T5 sequence-to-sequence model developed by Google and fine-tuning it specifically for transforming textual and verbal input into SQL queries, the system becomes adept at accurately comprehending and interpreting Arabic queries.\n",
      "The resulting system serves as a user-friendly tool that automates the generation of SQL queries. Users can input their queries in written or recorded Arabic utterances, eliminating the need for manual translation and query construction.\n",
      "This involved fine-tuning T5 models on a SQL dataset, splitting the dataset, tokenizing it, and setting training parameters. The implementation phase included loading the fine-tuned model, which incorporated PICARD to generate valid queries effectively. This paper also explores the impact of Arabic translation on the performance and accuracy of the model.\n",
      "The optimal testing on test set accuracy achieved was 63.11%, the real-world testing accuracy: T5-base without PICARD scored 56.96% and 51.36% on English and Arabic questions, respectively, while T5-large with PIARCD scored 77.74% and 70.34% on English and Arabic questions.\n",
      "Submitted_date: 2024-06-30\n",
      "DOI link: https://doi.org/10.1007/978-3-031-62624-1_11\n",
      "Processing link: https://link.springer.com/article/10.1186/s44342-024-00020-5\n",
      "PDF: https://link.springer.com/content/pdf/10.1186/s44342-024-00020-5.pdf\n",
      "Title: Customizing GPT for natural language dialogue interface in database access\n",
      "Authors: Jin-Dong Kim\n",
      "Abstract: The paper presents Anatomy3DExplorer, a customized ChatGPT designed as a natural language dialogue interface for exploring 3D models of anatomical structures. It illustrates the significant potential of large language models (LLMs) as user-friendly interfaces for database access. Furthermore, it showcases the seamless integration of LLMs and database APIs, within the GPTS framework, offering a promising and straightforward approach.\n",
      "Submitted_date: 2024-06-08\n",
      "DOI link: None\n",
      "Processing link: https://link.springer.com/article/10.1007/s41064-024-00301-2\n",
      "PDF: None\n",
      "Title: Enabling Spatial Digital Twins: Technologies, Challenges, and Future Research Directions\n",
      "Authors: Mohammed Eunus Ali & Tanzima Hashem\n",
      "Abstract: A Digital Twin (DT) is a virtual replica of a physical object or system, created to monitor, analyze, and optimize its behavior and characteristics. A Spatial Digital Twin (SDT) is a specific type of digital twin that emphasizes the geospatial aspects of the physical entity, incorporating precise location and dimensional attributes for a comprehensive understanding of its spatial environment. With the recent advancement in spatial technologies and breakthroughs in other computing technologies such as Artificial Intelligence (AI) and Machine Learning (ML), the SDTs market is expected to rise to 25 billion, covering a wide range of applications. The majority of existing research focuses on DTs and often fails to address the necessary spatial technologies essential for constructing SDTs. The current body of research on SDTs primarily concentrates on analyzing their potential impact and opportunities within various application domains. As building an SDT is a complex process and requires a variety of spatial computing technologies, it is not straightforward for practitioners and researchers of this multi-disciplinary domain to grasp the underlying details of enabling technologies of the SDT. In this paper, we are the first to systematically analyze different spatial technologies relevant to building an SDT in a layered approach (starting from data acquisition to visualization). More specifically, we present the tech stack of SDTs into five distinct layers of technologies: (i) data acquisition and processing; (ii) data integration, cataloging, and metadata management; (iii) data modeling, database management & big data analytics systems; (iv) Geographic Information System (GIS) software, maps, & APIs; and (v) key functional components such as visualizing, querying, mining, simulation, and prediction. Moreover, we discuss how modern technologies such as AI/ML, blockchains, and cloud computing can be effectively utilized in enabling and enhancing SDTs. Finally, we identify a number of research challenges and opportunities in SDTs. This work serves as an important resource for SDT researchers and practitioners as it explicitly distinguishes SDTs from traditional DTs, identifies unique applications, outlines the essential technological components of SDTs, and presents a vision for their future development along with the challenges that lie ahead.\n",
      "Submitted_date: 2023-08-08\n",
      "DOI link: https://doi.org/10.1007/s41064-024-00301-2\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-7254-8_51\n",
      "PDF: None\n",
      "Title: Task-Driven Neural Natural Language Interface to Database\n",
      "Authors: Yuquan Yang, Qifan Zhang & Junjie Yao\n",
      "Abstract: Natural language querying offers an intuitive and user-friendly interface. A Natural Language Interface over databases, often termed as “Text-to-SQL”, involves translating a query posed in natural language into a corresponding SQL query for structured databases. A significant number of recent methodologies, anchored in the pre-trained language model and encode-decode paradigms, have been developed to address this task. Yet, existing approaches often grapple with generating accurate SQL queries, especially in scenarios that involve multiple values and intricate column calculations.\n",
      "In this study, we present a task-driven Text-to-SQL model. This model breaks down the SQL prediction process into specific sub-tasks based on the unique task requirements of the query. Specifically, we amalgamate structure prediction, value extraction, and column relationship prediction into a cohesive workflow. The model is designed to construct target SQL queries incrementally, with each sub-task building upon the outcomes of its predecessors. Additionally, we introduce a novel filtering mechanism to refine and re-order candidates produced during the beam search phase. We substantiate the efficacy of our model using public datasets, showcasing its adeptness in both English and Chinese contexts.\n",
      "Submitted_date: 2023-10-21\n",
      "DOI link: https://doi.org/10.1007/978-981-99-7254-8_51\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-19-9304-6_21\n",
      "PDF: None\n",
      "Title: A Review of Datasets for NLIDBs\n",
      "Authors: Alaka Das & Rakesh Balabantaray\n",
      "Abstract: Natural language interface to database (NLIDB) is a research area that is gradually merging natural language processing (NLP) and data Sciences, specifically after the rise of deep learning. Traditional NLIDBs were built based on rules, syntactic and semantic grammars, statistical and ontology-based techniques and methods from NLP. Most of those systems usually were supporting controlled natural language queries (NLQs) where parsing error and ambiguities were manually handled by the users. As deep learning (DL) captures complex dependencies,human interventions is getting reduced. DL-based NLIDBs are again in focus and many recent NLIDBs got satisfactory performance by using sequence-to-sequence DL model and semantic parsing text-to-SQL datasets. These DL end to end models require huge, complex and cross-domain labeled dataset for exhaustive training. As none of the existing datasets for text-to-SQL tasks seems to be perfect, to meet the demand, new datasets are being produced in every couple of years. Though the role of dataset is very crucial for natural language interface, there are very few review papers focusing on datasets. This paper reviews the datasets used in different NLIDBs, discusses their importance and presents a summary report.\n",
      "Submitted_date: 2023-05-16\n",
      "DOI link: https://doi.org/10.1007/978-981-19-9304-6_21\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-92552-8_14\n",
      "PDF: None\n",
      "Title: Virtual Consultant: An Automated SAP Advisor\n",
      "Authors: Sadettin Melenli & Barış Arslan\n",
      "Abstract: SAP consultants play a crucial role in understanding client needs, analyzing business processes, and designing tailored SAP solutions. A significant part of their work involves converting functional documents into technical specifications, a time-consuming task. To streamline this process, we introduce “Virtual Consultant,” an automated advisor capable of generating Functional Specifications (FS), Technical Specifications (TS), and even ABAP code. By leveraging Azure AI services and advanced language models, Virtual Consultant can significantly reduce the time and effort required for SAP implementation projects. This study demonstrates the potential of AI-driven automation in revolutionizing SAP consulting practices.\n",
      "SAP consultants are responsible for understanding their clients’ needs, analyzing business processes, and designing and implementing customized SAP solutions to meet specific requirements. They can also write scripts and programs to create user-friendly interfaces and provide advanced functionality.\n",
      "SAP consultants are also responsible for converting functional documents written during studies into technical documents. This process involves translating business requirements and processes into technical specifications, creating instructions that will be used during application development and configuration. These documents cover topics such as system design, configuration, test scenarios, and user training.\n",
      "This study introduces “Virtual Consultant,” an automated advisor designed to address specific requirements within SAP systems for business units. Virtual Consultant takes the place of functional consultants by automatically generating Functional Specification (FS) and Technical Specification (TS) documents and creating ABAP code. Additionally, it can analyze existing code and rewrite FS and TS documents. This work highlights the potential of Virtual Consultant in optimizing business processes.\n",
      "Microsoft has made OpenAI’ s flagship model, GPT-4o, available on Azure AI. This ground-breaking multimodal model integrates text, image, and audio capabilities, setting a new standard for productive and chat-based artificial intelligence experiences.\n",
      "In this study we are using Azure AI services to generate the technical document using functional documents. Our main target for this study was not to get not full ready technical documentation. But during the study we achieved much more efficient results. In our first trials, we achieved around 50% ready document, after that we got more efficient. Finally, we achieved around 80% ready technical documents and some code parts to be used.\n",
      "By using this technique, it will be possible to complete SAP work in a shorter time. Thanks to the open source LLM used, it will be possible to achieve efficiency at other stages in the future.\n",
      "Submitted_date: 2025-05-30\n",
      "DOI link: https://doi.org/10.1007/978-3-031-92552-8_14\n",
      "Processing link: https://link.springer.com/article/10.1007/s10489-022-03587-0\n",
      "PDF: None\n",
      "Title: SQLSketch-TVC: Type, value and compatibility based approach for SQL queries\n",
      "Authors: Karam Ahkouk & Mustapha Machkour\n",
      "Abstract: Understanding the complexity of the translation of Natural Language (NL) sentences to SQL queries becomes an essential part in the resolution process. The majority of the proposed models either focus on simple queries or suffer when exposed to unseen domains or new schemas structures; This can be understood as the greater part of solutions are based on limited datasets or treat the problem in an end-to-end perspective. Our previously proposed model which is SQLSketch that provides an intelligent method for handling complex queries was able to outperform all the state-of-the-art models on the GreatSQL dataset. This paper addresses the problem of translating NL sentences to SQL queries in an effective way by leveraging our previous SQLSketch model with a type aware layer, a values classification method as well as a compatibility based module that enhance the quality of the predicted items (SQLSketch-TVC). We evaluate the new model using the Components and Exact matching metrics. The results show that SQLSketch-TVC outperforms the other models on all SQL components and provides a novel way for inferring values from the input Question.\n",
      "Submitted_date: 2022-03-10\n",
      "DOI link: https://doi.org/10.1007/s10489-022-03587-0\n",
      "Processing link: https://link.springer.com/article/10.1007/s10796-022-10295-0\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s10796-022-10295-0.pdf\n",
      "Title: Knowledge Graph and Deep Learning-based Text-to-GraphQL Model for Intelligent Medical Consultation Chatbot\n",
      "Authors: Pin Ni & Ramin Okhrati\n",
      "Abstract: Text-to-GraphQL (Text2GraphQL) is a task that converts the user's questions into Graph + QL (Query Language) when a graph database is given. That is a task of semantic parsing that transforms natural language problems into logical expressions, which will bring more efficient direct communication between humans and machines. The existing related work mainly focuses on Text-to-SQL tasks, and there is no available semantic parsing method and data set for the graph database. In order to fill the gaps in this field to serve the medical Human–Robot Interactions (HRI) better, we propose this task and a pipeline solution for the Text2GraphQL task. This solution uses the Adapter pre-trained by “the linking of GraphQL schemas and the corresponding utterances” as an external knowledge introduction plug-in. By inserting the Adapter into the language model, the mapping between logical language and natural language can be introduced faster and more directly to better realize the end-to-end human–machine language translation task. In the study, the proposed Text2GraphQL task model is mainly constructed based on an improved pipeline composed of a Language Model, Pre-trained Adapter plug-in, and Pointer Network. This enables the model to copy objects' tokens from utterances, generate corresponding GraphQL statements for graph database retrieval, and builds an adjustment mechanism to improve the final output. And the experiments have proved that our proposed method has certain competitiveness on the counterpart datasets (Spider, ATIS, GeoQuery, and 39.net) converted from the Text2SQL task, and the proposed method is also practical in medical scenarios.\n",
      "Submitted_date: 2022-05-10\n",
      "DOI link: https://doi.org/10.1007/s10796-022-10295-0\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-8031-4_12\n",
      "PDF: None\n",
      "Title: A Natural Language Understanding Sequential Model for Generating Queries with Multiple SQL Commands\n",
      "Authors: Tharushi C. Sonnadara & Y. H. P. P. Priyadarshana\n",
      "Abstract: With the increasing amount of textual data and the growing demand for data-driven decision-making, automated SQL generation systems have gained significant attention in recent years. This paper presents GenSQL, a novel natural language sequential model for generating queries with multiple SQL commands. GenSQL is designed to simplify the query writing process for individuals who do not have advanced SQL knowledge, by allowing them to generate queries through natural language input. GenSQL consists of a template-based approach to handle various SQL commands and their respective parameters. Experimental results show that GenSQL achieves high accuracy in generating multi-command queries, outperforming other state-of-the-art models. The system has the potential to improve query generation efficiency, reduce errors, and improve accessibility for users who are not SQL specialists. The preliminary results show that the system has a 91.80% chance of producing accurate results. Ultimately, the goal of natural language text to SQL generation is to enable users to query databases more efficiently and accurately and to reduce the need for specialized skills in writing SQL queries.\n",
      "Submitted_date: 2024-02-23\n",
      "DOI link: https://doi.org/10.1007/978-981-99-8031-4_12\n",
      "Processing link: https://link.springer.com/article/10.1007/s10664-024-10602-0\n",
      "PDF: None\n",
      "Title: Towards an understanding of large language models in software engineering tasks\n",
      "Authors: Zibin Zheng, Kaiwen Ning, Qingyuan Zhong, Jiachi Chen, Wenqing Chen, Lianghong Guo, Weicheng Wang & Yanlin Wang\n",
      "Abstract: Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.\n",
      "Submitted_date: 2024-12-03\n",
      "DOI link: https://doi.org/10.1007/s10664-024-10602-0\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-78386-9_15\n",
      "PDF: None\n",
      "Title: Supporting the Development of Oracle APEX Low-Code Applications with Large Language Models\n",
      "Authors: Simon C. Gorissen & Wolf G. Beckmann\n",
      "Abstract: Large Language Models (LLMs) and low-code development platforms (LCDPs) have shown potential to fundamentally change the way how software products and applications are developed and to both combat the shortage of skilled software developers by increasing developer efficiency and lower the entry barrier for citizen developers to develop and maintain software on their own. This paper investigates whether these technologies can be combined to be even more powerful. To do this, a solution concept and prototype implementation were developed. Using the prototype, one can describe desired changes in an Oracle Application Express (APEX) low-code application to the GPT-4 Turbo LLM in a chat. The LLM then performs the changes for the user by calling specific edit functions that the system offers and generates replies to the user. This solution enables citizen developers to edit a fully functional Web application through natural language without help from a professional software developer. We also present a qualitative user study that we performed with ten Oracle APEX customers. It showed that participants have a rather positive opinion of both the fundamental concept and the prototype, liking aspects like its time-savings and ease of use. The study also uncovered some problems like a lack of a common vocabulary or technical understanding between the LLM and some users. However, participants already suggested ways to remedy such problems like integrating an element inspector into the prototype. Overall, the study highlights the feasibility and potential of the system and outlines multiple directions for its further development.\n",
      "Submitted_date: 2024-11-27\n",
      "DOI link: https://doi.org/10.1007/978-3-031-78386-9_15\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-83705-0_5\n",
      "PDF: None\n",
      "Title: Boosting the Querying Accuracy of Multi-Level Occupancy Data with Ontology-Guided LLMs\n",
      "Authors: Stefan Neubig, Rahul Radhakrishnan, Linus Göhl, Ronja Loges & Madalina Polgar\n",
      "Abstract: As overtourism and local overcrowding are becoming increasingly critical concerns, determining and predicting occupancy levels based on real-time data and predictive models that serve as a decision-making basis for necessary countermeasures are gaining popularity. Moreover, with the rise of large language models (LLMs), approaches that automate related data access have become tempting. However, real-world databases are often inherently complex and heterogeneously structured, complicating using LLM-based text-to-SQL. Previous studies report an accuracy of only 16%, which indicates the need for better approaches. This paper investigates how ontologies can support LLMs in increasing the accuracy of querying real-world databases. Based on the need to reduce overcrowding, we propose an ontology for modeling complex, multi-level occupancy data. Our ontology, based on previous work, is theoretically well-founded and compatible with existing tourism ontologies. In a case study based on a real-world database from Outdooractive, one of the largest European outdoor tourism platforms, we compare vanilla LLM-based text-to-SQL's performance with ontology-based data access. Our results show that the ontology-based approach almost triples the querying accuracy, which illustrates the effectiveness and potential of such semantic approaches.\n",
      "Submitted_date: 2025-05-27\n",
      "DOI link: https://doi.org/10.1007/978-3-031-83705-0_5\n",
      "Processing link: https://link.springer.com/article/10.1007/s12176-024-1464-2\n",
      "PDF: None\n",
      "Title: Künstliche Intelligenz im Controlling der Deutschen Telekom\n",
      "Authors: Dennis Koenigs\n",
      "Abstract: None\n",
      "Submitted_date: 2025-02-07\n",
      "DOI link: https://doi.org/10.1007/s12176-024-1464-2\n",
      "Processing link: https://link.springer.com/article/10.1007/s41870-023-01342-3\n",
      "PDF: None\n",
      "Title: A multilingual translator to SQL with database schema pruning to improve self-attention\n",
      "Authors: Marcelo Archanjo Jose\n",
      "Abstract: Databases have a large amount of information that can be accessed by the structured query language (SQL), but this language requires technical knowledge. An alternative to facilitating access to this information is to use natural language to make queries, and an artificial intelligence model to translate to SQL. Transformer-based language models have been incredibly successful in this regard. However, transformers are limited by the size of the input text; therefore, long sentences can interfere with the quality of the results. We present two techniques to improve results. The first is an innovative technique that allows long-text sequences to be handled by transformers with up to 512 input tokens. We run database schema pruning (removal of table names and column names that are useless for the query of interest) during a fine-tuning process. The second technique is a multilingual approach. The model is fine-tuned using a data-augmented Spider dataset [a specialized dataset for Natural Language to SQL (NL2SQL)] in four languages simultaneously: English, Portuguese, Spanish, and French. The combination of these techniques allowed an increase in the exact set match accuracy results from 0.718 to 0.736 in our validation dataset. The process of improving results is challenging because NL2SQL techniques are already significantly optimized, and the two techniques presented here are important because they are applied in the training dataset, allowing them to be used with any current technique. Source code, evaluations, and checkpoints are available at https://github.com/C4AI/gap-text2sql.\n",
      "Submitted_date: 2023-01-11\n",
      "DOI link: https://doi.org/10.1007/s41870-023-01342-3\n",
      "Processing link: https://link.springer.com/article/10.1007/s11042-022-12841-z\n",
      "PDF: None\n",
      "Title: Symbol question conversion in structured query language using fuzzy with deep attention based rain LSTM\n",
      "Authors: Preeti Tuli & Jyoti Prakash Patra\n",
      "Abstract: Effective communication between human and machine is achieved using Natural Language Processing (NLP). However, the users are not aware about the Structured Query Language (SQL) of the normal database (DB). Therefore, to make users more aware of SQL, a deep attention based rain LSTM (Long Short-Term Memory) is introduced in proposed process for developing the SQL query model. The input query of user is entered in natural language form which is initially pre-processed. Different steps are included in pre-processing they are Lower case conversion, Tokenization, Escape Word Removal and Parts Of speech (POS) tagger. After pre-processing stage, a novel fuzzy approach is introduced with the help of rain LSTM method. With the help of the feature matrix technique, the words of the pre-processed query are given a certain rank. The LSTM-ROA (Rain Optimization Algorithm) method is used for categorizing the words into relations, attributes and clauses on the basis of tagged elements. After removing ambiguous attributes, a final query is generated based on the extracted elements. The proposed method is implemented in the Python platform and the performances are analysed in terms of Accuracy, Precision, Recall and Error Rate. The proposed DA-RLSTM achieved 95.74% accuracy which illustrates that the proposed work has attained better result than the existing methods.\n",
      "Submitted_date: 2021-07-27\n",
      "DOI link: https://doi.org/10.1007/s11042-022-12841-z\n",
      "Processing link: https://link.springer.com/chapter/10.1007/979-8-8688-0968-2_6\n",
      "PDF: None\n",
      "Title: Building GenAI with SAP, Lambda, and Amazon Bedrock\n",
      "Authors: Miguel Figueiredo\n",
      "Abstract: www.com\n",
      "Submitted_date: 2025-02-20\n",
      "DOI link: https://doi.org/10.1007/979-8-8688-0968-2_6\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-3442-9_33\n",
      "PDF: None\n",
      "Title: Empowering Real-Time Insights Through LLM, LangChain, and SAP HANA Integration\n",
      "Authors: Md. Easin Arafat, Georgina Asuah & Tamas Orosz\n",
      "Abstract: Real-time integration within the SAP HANA (High-Performance Analytics Appliance) environment through open-source components signifies a substantial financial investment in information technology (IT) projects. The prevalence of budget overruns, delays in implementation, and imprecise forecasts remain significant factors contributing to project failures. This has led many companies to adopt the usage of real-time insights in planning and decision-making facilities. By keeping the fundamental components of SAP, businesses can upgrade their information systems without having to completely reconfigure them, which proves to be cost-effective. This study initiates an exploration of the intricate integration of large language model (LLM), LangChain, and SAP HANA, underlining the intrinsic value of real-time insights and their multi-faceted applications. The study's main objective is to provide individuals with a road map for utilizing these essential components, customized to their enterprise's specific phase of development. This study highlights how important this integration is and provides a strong push in the direction of operational effectiveness and long-term expansion. The study equally outlines the challenges and considerations of such components in operational effectiveness and long-term expansion. The integrated framework offers the following key benefits, linguistic precision, seamless language-technology integration, and robust analytics infrastructure. As the integrated framework develops, it becomes a revolutionary instrument that changes how real-time insights and decision-making are conducted across a variety of industries.\n",
      "Submitted_date: 2024-10-23\n",
      "DOI link: https://doi.org/10.1007/978-981-97-3442-9_33\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-77847-6_18\n",
      "PDF: None\n",
      "Title: Increasing the Accuracy of LLM Question-Answering Systems with Ontologies\n",
      "Authors: Dean Allemang & Juan Sequeda\n",
      "Abstract: There is increasing evidence that question-answering (QA) systems with Large Language Models (LLMs), which employ a knowledge graph representation of an enterprise SQL database (Text-to-SPARQL), achieve higher accuracy compared to systems that answer questions directly on SQL databases (Text-to-SQL). The objective of this research is to further improve the accuracy of these LLM Question Answering systems. Our approach, Ontology-based Query Check (OBQC), is to check the LLM generated SPARQL query against the semantics specified by the ontology. A query will be flagged as incorrect and prevented from execution if it does not align with the ontological semantics. The study also explores the LLM’s capability in repairing a SPARQL query given an explanation of the error (LLM Repair). Our methods are evaluated using the chat with the data benchmark. The primary finding is our method further increases the accuracy overall by 21.59% thus pushing the overall accuracy level to 65.63%. These results provide further evidence that investing knowledge graphs, namely the ontology, provides higher accuracy for LLM powered question answering systems. Our method is a component of the data.world AI Context Engine which is being widely used by customers in Generative AI production use cases that enable business users to chat with SQL databases.\n",
      "Submitted_date: 2024-11-27\n",
      "DOI link: https://doi.org/10.1007/978-3-031-77847-6_18\n",
      "Processing link: https://link.springer.com/article/10.1007/s10278-022-00692-x\n",
      "PDF: None\n",
      "Title: Application of Deep Learning in Generating Structured Radiology Reports: A Transformer-Based Technique\n",
      "Authors: Seyed Ali Reza Moezzi, Abdolrahman Ghaedi, Mojdeh Rahmanian & Ashkan Sami\n",
      "Abstract: Since radiology reports needed for clinical practice and research are written and stored in free-text narrations, extraction of relative information for further analysis is difficult. In these circumstances, natural language processing (NLP) techniques can facilitate automatic information extraction and transformation of free-text formats to structured data. In recent years, deep learning (DL)-based models have been adapted for NLP experiments with promising results. Despite the significant potential of DL models based on artificial neural networks (ANN) and convolutional neural networks (CNN), the models face some limitations to implement in clinical practice. Transformers, another new DL architecture, have been increasingly applied to improve the process. Therefore, in this study, we propose a transformer-based fine-grained named entity recognition (NER) architecture for clinical information extraction. We collected 88 abdominopelvic sonography reports in free-text formats and annotated them based on our developed information schema. The text-to-text transfer transformer model (T5) and Scifive, a pre-trained domain-specific adaptation of the T5 model, were applied for fine-tuning to extract entities and relations and transform the input into a structured format. Our transformer-based model in this study outperformed previously applied approaches such as ANN and CNN models based on ROUGE-1, ROUGE-2, ROUGE-L, and BLEU scores of 0.816, 0.668, 0.528, and 0.743, respectively, while providing an interpretable structured report.\n",
      "Submitted_date: 2021-11-16\n",
      "DOI link: https://doi.org/10.1007/s10278-022-00692-x\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-9247-8_46\n",
      "PDF: None\n",
      "Title: A Survey on the Integration of Blockchain Smart Contracts and Natural Language Processing\n",
      "Authors: Zikai Song, Pengxu Shen, Chuan Liu, Haoyu Gao & Hong Lei\n",
      "Abstract: Smart contract is an automated contract system based on blockchain technology, which is self-executing, tamper-evident and decentralized. The writing and analysis of smart contracts still face several challenges, including complex programming languages and potential security vulnerabilities. Natural Language Processing (NLP) as a discipline that studies the interaction between natural language and computers, can provide strong support for the development and analysis of smart contracts. This paper explores the cross-application of blockchain, smart contracts and NLP. First, this paper introduces the basic principles of blockchain technology and the concept of smart contracts. Then it points out the problems in the development process of smart contracts, and focuses on the analysis and summary of the relevant research results of NLP technology in the generation of smart contract code and annotation generation, and summarizes and analyzes the important role of NLP technology on the efficiency of smart contract development, the correctness, reliability, readability, and maintainability of the code. Secondly, for the security of smart contracts, the research related to smart contract vulnerability detection using NLP technology is summarized. Finally, the advantages, challenges and future development directions of combining natural language processing with blockchain smart contracts are pointed out to provide reference and inspiration for research and application in related fields.\n",
      "Submitted_date: 2024-01-04\n",
      "DOI link: https://doi.org/10.1007/978-981-99-9247-8_46\n",
      "Processing link: https://link.springer.com/article/10.1007/s00778-023-00831-y\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s00778-023-00831-y.pdf\n",
      "Title: DB-BERT: making database tuning tools “read” the manual\n",
      "Authors: Immanuel Trummer\n",
      "Abstract: DB-BERT is a database tuning tool that exploits information gained via natural language analysis of manuals and other relevant text documents. It uses text to identify database system parameters to tune as well as recommended parameter values. DB-BERT applies large, pre-trained language models (specifically, the BERT model) for text analysis. During an initial training phase, it fine-tunes model weights in order to translate natural language hints into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and prioritize hints to achieve optimal performance for a specific database system and benchmark. Both phases are iterative and use reinforcement learning to guide the selection of tuning settings to evaluate (penalizing settings that the database system rejects while rewarding settings that improve performance). In our experiments, we leverage hundreds of text documents about database tuning as input for DB-BERT. We compare DB-BERT against various baselines, considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run time), as well as database systems (PostgreSQL and MySQL). The experiments demonstrate clearly that DB-BERT benefits from combining general information about database tuning, mined from text documents, with scenario-specific insights, gained via trial runs. The full source code of DB-BERT is available online at https://itrummer.github.io/dbbert/.\n",
      "Submitted_date: 2023-01-31\n",
      "DOI link: https://doi.org/10.1007/s00778-023-00831-y\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-16-6605-6_20\n",
      "PDF: None\n",
      "Title: GenSQL—NLP-Based SQL Generation\n",
      "Authors: M. Sri Geetha, R. Yashwanthika, M. Sanjana Sri & M. Sudiksa\n",
      "Abstract: Relational databases saved the world’s facts in a considerable amount. The inability of insight of query languages limits the ability of the users to recapture the adequate knowledge. Here, natural language processing is enforced to relational databases. An NLP-based model is proposed to convert natural language utterances to SQL queries. Semantic parsing models find it difficult to derive to imaginary database schemas while convertion, which then retrieves corresponding results from the database. To handle schema encoding, schema look up and feature illustration within an encoder, the word uttered is taken as the input, and then sequence-sequence modelling takes place to show consideration to schema encoding based on the relation-aware self-attention system. The framework is evaluated on the two models based on the specific coordination and hardness norms of inquiry. The expected model upgrades the results of text-to-SQL (Yu et al. in Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases, pp. 1962–1979, 2019) task that is displayed on results of the Spider dataset.\n",
      "Submitted_date: 2022-01-17\n",
      "DOI link: https://doi.org/10.1007/978-981-16-6605-6_20\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-8193-5_6\n",
      "PDF: None\n",
      "Title: QueryAssist: Multimodal Verbal Specifications to Structured Query Conversion Model Using Word Vector-Based Semantic Analysis\n",
      "Authors: Surya Venkat Boddu, Rahul Aditya Butta, Geethan Sannidhi & Midhun Varma Yerakaraju\n",
      "Abstract: QueryAssist is a model designed to enhance communication with databases by transforming Telugu natural language queries, both text and speech, into SQL queries. Built for government schools of the Telugu-speaking states in India, this model utilizes Word Vectors for semantic analysis, ensuring accurate query generation. QueryAssist acts as an intuitive interface to interact with SQL databases, by addressing challenges faced by schools in accessing and utilizing data. Its standout features are its ability to comprehend Telugu queries and its error handling and refinement system. Through extensive experiments, QueryAssist has proven its effectiveness in transforming natural language queries into SQL commands. The model’s architecture, results, and its potential to improve the quality of decision-making processes within government schools are presented in this paper.\n",
      "Submitted_date: 2025-02-14\n",
      "DOI link: https://doi.org/10.1007/978-981-97-8193-5_6\n",
      "Processing link: https://link.springer.com/article/10.1007/s11390-024-4058-8\n",
      "PDF: None\n",
      "Title: A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models\n",
      "Authors: Yuan-Feng Song  (宋元峰), Yuan-Qin He  (何元钦), Xue-Fang Zhao  (赵雪芳), Han-Lin Gu  (古瀚林), Di Jiang  (姜 迪), Hai-Jun Yang  (杨海军) & Li-Xin Fan  (范力欣)\n",
      "Abstract: The springing up of large language models (LLMs) has shifted the community from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning paradigm. Along this line of research endeavors in the area, LLM-based prompting methods have attracted much attention, partially due to the technological advantages brought by prompt engineering (PE) as well as the underlying NLP principles disclosed by various prompting methods. Traditional supervised learning usually requires training a model based on labeled data and then making predictions. In contrast, PE methods directly use the powerful capabilities of existing LLMs (e.g., GPT-3 and GPT-4) via composing appropriate prompts, especially under few-shot or zero-shot scenarios. Facing the abundance of studies related to the prompting and the ever-evolving nature of this field, this article aims to 1) illustrate a novel perspective to review existing PE methods within the well-established communication theory framework, 2) facilitate a better/deeper understanding of developing trends of existing PE methods used in three typical tasks, and 3) shed light on promising research directions for future PE methods.\n",
      "Submitted_date: 2023-12-21\n",
      "DOI link: https://doi.org/10.1007/s11390-024-4058-8\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-9559-8_31\n",
      "PDF: None\n",
      "Title: Rules-Based Query Conversion for Cypher Query Language (Case Study: Cultural Heritage of Myanmar)\n",
      "Authors: Nang Nandar Tun\n",
      "Abstract: Culture plays an important role in a country. The Myanmar people have their own culture and literature. In the age of information, understanding and retrieving relevant information is of paramount importance. The special query language is required to retrieve the information in a semantic manner in order to apply on the huge web and database. The users need to know the semantic query to get the semantic results instead of Natural Language Query (NLQ). The existing information retrieval methodologies generally have problems with keyword-search cannot give actual search result. In this paper, we proposed rules-based query conversion for Cypher Query Language in Neo4j Graph Database. This system is developed in PHP (Hypertext Preprocessor) language. We consider Myanmar Cultural Heritage as a case study.\n",
      "Submitted_date: 2025-04-15\n",
      "DOI link: https://doi.org/10.1007/978-981-97-9559-8_31\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-658-46113-3_18\n",
      "PDF: None\n",
      "Title: Der Einfluss von Datenkompetenz, Feintuning und Transparenz auf Conversational Business Analytics\n",
      "Authors: Adem Alparslan\n",
      "Abstract: Business Analytics beschäftigt sich mit der Umwandlung von Daten in handlungsrelevante Informationen zur Entscheidungsunterstützung. Ein zentrales Konzept in diesem Bereich ist Self-Service Analytics, das es Endanwendern ermöglicht, eigenständig Informationen zu generieren und flexibel auf sich verändernde Informationsbedarfe zu reagieren. Fortschritte in der Künstlichen Intelligenz, insbesondere im Bereich der natürlichen Sprachverarbeitung, leiten eine neue Ära der eigenständigen Informationsgenerierung ein. Mit Conversational Business Analytics (CBA) wird die Möglichkeit eröffnet, dass Endanwender in natürlicher Sprache Fragen stellen, Analysen anfordern oder Berichte erhalten, ohne tiefgehendes technisches Wissen zu besitzen. Diese Studie untersucht das Potenzial und die Grenzen von CBA, wobei der Fokus auf dem Spannungsfeld zwischen der Datenkompetenz der Endanwender, dem Feintuning großer Sprachmodelle (als ein möglicher Ansatz zur Leistungssteigerung von Künstlicher Intelligenz) und der Notwendigkeit zur Schaffung von Transparenz liegt. Es wird gezeigt, dass neben der Effektivität der Informationsgenerierung auch die Schaffung von Transparenz eine zentrale Rolle spielt. Nur so können valide Informationen identifiziert und in der Entscheidungsunterstützung genutzt werden.\n",
      "Submitted_date: 2024-12-31\n",
      "DOI link: https://doi.org/10.1007/978-3-658-46113-3_18\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-86193-2_1\n",
      "PDF: None\n",
      "Title: WireLlama - Agentic Query System for PCAP Analysis\n",
      "Authors: Milind Choudhary & Parmeet Kaur\n",
      "Abstract: Network security researchers and practitioners constantly seek improved methods to manage and interpret vast amounts of network log data. The paper presents WireLlama, an Agentic Query system tailored for packet capture (PCAP) analysis. By leveraging the llama-agents and Scapy frameworks, WireLlama aims to enhance the capability of deriving insights from network traffic data. A pre-trained LLaMA model is utilized from Hugging Face for processing of user queries in natural language. These are translated into SQL and then the model then generates responses based on the SQL query and the preprocessed network data. The responses are also provided in natural language for easy comprehension by the user. As illustrated by results, the proposed system is successful in handling specific query functions, like identifying top IP addresses by traffic volume or detecting any suspicious activity.\n",
      "Submitted_date: 2025-03-31\n",
      "DOI link: https://doi.org/10.1007/978-3-031-86193-2_1\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-44201-8_33\n",
      "PDF: None\n",
      "Title: TableSF: A Structural Bias Framework for Table-To-Text Generation\n",
      "Authors: Di Liu, Weihua Wang, Feilong Bao & Guanglai Gao\n",
      "Abstract: Table-to-text generation is to generate a description from the tabular data. Existing methods typically encoded table content in a fixed order and relied heavily on the table row or column sequence. They generated error text descriptions when the row or column sequence changed. To solve the above problems, we proposed a novel structural bias framework that encodes tables using a modified self-attention mechanism. The framework captures the connectivity of cells in the same row or column through structural bias attention, distinguishing important cells from unimportant cells from a structural perspective. The structural bias attention will be added on top of the full self-attention, which can obtain the full structural information of the table. Experimental results show that this method generates better text descriptions on the public dataset and accomplishes a better understanding of the structured tables. This method not only obtains the relationship between cells but also improves the robustness of the pre-trained model.\n",
      "Submitted_date: 2023-09-23\n",
      "DOI link: https://doi.org/10.1007/978-3-031-44201-8_33\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-60626-7_17\n",
      "PDF: None\n",
      "Title: Generative Expression Constrained Knowledge-Based Decoding for Open Data\n",
      "Authors: Lucas Lageweg\n",
      "Abstract: In this paper, we present GECKO, a knowledge graph question answering (KGQA) system for data from Statistics Netherlands (Centraal Bureau voor de Statistiek). QA poses great challenges in means of generating relevant answers, as well as preventing hallucinations. This is a phenomenon found in language models and creates issues when attempting factual QA with these models alone. To overcome these limitations, the Statistics Netherlands’ publicly available OData4 data was used to create a knowledge graph, in which the answer generation decoding process is grounded, ensuring faithful answers. When processing a question, GECKO performs entity and schema retrieval, does schema-constrained expression decoding, makes assumptions where needed and executes the generated expression as an OData4 query to retrieve information. A novel method was implemented to perform the constrained knowledge-based expression decoding using an encoder-decoder model. Both a sparse and dense entity retrieval method were evaluated. While the encoder-decoder model did not achieve production-ready performance, experiments show promising results for a rule-based baseline using a sparse entity retriever. Additionally, the results of qualitative user testing were positive. We therefore formulate recommendations for deployment help guide users of Statistics Netherlands data to their answers more quickly.\n",
      "Submitted_date: 2024-05-19\n",
      "DOI link: https://doi.org/10.1007/978-3-031-60626-7_17\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-45043-3_7\n",
      "PDF: None\n",
      "Title: Interactivity\n",
      "Authors: Yunyao Li\n",
      "Abstract: In this chapter, we discuss the top of interactivity, a key element of NLIDB impacting its effectiveness and usefulness. We introduce different dimensions of interactivity, present the corresponding techniques to support each dimension in detail, and discuss their benefits and corresponding challenges. We first focus on interactivity at query time and discuss methods for query disambiguation and query suggestions. We then switch focus to interactivity after the initial query and discuss automatic data insights and explanation techniques for different aspects of NLIDBs. Finally, we present how different interactive techniques discussed in the chapter may be combined together to provide conversational NLIDBs and multi-modal NLIDBs where user input is beyond natural language.\n",
      "Submitted_date: 2023-11-25\n",
      "DOI link: https://doi.org/10.1007/978-3-031-45043-3_7\n",
      "Processing link: https://link.springer.com/chapter/10.1007/979-8-8688-0968-2_5\n",
      "PDF: None\n",
      "Title: Getting Started with GenAI Using SAP BTP and Amazon Bedrock\n",
      "Authors: Miguel Figueiredo\n",
      "Abstract: In this chapter, we will explore integrate a GenAI large language model (LLM) into Amazon SageMaker Notebook to improve the user’s experience using business data stored in SAP HANA Cloud. We will use Amazon Bedrock, the Python language, and Langchain.\n",
      "Submitted_date: 2025-02-20\n",
      "DOI link: https://doi.org/10.1007/979-8-8688-0968-2_5\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-7545-7_44\n",
      "PDF: None\n",
      "Title: Research on Cross-Domain Text2SQL Enhancing Database Schema Awareness\n",
      "Authors: Jianyun Lei, Zishan Wang, Chong Sun & Zejin Zhang\n",
      "Abstract: Text2SQL has become an important research direction of natural language processing because of its abundant practical scenarios and research value. RAT-SQL is a Text2SQL method based on relational self-attention, which solves the domain generalization problem by improving the schema encoding, schema linking and feature representations in the encoder. However, the approach only focuses on adding the known relation representation into self-attention to achieve alignment between entities and schema items, which cannot well fuse external heterogeneous information. The ability of the parser to generalize to invisible target domains remains limited. To address this problem, this paper proposes a new approach for Text2SQL tasks that injects factual relations within domain knowledge and extends some of the capabilities of RAT-SQL by jointly encoding natural language questions, database schemas and exogenous knowledge. Experiments show that the accuracy of EDSA-SQL is significantly improved compared to RAT-SQL on the publicly available dataset Spider.\n",
      "Submitted_date: 2024-03-23\n",
      "DOI link: https://doi.org/10.1007/978-981-99-7545-7_44\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-54827-7_13\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/978-3-031-54827-7.pdf\n",
      "Title: LLM Adoption Trends and Associated Risks\n",
      "Authors: Zachary Schillaci\n",
      "Abstract: The emergence of Large Language Models (LLMs) is expected to impact the job market significantly, accelerating automation trends and posing a risk to traditionally creative-oriented jobs. LLMs can automate tasks in various fields, including design, journalism, and creative writing. Companies and public institutions can leverage generative models to enhance productivity and reduce workforce requirements through machine-assisted workflows and natural language interactions. While technical skills like programming may become less important in certain roles, generative models are unlikely to fully replace programmers due to the need for expertise in code validation and niche development. The enterprise landscape of LLMs comprises providers (organizations training proprietary models), integrators (technology companies fine-tuning LLMs for specific applications), and users (companies and individuals adopting LLM-powered solutions). The applications of the models include conversational search, customer service chatbots, content creation, personalized marketing, data analysis, and basic workflow automation. The regulatory landscape is rapidly evolving, with key considerations including copyright, data security, and liability. Government involvement and informed expertise are recommended to guide governance and decision-making processes in this domain.\n",
      "Submitted_date: 2024-04-12\n",
      "DOI link: https://doi.org/10.1007/978-3-031-54827-7_13\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-62495-7_12\n",
      "PDF: None\n",
      "Title: Enhancing Natural Language Query to SQL Query Generation Through Classification-Based Table Selection\n",
      "Authors: Ankush Chopra & Rauful Azam\n",
      "Abstract: In recent years, the convergence of natural language processing (NLP) and large language models (LLMs) has propelled the development of solutions enabling users to interact seamlessly with structured databases using natural language queries (NLQs). Existing NLQ-to-SQL models primarily approach this as a translation problem, converting NLQs into SQL queries for database interaction. However, challenges arise when dealing with extensive databases containing numerous tables, necessitating a robust approach for table selection to improve the efficiency of downstream NLQ-to-SQL models.\n",
      "This paper introduces a classification-based method for table selection, addressing limitations in existing embedding-based approaches. By predicting the necessity of tables in query formulation, the proposed approach offers a more meaningful interpretation of model scores, facilitating the determination of a universal threshold for table selection. To validate this innovative approach, a custom dataset was curated, leveraging the Spider dataset for NLQ-to-SQL tasks, and a comprehensive set of experiments was conducted using various language models, including GPT-4, GPT-3.5, and DeBERTa.\n",
      "Results demonstrate the effectiveness of the fine-tuned DeBERTa model in consistently outperforming other models across key metrics, showcasing its advancements in table selection tasks. This research not only addresses the challenge of context length in NLQ-to-SQL models but also highlights the potential of smaller LLMs when fine-tuned for specific tasks. The proposed classification-based approach offers a practical solution for improving the accuracy and efficiency of NLQ-to-SQL models, paving the way for enhanced interactions between users and structured databases.\n",
      "Submitted_date: 2024-06-22\n",
      "DOI link: https://doi.org/10.1007/978-3-031-62495-7_12\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-48189-5_4\n",
      "PDF: None\n",
      "Title: Continuous and Interactive Language Learning and Grounding\n",
      "Authors: Sahisnu Mazumder\n",
      "Abstract: Many task-oriented chatbots and virtual assistants like Siri, Alexa, and Google Assistant are built as Natural Language (command) Interfaces (NLIs) that allow users to issue natural language (NL) commands to be mapped to some actions for execution in the underlying application in order to accomplish some tasks intended by the users. A fundamental feature of such systems is the ability to understand users’ language and ground them to intended actions (often in symbolic form). Due to their diverse and wide-spread real-world applications, such NLI systems have driven research in language understanding, grounding and human-robot interactions over the years. This chapter discusses the scope for continual and interactive language learning in the context of NLIs and introduces some of the representative works along this direction.\n",
      "Submitted_date: 2024-01-09\n",
      "DOI link: https://doi.org/10.1007/978-3-031-48189-5_4\n",
      "Processing link: https://link.springer.com/article/10.1038/s41597-024-03605-5\n",
      "PDF: https://link.springer.com/content/pdf/10.1038/s41597-024-03605-5.pdf\n",
      "Title: ENTRANT: A Large Financial Dataset for Table Understanding\n",
      "Authors: Elias Zavitsanos, Dimitris Mavroeidis, Eirini Spyropoulou, Manos Fergadiotis & Georgios Paliouras\n",
      "Abstract: Tabular data is a way to structure, organize, and present information conveniently and effectively. Real-world tables present data in two dimensions by arranging cells in matrices that summarize information and facilitate side-by-side comparisons. Recent research efforts aim to train large models to understand structured tables, a process that enables knowledge transfer in various downstream tasks. Model pre-training, though, requires large datasets, conveniently formatted to reflect cell and table characteristics. This paper presents ENTRANT, a financial dataset that comprises millions of tables, which are transformed to reflect cell attributes, as well as positional and hierarchical information. Hence, they facilitate, among other things, pre-training tasks for table understanding with deep learning methods. The dataset provides table and cell information along with the corresponding metadata in a machine-readable format. We have automated all data processing and curation and technically validated the dataset through unit testing of high code coverage. Finally, we demonstrate the use of the dataset in a pre-training task of a state-of-the-art model, which we use for downstream cell classification.\n",
      "Submitted_date: 2024-02-23\n",
      "DOI link: None\n",
      "Processing link: https://link.springer.com/article/10.1007/s10791-025-09582-6\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s10791-025-09582-6.pdf\n",
      "Title: Few-shot controlled dialogue generation using in-context learning\n",
      "Authors: Zhongqin Bi, Xueni Hu, Weina Zhang & Xiaoyu Wang\n",
      "Abstract: Annotated dialogue datasets are crucial for training models related to task-oriented systems, but such datasets are often scarce. Despite the existence of automated approaches for generating dialogue data, generating high-quality dialogue and accurate annotation remains a challenge. To address this issue, we propose Few-Shot Controlled Dialogue Generation Using In-Context Learning (ICI-CDG). It uses turn-level dialogue retrieval to enhance the in-context learning ability of large language models, enabling the rapid and automatic generation of high-quality controllable dialogues. The ICI-CDG consists of three modules: goal generation, turn-level dialogue retrieval, and state matching filtering. The goal generation generates new dialogue overall goals by randomly sampling from diverse dialogues, and these goals provide a clear direction and framework for the generation of new dialogues. The turn-level dialogue retrieval searches for turns with higher similarity as prompt, improving generated dialogue quality. The state matching filtering looks for generated content corresponding to the turn goals to reduce the semantic deviation between the annotation and the dialogue. The experimental results on the two datasets show that our method generates more natural dialogues with more accurate annotations, outperforming existing methods in few-shot settings.\n",
      "Submitted_date: 2024-11-30\n",
      "DOI link: https://doi.org/10.1007/s10791-025-09582-6\n",
      "Processing link: https://link.springer.com/article/10.1007/s10009-025-00798-x\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s10009-025-00798-x.pdf\n",
      "Title: LLM-based code generation and system migration in language-driven engineering\n",
      "Authors: Daniel Busch, Alexander Bainczyk, Steven Smyth & Bernhard Steffen\n",
      "Abstract: This paper illustrates the power of extending Language Driven Engineering (LDE) with Domain-Specific Natural Languages (DSNLs) through a case study on two levels. Both cases benefit from the characteristic decomposition feature of LDE, resulting in tasks tailored to the application of domain-specific languages, here with a focus on the application of DSNLs supported by LLM-based code generation. In the first case study, we show how DSNL-supported LDE facilitates the development of point-and-click adventures, whereas the second case study focuses on migration: We demonstrate how the entire LDE scenario for point-and-click adventure games can be migrated to output TypeScript instead of JavaScript using LLM-based code generation exclusively, without manually writing any code. This migration not only infers the required types, but also preserves an important property of the original LDE scenario: generated web applications can be automatically validated by design via automata learning and subsequent model checking. Even better, this property can be exploited to automatically validate the correctness of the migration by learning so-called difference automata that characterize the behavioral differences between the generated JavaScript-based and Type-Script-based applications.\n",
      "Submitted_date: 2025-04-04\n",
      "DOI link: https://doi.org/10.1007/s10009-025-00798-x\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-43458-7_42\n",
      "PDF: None\n",
      "Title: Semantic Parsing for Knowledge Graph Question Answering with Large Language Models\n",
      "Authors: Debayan Banerjee\n",
      "Abstract: This thesis explores the topic of Knowledge Graph Question Answering with a special emphasis on semantic parsing approaches, incorporating pre-trained text-to-text language models. We use the text generation ability of these models to convert natural language questions to logical forms. We test whether correct logical forms are being generated, and if not, how to mitigate the failure cases. As a second step, we try to make the same models generate additional information to aid the process of grounding of the logical forms to entities, relations and literals in the Knowledge Graph. In experiments conducted so far, we see encouraging results on both generation of base logical forms, and grounding them to the KG elements. At the same time, we discover failure cases prompting directions in future work (The author considers himself a ‘middle-stage’ Ph.D. candidate).\n",
      "Submitted_date: 2023-10-21\n",
      "DOI link: https://doi.org/10.1007/978-3-031-43458-7_42\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-8702-9_3\n",
      "PDF: None\n",
      "Title: Hierarchical Attention Decoder for Solving Math Word Problems\n",
      "Authors: Jiajia Li & Ping Wang\n",
      "Abstract: To answer math word problems (MWPs), models must formalize equations from the source text of math problems. Recently, the tree-structured decoder has significantly improved model performance on this task by generating the target equation in a tree format. However, current decoders usually ignore the hierarchical relationships between tree nodes and their parents, which hinders further improvement. Thus, we propose a structure called a hierarchical attention tree to aid the generation procedure of the decoder. As our decoder follows a graph-based encoder, our full model is therefore named Graph to Hierarchical Attention Tree (G2HAT). We show that a tree-structured decoder with hierarchical accumulative multi-head attention leads to a significant performance improvement and reaches a significant improvement on various strong baselines on both English MAWPS and Chinese Math23k MWP benchmarks. For further study, we also apply a pre-trained language model to G2HAT, which even results in a new higher performance.\n",
      "Submitted_date: 2025-02-08\n",
      "DOI link: https://doi.org/10.1007/978-981-97-8702-9_3\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-85356-2_8\n",
      "PDF: None\n",
      "Title: MDE in the Era of Generative AI\n",
      "Authors: Ahmed Alaoui Mdaghri\n",
      "Abstract: Domain-Specific Languages (DSLs) play a vital role in software development, enabling the concise expression of domain-specific concepts and requirements. In this study, we propose a novel approach leveraging Large Language Models (LLMs) to assist the DSLs modelling starting from natural language description. Our solution is a proof of concept where Model Driven Engineering (MDE) is revisited taking advantage from the power of generative AI. Starting from human friendly description and domain modelling language document type, LLM-based system extracts relevant domain knowledge and builds the corresponding DSL model. Such a model is then validated through an iterative process. We applied our proposal to several case studies from different application domains including software engineering, healthcare, and finance. Furthermore, we consider a wide range of existing LLMs usually adapted for code generation. We also study the effectiveness of our solution through multi-criteria experimental evaluation. Lastly, the results demonstrate the feasibility and efficiency of our LLM driven MDE for DSL development, and then advancing domain-specific modelling practices. By doing so, we would enable the developer to save time and effort for further tasks such as functional properties’ verification. A demo as well as a web application for our developed solution are available online via the following link https://alaouimdaghriahmed.github.io/demo-ecore-gen/.\n",
      "Submitted_date: 2025-04-17\n",
      "DOI link: https://doi.org/10.1007/978-3-031-85356-2_8\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-6684-0_7\n",
      "PDF: None\n",
      "Title: LlaMA2 and Django Web Application Approach for Simplifying Table Parsing: Review\n",
      "Authors: Sagar Shinde, Aparna Pande, Ashay Chaudhari & Ketan More\n",
      "Abstract: The Proposed work revolutionizes the data analytics field by implementing a Large Language Model (LLM) in a web application through which data analysis can be done easily. From the very beginning, the methodologies for data analysis are traditional methodologies such that one has to have expertise in those methodologies. But to overcome this barrier the Proposed work-based Question Answering (TabQA) application Plays a Pivotal role as it completely simplifies the data analysis methodology using the LLM model Large Language Model Meta AI (LlaMA2) by META as this model holds strong potential to analyze tabular data efficiently as it is trained on very vast data. Also, for user authentication JSON Web Token (JWT) and Google authentication are added up as a security layer. Many Businesses need these types of solutions and TabQA holds strong Potential to be scaled to enterprise-level applications. For this TabQA provides API to developers to fulfill their business needs. For fast and secure implementation Django plays an important role as a backend framework apart from that Postgre Database handles the Database management and can be deployed on the Heroku cloud for fast usage of TabQA.\n",
      "Submitted_date: 2024-12-27\n",
      "DOI link: https://doi.org/10.1007/978-981-97-6684-0_7\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-42508-0_15\n",
      "PDF: None\n",
      "Title: Removing Ambiguity in Natural Language for Generating Self-Join Queries\n",
      "Authors: Pradnya Sawant & Kavita Sonawane\n",
      "Abstract: Databases systems are used almost everywhere in modern life and a prior knowledge of query languages like SQL is required to interact with these systems. Generally, relational databases are very useful for storing a significant amount of the world’s data. But the user must know a query language to access this data efficiently. Nowadays, Natural Language Processing is used in almost every field of human-to-machine interaction. Mapping natural language into the form of structured query Language (SQL) has many benefits. An interface that uses natural language processing allows the user to interact with the relational database as if the communication is happening between human beings. There are different models available for mapping the natural language to SQL addressing various types of SQL queries. But some of the SQL queries like self join where the table needs to join with itself for generating the results, can not be handled by these existing state-of-the-art models as they require some common sense knowledge. And because of these ambiguities in natural language, converting natural language to SQL becomes a complex task. In this paper, a novel approach using word sense disambiguation, for understanding the context of the natural language question addressing self-join queries is being proposed. This proposed approach can be integrated with existing text to SQL models to understand the natural language questions which require common sense knowledge.\n",
      "Submitted_date: 2023-09-14\n",
      "DOI link: https://doi.org/10.1007/978-3-031-42508-0_15\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-1256-8_23\n",
      "PDF: None\n",
      "Title: INSL: Text2SQL Generation Based on Inverse Normalized Schema Linking\n",
      "Authors: Tie Jun, Fan Ziqi, Sun Chong, Zheng Lu & Zhu Boer\n",
      "Abstract: Structured Query Language (SQL) is a query language widely used in databases, Text2SQL automatically parses natural language into SQL, which has great potential to facilitate non-expert users to query and mine structured data using natural language. Current research focuses on improving the matching accuracy of SQL clause tasks, but ignores the correctness of SQL syntax generation, and SQL generation involving multi-table joins still suffers from a large number of errors. Therefore, a neural network-based Text2SQL approach is proposed. To implement a practical Text2SQL workflow, the model associates natural language queries with an inverse normalized database schema, called INSL (Inverse Normalized Schema Link Generation Network). Through theoretical analysis and experimental validation on the public dataset Spider, INSL can effectively improve the quality of Text2SQL tasks.\n",
      "Submitted_date: 2023-04-02\n",
      "DOI link: https://doi.org/10.1007/978-981-99-1256-8_23\n",
      "Processing link: https://link.springer.com/article/10.1007/s11704-024-40231-1\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s11704-024-40231-1.pdf\n",
      "Title: A survey on large language model based autonomous agents\n",
      "Authors: Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei & Jirong Wen\n",
      "Abstract: Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.\n",
      "Submitted_date: 2024-03-05\n",
      "DOI link: https://doi.org/10.1007/s11704-024-40231-1\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-08421-8_12\n",
      "PDF: None\n",
      "Title: A Neural-Machine-Translation System Resilient to Out of Vocabulary Words for Translating Natural Language to SPARQL\n",
      "Authors: Manuel Borroto, Francesco Ricca & Bernardo Cuteri\n",
      "Abstract: The development and diffusion of ontologies allowed the creation of large banks of information regarding multiple domains known as knowledge bases. Ontologies propose a way to represent information providing semantic meaning that allows the data to be machine-interpretable. However, enjoying such rich knowledge is a difficult task for the majority of potential users who do not know either the knowledge-base definition or how to write queries with SPARQL. Systems able to translate natural language questions into SPARQL queries have the potential to overcome this problem. In this paper, we propose an approach that combines the Named Entity Recognition and Neural Machine Translation tasks to perform an automatic translation of natural language questions into executables SPARQL queries. The resulting approach provides robustness to the presence of terms that do not occur in the training set. We evaluate the potential of our approach by using Monument and QALD-9, which are well-known datasets for Question Answering over the DBpedia ontology.\n",
      "Submitted_date: 2022-07-19\n",
      "DOI link: https://doi.org/10.1007/978-3-031-08421-8_12\n",
      "Processing link: https://link.springer.com/article/10.1007/s11704-024-40663-9\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s11704-024-40663-9.pdf\n",
      "Title: A survey on LoRA of large language models\n",
      "Authors: Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu & Yunjun Gao\n",
      "Abstract: Low-Rank Adaptation (LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRA’s performance on downstream tasks; (2) cross-task generalization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this survey also discusses the future directions in this field.\n",
      "Submitted_date: 2024-07-02\n",
      "DOI link: https://doi.org/10.1007/s11704-024-40663-9\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-47112-4_8\n",
      "PDF: None\n",
      "Title: Using Knowledge Graphs to Generate SQL Queries from Textual Specifications\n",
      "Authors: Robson A. Campêlo\n",
      "Abstract: In this paper, we present a tool for querying relational DBs that uses a KG as an approach to generate SQL queries from NL specifications. In this approach, we argue that a KG representation of a relational DB schema can become an auxiliary tool in the translation process. Furthermore, we propose to automate the process of generating such a KG. Our approach to provide an NL interface for relational DBs comprises two major tasks: (1) generation of a KG from a relational DB schema and (2) translation of NL queries to SQL based on the semantics provided by the respective KG. We study the effectiveness of our approach using a benchmark dataset containing 82 NL query examples from the Spider dataset, considering the domain of Formula 1. Our approach is able to correctly translate these queries, which is verified against the expected results provided by our benchmark.\n",
      "Submitted_date: 2023-10-26\n",
      "DOI link: https://doi.org/10.1007/978-3-031-47112-4_8\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-49011-8_37\n",
      "PDF: None\n",
      "Title: Source-Code Generation Using Deep Learning: A Survey\n",
      "Authors: Areeg Ahmed, Shahira Azab & Yasser Abdelhamid\n",
      "Abstract: In recent years, the need for writing effective, reusable, and high-quality source code has grown exponentially. Writing source code is an integral part of building any software system; the development phase of the software lifecycle contains code implementation, refactoring, maintenance, and fixing bugs. Software developers implement the desired solution by turning the system requirements into viable software products. For the most part, the implementation phase can be challenging as it requires a certain level of problem-solving skills and the ability to produce high-quality outcomes without decreasing productivity rates or not meeting the business plans and deadlines. Programmers’ daily tasks might also include writing large amounts of repetitive boilerplate code, which can be tedious, not to mention the potential bugs that could arise from human errors during the development process. The ability to automatically generate source code will save significant time and effort invested in the software development process by increasing the speed and efficiency of software development teams. In this survey, we review and summarize the recent studies on deep learning approaches used to generate source code in different programming languages such as Java, Python, and SQL (Structured Query Language). We categorize the surveyed work into two groups, Natural Language-based solutions for approaches that use natural text as input and Computer Vision-based solutions which generate code based on images as input.\n",
      "Submitted_date: 2023-12-15\n",
      "DOI link: https://doi.org/10.1007/978-3-031-49011-8_37\n",
      "Processing link: https://link.springer.com/article/10.1007/s41060-024-00643-5\n",
      "PDF: None\n",
      "Title: Utilizing structural metrics from knowledge graphs to enhance the robustness quantification of large language models\n",
      "Authors: Mohd Ariful Haque, Roy George & Kishor Datta Gupta\n",
      "Abstract: Knowledge graphs (KGs) play a critical role in organizing large stores of unstructured information into structured formats. This structured information is then accessible through SPARQL queries or graph libraries based on their structure. KGs enhance search, power AI systems, and facilitate knowledge discovery across domains. In this research, we explore the capabilities of different large language models (LLMs) like CodeLlama, Mistral, and Vicuna, which are recognized for text generation, in handling textual information tasks for constructing knowledge graphs with structured data. Utilizing these LLMs, we generate class descriptions for all the classes of well-known KGs like DBpedia, YAGO, and Google Knowledge Graph. Using these class descriptions, we have extracted RDF triples and used different preprocessing techniques for better refinement and extraction of the graph triples from the generated result. These extracted triples are used for the graph ontology creation. Highlighting the contribution of LLMs to structured graph formation, our study includes a comparison of the constructed KGs using the three LLMs with the existing Knowledge Graphs. Later, these KGs are evaluated using six structural quality metrics encompassing both class and property-related information crucial for KG formation. Our insights prove valuable for researchers exploring these domains, offering guidance on overcoming challenges and maximizing the potential of large language models in knowledge graph construction, text generation, and text extraction.\n",
      "Submitted_date: 2024-04-30\n",
      "DOI link: https://doi.org/10.1007/s41060-024-00643-5\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-88244-0_10\n",
      "PDF: None\n",
      "Title: COMBINE: A Pipeline for SQL Generation from Natural Language\n",
      "Authors: Youssef Mellah, Abdelkader Rhouati & El Hassane Ettifouri\n",
      "Abstract: Accessing data stored in relational databases requires an understanding of the database schema and mainly a query language such as SQL, which, while powerful, is difficult to master. In this sense, recent researches try to approach systems to facilitate this task, in particular by making Text-to-SQL models that attempt to map a question in Natural Language (NL) to the corresponding SQL query. In this paper, we present COMBINE, a pipeline for SQL generation from NL, in which we combine two existing models, RATSQL (We used the version RATSQL v3+BERT; paper’s url: arxiv.org/abs/1911.04942.) and BRIDGE (We used the version BRIDGE v1+BERT; paper’s url: aclweb.org/anthology/2020.findings-emnlp.438/.), that are based on recent advances in Deep Learning (DL) for Natural Language Processing (NLP). Our model is evaluated on the Spider challenge, using Exact Matching Accuracy (EMA) and Execution Accuracy (EA) metrics. Our experimental evaluation demonstrates that COMBINE outperforms the two used models in the same challenge, and at the time of writing, achieving the state of the art in EA with 70%, and competitive result in EMA with 71.4%, on Spider Dev Set.\n",
      "Submitted_date: 2021-10-21\n",
      "DOI link: https://doi.org/10.1007/978-3-030-88244-0_10\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-68323-7_22\n",
      "PDF: None\n",
      "Title: Creating and Querying Data Cubes in Python Using PyCube\n",
      "Authors: Sigmundur Vang, Christian Thomsen & Torben Bach Pedersen\n",
      "Abstract: Data cubes are used for analyzing large data sets usually contained in data warehouses. The most popular data cube tools use graphical user interfaces (GUI) to do the data analysis. Traditionally this was necessary since data analysts were not expected to be technical people. However, in the subsequent decades the data landscape changed dramatically requiring companies to employ large teams of highly technical data scientists in order to manage and use the ever increasing amount of data. These data scientists generally use tools like Python, interactive notebooks, pandas, etc. while modern data cube tools are still GUI based. To bridge this gap, this paper proposes a Python-based data cube tool called pyCube. pyCube is able to semi-automatically create data cubes for data stored in an RDBMS and manages the data cube metadata. pyCube’s programmatic interface enables data scientists to query data cubes by specifying the metadata of the desired result. pyCube is experimentally evaluated on Star Schema Benchmark (SSB). The results show that pyCube vastly outperforms different implementations of SSB queries in pandas in both runtime and memory while being easier to read and write.\n",
      "Submitted_date: 2024-08-18\n",
      "DOI link: https://doi.org/10.1007/978-3-031-68323-7_22\n",
      "Processing link: https://link.springer.com/article/10.1007/s10618-024-01039-6\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s10618-024-01039-6.pdf\n",
      "Title: Modeling the impact of out-of-schema questions in task-oriented dialog systems\n",
      "Authors: Jannat Ara Meem, Muhammad Shihab Rashid & Vagelis Hristidis\n",
      "Abstract: Existing work on task-oriented dialog systems generally assumes that the interaction of users with the system is restricted to the information stored in a closed data schema. However, in practice users may ask ‘out-of-schema’ questions, that is, questions that the system cannot answer, because the information does not exist in the schema. Failure to answer these questions may lead the users to drop out of the chat before reaching the success state (e.g. reserving a restaurant). A key challenge is that the number of these questions may be too high for a domain expert to answer them all. We formulate the problem of out-of-schema question detection and selection that identifies the most critical out-of-schema questions to answer, in order to maximize the expected success rate of the system. We propose a two-stage pipeline to solve the problem. In the first stage, we propose a novel in-context learning (ICL) approach to detect out-of-schema questions. In the second stage, we propose two algorithms for out-of-schema question selection (OQS): a naive approach that chooses a question based on its frequency in the dropped-out conversations, and a probabilistic approach that represents each conversation as a Markov chain and a question is picked based on its overall benefit. We propose and publish two new datasets for the problem, as existing datasets do not contain out-of-schema questions or user drop-outs. Our quantitative and simulation-based experimental analyses on these datasets measure how our methods can effectively identify out-of-schema questions and positively impact the success rate of the system.\n",
      "Submitted_date: 2023-12-06\n",
      "DOI link: https://doi.org/10.1007/s10618-024-01039-6\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-7224-1_24\n",
      "PDF: None\n",
      "Title: LLM-Based SPARQL Generation with Selected Schema from Large Scale Knowledge Base\n",
      "Authors: Shuangtao Yang, Mao Teng, Xiaozheng Dong & Fu Bo\n",
      "Abstract: Knowledge base question answering (KBQA) aims to answer natural language questions using structured knowledge bases. Common approaches include semantic parsing-based approaches and retrieval-based approaches. However, both approaches have some limitations. Retrieval-based methods struggle with complex reasoning requirements. Semantic parsing approaches have a complex reasoning process and cannot tolerate errors in earlier steps when generating the final logical form. In this paper, we proposed a large language model (LLM)-based SPARQL generation model, which accepts multiple candidate entities and relations as inputs, reducing the reliance on mention extraction and entity linking performance, and we found an entity combination strategy based on mentions, which can produce multiple SPARQL queries for a single question to boost the chances of finding the correct answer. Finally, our model achieves state-of-the-art performance in the CCKS2023 CKBQA competition, F1 score is 75.63%.\n",
      "Submitted_date: 2023-10-28\n",
      "DOI link: https://doi.org/10.1007/978-981-99-7224-1_24\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-19-8746-5_13\n",
      "PDF: None\n",
      "Title: Natural Language Query for Technical Knowledge Graph Navigation\n",
      "Authors: Ziyu Zhao, Michael Stewart, Wei Liu, Tim French & Melinda Hodkiewicz\n",
      "Abstract: Technical knowledge graphs are difficult to navigate. To support users with no coding experience, one can use traditional structured HTML form controls, such as drop-down lists and check-boxes, to construct queries. However, this requires multiple clicks and selections. Natural language queries, on the other hand, are more convenient and less restrictive for knowledge graphs navigation. In this paper, we propose a system that enables natural language queries against technical knowledge graphs. Given an input utterance (i.e., a query in human language), we first perform Named Entity Recognition (NER) to identify domain specific entity mentions as node names, entity types as node labels, and question words (e.g., what, how many and list) as keywords of a structured query language before the rule-based formal query constructions. Three rules are exploited to generate a valid structured formal query. The web-based interactive application is developed to help maintainers access industrial maintenance knowledge graph which is constructed from text data.\n",
      "Submitted_date: 2022-12-05\n",
      "DOI link: https://doi.org/10.1007/978-981-19-8746-5_13\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-4626-6_66\n",
      "PDF: None\n",
      "Title: NL2SQL: Rule-Based Model for Natural Language to SQL\n",
      "Authors: Kevin Tony, Kripa Susan Shaji, Nijo Noble, Ruben Joseph Devasia & Neethu Chandrasekhar\n",
      "Abstract: Every day, in today’s fast-paced, digital world, an enormous amount of data is generated. An intelligent and creative interface is required to organize and engage with massive amounts of new data. Traditional techniques rely on sequence-to-sequence models; however, such models are too generic to take advantage of the whole structure of a SQL query. Furthermore, the knowledge of SQL is not widespread among the common people. We investigate and build a rule-based approach for transforming natural language to SQL queries in this work, which also includes a voice input capability. Our approach involves parsing the natural language statement to extract the relevant keywords, mapping them to a semantic representation, and generating the corresponding SQL query using a set of pre-defined rules. The rules are designed to map natural language verbs, nouns, and phrases to SQL operators, tables, columns, and clauses. ...\n",
      "Submitted_date: 2023-10-11\n",
      "DOI link: https://doi.org/10.1007/978-981-99-4626-6_66\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-53970-2_17\n",
      "PDF: None\n",
      "Title: SQL Generation from Natural Language Using Supervised Learning and Recurrent Neural Networks\n",
      "Authors: Youssef Mellah, El Hassane Ettifouri, Abdelkader Rhouati & Walid Dahhane\n",
      "Abstract: Databases store a vast amount of today’s data and information, and to access that data users are required to have command over SQL or equivalent interface language. Hence, using a system that can convert a natural language to equivalent SQL query would make the data more accessible. In this sense, building natural language interfaces to relational databases is an important and challenging problem in natural language processing (NLP) and a widely studied field, and found recently momentum again due to the introduction of large-scale Datasets. In this paper, we present our approach based on word embedding and Recurrent Neural Networks (RNN), precisely on Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) cells. We present also the DataSet used for training and testing our models, based on WikiSQL, and finally we show where we arrived in terms of accuracy.\n",
      "Submitted_date: 2020-07-19\n",
      "DOI link: https://doi.org/10.1007/978-3-030-53970-2_17\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-57259-3_11\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/978-3-031-57259-3.pdf\n",
      "Title: Towards Reliable SQL Synthesis: Fuzzing-Based Evaluation and Disambiguation\n",
      "Authors: Ricardo Brancas & Vasco Manquinho\n",
      "Abstract: In recent years, more people have seen their work depend on data manipulation tasks. However, many of these users do not have the background in programming required to write complex programs, particularly SQL queries. One way of helping these users is automatically synthesizing the SQL query given a small set of examples. Several program synthesizers for SQL have been recently proposed, but they do not leverage multicore architectures.\n",
      "This paper proposes Cubes, a parallel program synthesizer for the domain of SQL queries using input-output examples. Since input-output examples are an under-specification of the desired SQL query, sometimes, the synthesized query does not match the user’s intent. Cubes incorporates a new disambiguation procedure based on fuzzing techniques that interacts with the user and increases the confidence that the returned query matches the user intent. We perform an extensive evaluation on around 4000 SQL queries from different domains. Experimental results show that our parallel approach can scale up to 16 processes with super-linear speedups for many hard instances, and that our disambiguation approach is critical to achieving an accuracy of around 60%, significantly larger than other SQL synthesizers.\n",
      "Submitted_date: 2024-04-06\n",
      "DOI link: https://doi.org/10.1007/978-3-031-57259-3_11\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-19-8300-9_23\n",
      "PDF: None\n",
      "Title: Retrieval-Then-Parsing: A Two-Stage Model for SQL Generation in Financial Domain\n",
      "Authors: Nengzheng Jin, Dongfang Li, Junying Chen & Qingcai Chen\n",
      "Abstract: Querying financial databases with natural language has been a strong requirement for financial company staff in recent years. The common method for this task is to transform the natural language into structured query language (SQL), which is referred to as semantic parsing. In this paper, we propose a two-stage model for semantic parsing in large financial databases. The two-stage model consists of an integrated table retriever that is used to retrieve related tables from a large database and a knowledge-enhanced semantic parser that utilizes a knowledge base for SQL generation. Experimental results show that our model achieves decent performance, which ranks the first place on both development and test datasets of CCKS 2022 evaluation task 11.\n",
      "Submitted_date: 2022-12-02\n",
      "DOI link: https://doi.org/10.1007/978-981-19-8300-9_23\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-80599-9_7\n",
      "PDF: None\n",
      "Title: NumER: A Fine-Grained Numeral Entity Recognition Dataset\n",
      "Authors: Thanakrit Julavanich & Akiko Aizawa\n",
      "Abstract: Named entity recognition (NER) is essential and widely used in natural language processing tasks such as question answering, entity linking, and text summarization. However, most current NER models and datasets focus more on words than on numerals. Numerals in documents can also carry useful and in-depth features beyond simply being described as cardinal or ordinal; for example, numerals can indicate age, length, or capacity. To better understand documents, it is necessary to analyze not only textual words but also numeral information. This paper describes NumER, a fine-grained Numeral Entity Recognition dataset comprising 5,447 numerals of 8 entity types over 2,481 sentences. The documents consist of news, Wikipedia articles, questions, and instructions. To demonstrate the use of this dataset, we train a numeral BERT model to detect and categorize numerals in documents. Our baseline model achieves an F1-score of 95% and hence demonstrating that the model can capture the semantic meaning of the numeral tokens.\n",
      "Submitted_date: 2021-06-20\n",
      "DOI link: https://doi.org/10.1007/978-3-030-80599-9_7\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-658-32182-6_3\n",
      "PDF: None\n",
      "Title: Introducing Natural Language Interface to Databases for Data-Driven Small and Medium Enterprises\n",
      "Authors: Dejan Radovanovic\n",
      "Abstract: Reading text, identifying key ideas, summarizing, making connections and other tasks that require comprehension and context are easy tasks for humans, but training a computer to perform these tasks is a challenge. Recent advances in deep learning make it possible to interpret the text effectively and achieve high performance results across natural language tasks. Interacting with relational databases trough natural language enables users of any background to query and analyze a huge amount of data in a user-friendly way. The purpose of Natural Language Interface is to allow users to compose questions in Natural Language and receive the response also in Natural Language. The idea of using natural language instead of SQL has promoted the development of new type of processing called Natural Language Interface to Database (NLIDB). This paper is an introduction to Natural Language Processing and Natural Language Interface to Database, significant challenges in this research field and how to construct a company specific dataset. It also gives a brief overview of the major techniques used to develop Natural Language Interface to Databases.\n",
      "Submitted_date: 2021-01-05\n",
      "DOI link: https://doi.org/10.1007/978-3-658-32182-6_3\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-00129-1_13\n",
      "PDF: None\n",
      "Title: FalCon: A Faithful Contrastive Framework for Response Generation in TableQA Systems\n",
      "Authors: Shineng Fang, Jiangjie Chen, Xinyao Shen & Yanghua Xiao\n",
      "Abstract: In a practical TableQA system, response generation is a critical module to generate a natural language description of the SQL and the execution result. Due to the complex syntax of SQL and matching issues with table content, this task is prone to produce factual errors. In this paper, we propose FalCon, a FaithfulContrastive generation framework to improve the factual correctness of generated responses. FalCon forces the generation model to identify examples with factual errors in the latent space during training and takes contrastive examples into consideration during inference. We also propose two new automatic metrics to further evaluate faithfulness specialized to this task. Experimental results show FalCon brings a favorable performance improvement on both automatic and human evaluation amongst various baseline methods (The code of FalCon is released at https://github.com/whuFSN/FalCon).\n",
      "Submitted_date: 2022-04-08\n",
      "DOI link: https://doi.org/10.1007/978-3-031-00129-1_13\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-19-7960-6_12\n",
      "PDF: None\n",
      "Title: Optimizing Deep Transformers for Chinese-Thai Low-Resource Translation\n",
      "Authors: Wenjie Hao, Hongfei Xu, Lingling Mu & Hongying Zan\n",
      "Abstract: In this paper, we study the use of deep Transformer translation model for the CCMT 2022 Chinese\\(\\leftrightarrow \\)Thai low-resource machine translation task. We first explore the experiment settings (including the number of BPE merge operations, dropout probability, embedding size, etc.) for the low-resource scenario with the 6-layer Transformer. Considering that increasing the number of layers also increases the regularization on new model parameters (dropout modules are also introduced when using more layers), we adopt the highest performance setting but increase the depth of the Transformer to 24 layers to obtain improved translation quality. Our work obtains the SOTA performance in the Chinese-to-Thai translation in the constrained evaluation.\n",
      "Submitted_date: 2022-12-09\n",
      "DOI link: https://doi.org/10.1007/978-981-19-7960-6_12\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-46238-2_1\n",
      "PDF: None\n",
      "Title: Generative AI as a Supportive Tool for Scientific Research\n",
      "Authors: Abraham Itzhak Weinberg\n",
      "Abstract: This chapter Abrahamaims to bridge the gap between the theoretical potential of Generative AI (GAI) tools, such as Generative Pretrained Transformer (GPT), and their practical applications as supportive tools for scientific research. The chapter provides approaches and techniques for leveraging GAI to address research challenges and activities. It describes common research tasks and provides guidance on how to use GPT to solve them. To the best of our experience, at the current stage, the integration of researchers and GPT has the potential to yield better results than either could achieve alone. Furthermore, the increasing availability of GPT tools suggests that the synergy between the two will continue to improve research output quality and save time. The key to successful integration lies in the appropriate use of GPT that can be achieved by directing AI tools to solve tasks effectively and using prompt engineering techniques.\n",
      "Submitted_date: 2024-03-06\n",
      "DOI link: https://doi.org/10.1007/978-3-031-46238-2_1\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-7254-8_50\n",
      "PDF: None\n",
      "Title: An Integrated Interactive Framework for Natural Language to SQL Translation\n",
      "Authors: Yuankai Fan, Tonghui Ren, Dianjun Guo, Zhigang Zhao, Zhenying He & X. Sean Wang\n",
      "Abstract: Numerous web applications rely on databases, yet the traditional database interface often proves inconvenient for effective data utilization. It is crucial to address the significant demand from a vast number of end users who seek the ability to input their requirements and obtain query results effortlessly. Natural Language (NL) Interfaces to Databases (NLIDBs) with interactive query mechanisms make databases accessible to end users and simultaneously retain user confidence in the results. This paper proposes an approach called IKnow-SQL for building interactive NLIDBs. IKnow-SQL introduces a unified framework for translation models to improve accuracy and increase interactivity. Specifically, IKnow-SQL first employs an underlying translation model to parse the semantics of a given NL query. By evaluating the model behavior, IKnow-SQL then recognizes the parts of the model output that may require human intervention. Next, IKnow-SQL presents clarifying questions to solicit and memorize user feedback until a polished result is obtained. Extensive experiments are performed to study IKnow-SQL on the public benchmark. The results show that the translation models can be effectively improved using IKnow-SQL with less user feedback.\n",
      "Submitted_date: 2023-10-21\n",
      "DOI link: https://doi.org/10.1007/978-981-99-7254-8_50\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-1-4842-8844-3_3\n",
      "PDF: None\n",
      "Title: BERT\n",
      "Authors: Shashank Mohan Jain\n",
      "Abstract: In this chapter, you will learn one of the implementations of the transformer architecture, developed by Google, called BERT.\n",
      "Submitted_date: 2022-10-21\n",
      "DOI link: https://doi.org/10.1007/978-1-4842-8844-3_3\n",
      "Processing link: https://link.springer.com/article/10.1007/s11227-022-05035-9\n",
      "PDF: None\n",
      "Title: Row-based hierarchical graph network for multi-hop question answering over textual and tabular data\n",
      "Authors: Peng Yang, Wenjun Li, Guangzhen Zhao & Xianyu Zha\n",
      "Abstract: Multi-hop Question Answering over heterogeneous data is a challenging task in Natural Language Processing(NLP), which aims to find the answer among heterogeneous data sources and reasoning chains. When facing complex reasoning scenarios, most existing QA systems can only focus on some specific types of data. To solve this issue, we propose a new approach based on Row Hierarchical Graph Network(RHGN), which can accomplish multi-hop QA over both textual and tabular data. Specifically, RHGN consists of two phases: the row selection phase is designed to find the table row that most likely contains the answer, and the row reading comprehension phase that aims to locate the final answer in the answer row. In the row selection phase, we utilize a retriever to search all the supporting evidence related to the question, and a pre-training language model is employed to select the appropriate answer row. In the succeeding stage of row reading comprehension, we propose a row-based hierarchical graph network to capture the structural information, and a gated mechanism is used to perform graph reasoning. Eventually, the optimum final answer can be obtained by three interrelated sub-tasks. The experimental results demonstrate the effectiveness of RHGN and it achieves superior performance on the HybridQA dataset.\n",
      "Submitted_date: 2022-12-29\n",
      "DOI link: https://doi.org/10.1007/s11227-022-05035-9\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-86517-7_21\n",
      "PDF: None\n",
      "Title: An Optimized NL2SQL System for Enterprise Data Mart\n",
      "Authors: Kaiwen Dong, Kai Lu, Xin Xia, David Cieslak & Nitesh V. Chawla\n",
      "Abstract: Natural language interfaces to databases is a growing field that enables end users to interact with relational databases without technical database skills. These interfaces solve the problem of synthesizing SQL queries based on natural language input from the user. There are considerable research interests around the topic but there are few systems to date that are deployed on top of an active enterprise data mart. We present our NL2SQL system designed for the banking sector, which can generate a SQL query from a user’s natural language question. The system is comprised of the NL2SQL model we developed, as well as the data simulation and the adaptive feedback framework to continuously improve model performance. The architecture of this NL2SQL model is built on our research on WikiSQL data, which we extended to support multitable scenarios via our unique table expand process. The data simulation and the feedback loop help the model continuously adjust to linguistic variation introduced by the domain specific knowledge.\n",
      "Submitted_date: 2021-09-10\n",
      "DOI link: https://doi.org/10.1007/978-3-030-86517-7_21\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-20309-1_18\n",
      "PDF: None\n",
      "Title: AOED: Generating SQL with the Aggregation Operator Enhanced Decoding\n",
      "Authors: Yilin Li, Xuan Pan & Yanlong Wen\n",
      "Abstract: NL2SQL is a translation task that converts natural language queries to SQL. We revisit the popular NL2SQL models and find that the accuracy of aggregation operator prediction remains a bottleneck of current NL2SQL models. We present a novel statistics-based approach called AOED, which stands for Aggregation Operator Enhanced Decoding, to help predict aggregation operator. AOED is a carefully designed mechanism that takes full advantage of the statistical information of the aggregation keywords in the natural language query to help improve the prediction accuracy of the aggregation operator. Experiments on the WikiSQL dataset show that our model outperforms the state-of-the-art model SQLova and NL2SQL-RULE by 3.4% and 0.7% on overall SQL results in the logical form accuracy and by 0.2% and 0.7% on aggregation operator result.\n",
      "Submitted_date: 2022-12-08\n",
      "DOI link: https://doi.org/10.1007/978-3-031-20309-1_18\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-73197-7_19\n",
      "PDF: None\n",
      "Title: An Interactive NL2SQL Approach with Reuse Strategy\n",
      "Authors: Xiaxia Wang, Sai Wu, Lidan Shou & Ke Chen\n",
      "Abstract: This paper studies a recently proposed task that maps contextual natural language questions to SQL queries in a multi-turn interaction. Instead of synthesizing an SQL query in an end-to-end way, we propose a new model which first generates an SQL grammar tree, called Tree-SQL, as the intermediate representation, and then infers an SQL query from the Tree-SQL with domain knowledge. For semantic dependency among context-dependent questions, we propose a reuse strategy that assigns a probability for each sub-tree of historical Tree-SQLs. On the challenging contextual Text-to-SQL benchmark SParC (https://yale-lily.github.io/sparc) with the ‘value selection’ task which includes values in queries, our approach achieves SOTA accuracy of 48.5% in question execution accuracy and 21.6% in interaction execution accuracy. In addition, we experimentally demonstrate the significant improvements on the reuse strategy.\n",
      "Submitted_date: 2021-04-06\n",
      "DOI link: https://doi.org/10.1007/978-3-030-73197-7_19\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-75765-6_53\n",
      "PDF: None\n",
      "Title: Capturing SQL Query Overlapping via Subtree Copy for Cross-Domain Context-Dependent SQL Generation\n",
      "Authors: Ruizhuo Zhao, Jinhua Gao, Huawei Shen & Xueqi Cheng\n",
      "Abstract: The key challenge of cross-domain context-dependent text-to-SQL generation tasks lies in capturing the relation of natural language utterance and SQL queries in different turns. A line of works attempt to combat this challenge by capturing the overlaps among consecutively generated SQL queries. Existing models sequentially generate the SQL query for a single turn and model the SQL overlaps via copying tokens or segments generated in previous turns. However, they are not flexible enough to capture various overlapping granularities, e.g., columns, filters, or even the whole query, as they neglect the intrinsic structures inhabited in SQL queries. In this paper, we employ tree-structured intermediate representations of SQL queries, i.e., SemQL, for SQL generation and propose a novel subtree-copy mechanism to characterize the SQL overlaps. At each turn, we encode the interaction questions and previously generated trees as context and decode the SemQL tree in a top-down fashion. Each node is either generated according to SemQL grammar or copied from previously generated SemQL subtrees. Our model can capture various overlapping granularities by copying nodes at different levels of SemQL trees. We evaluate our approach on the SParC dataset and the experimental results show the superior performance of our model compared with state-of-the-art baselines.\n",
      "Submitted_date: 2021-05-08\n",
      "DOI link: https://doi.org/10.1007/978-3-030-75765-6_53\n",
      "Processing link: https://link.springer.com/article/10.1007/s10515-022-00359-5\n",
      "PDF: None\n",
      "Title: HQLgen: deep learning based HQL query generation from program context\n",
      "Authors: Ziyi Zhou, Huiqun Yu, Guisheng Fan, Zijie Huang & Kang Yang\n",
      "Abstract: To facilitate Object-Oriented Programming (OOP) in data persistence, practitioners use Object Relational-Mapping (ORM) framework to map data bidirectionally between data classes and tables of Relational Database Management System (RDBMS). In terms of Java applications, the most trending ORM solution is Hibernate, where Hibernate Query Language (HQL) is proposed to perform customizable queries in an OOP style. However, HQL queries are hard to implement and maintain due to their flexibility and complexity. To address these issues, we propose a model called HQLgen that combines deep learning and template to automatically generate HQL queries from program context. It employs recurrent neural network to learn the contextual information of Java program, and predicts the key elements within HQL clauses via attention mechanism. To construct the dataset for model training and evaluation, we locate and extract projects containing HQL queries on GitHub followed by extensive cleaning and preprocessing, and finally obtain 24,118 HQL queries from 3,481 projects. Experimental results show that the proposed approach achieves an accuracy of 34.52% on predicting simple HQL queries. In addition, we release the collected dataset for future research interest.\n",
      "Submitted_date: 2021-11-30\n",
      "DOI link: https://doi.org/10.1007/s10515-022-00359-5\n",
      "Processing link: https://link.springer.com/article/10.1007/s10664-024-10569-y\n",
      "PDF: None\n",
      "Title: Enhancing robustness of AI offensive code generators via data augmentation\n",
      "Authors: Cristina Improta, Pietro Liguori, Roberto Natella & Domenico Cotroneo\n",
      "Abstract: Since manually writing software exploits for offensive security is time-consuming and requires expert knowledge, AI-base code generators are an attractive solution to enhance security analysts’ productivity by automatically crafting exploits for security testing. However, the variability in the natural language and technical skills used to describe offensive code poses unique challenges to their robustness and applicability. In this work, we present a method to add perturbations to the code descriptions to create new inputs in natural language (NL) from well-intentioned developers that diverge from the original ones due to the use of new words or because they miss part of them. The goal is to analyze how and to what extent perturbations affect the performance of AI code generators in the context of offensive code. First, we show that perturbed descriptions preserve the semantics of the original, non-perturbed ones. Then, we use the method to assess the robustness of three state-of-the-art code generators against the newly perturbed inputs, showing that the performance of these AI-based solutions is highly affected by perturbations in the NL descriptions. To enhance their robustness, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the NL descriptions in the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.\n",
      "Submitted_date: 2024-10-10\n",
      "DOI link: https://doi.org/10.1007/s10664-024-10569-y\n",
      "Processing link: https://link.springer.com/article/10.1631/FITEE.2100467\n",
      "PDF: None\n",
      "Title: How to manage a task-oriented virtual assistant software project: an experience report\n",
      "Authors: Shuyue Li  (李姝玥), Jiaqi Guo  (郭家琪), Yadong Zhou  (周亚东) & Ting Liu  (刘烃)\n",
      "Abstract: Task-oriented virtual assistants are software systems that provide users with a natural language interface to complete domain-specific tasks. With the recent technological advances in natural language processing and machine learning, an increasing number of task-oriented virtual assistants have been developed. However, due to the well-known complexity and difficulties of the natural language understanding problem, it is challenging to manage a task-oriented virtual assistant software project. Meanwhile, the management and experience related to the development of virtual assistants are hardly studied or shared in the research community or industry, to the best of our knowledge. To bridge this knowledge gap, in this paper, we share our experience and the lessons that we have learned at managing a task-oriented virtual assistant software project at Microsoft. We believe that our practices and the lessons learned can provide a useful reference for other researchers and practitioners who aim to develop a virtual assistant system. Finally, we have developed a requirement management tool, named SpecSpace, which can facilitate the management of virtual assistant projects.\n",
      "Submitted_date: 2021-09-30\n",
      "DOI link: None\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-53440-0_5\n",
      "PDF: None\n",
      "Title: Interactivity for Artificial Intelligence Systems: NL2SQL\n",
      "Authors: Karam Ahkouk, Mustapha Machkour, Rachid Mama & Khadija Majhadi\n",
      "Abstract: For sustainability purposes, Artificial Intelligence modules that help translating natural language sentences to SQL queries are gaining more and more momentum in the recent decade, since they allow the automatic conversion of questions in English to the SQL without any interference from the user during the process of translation. We show in this paper the utility of the previously proposed models and their added value to the resolution of the complicated problem of inferring complex queries from English sentences. We will also discuss their limits in addition to the necessity of using interactivity between the user and the system in order to enhance the quality of the outputs. In this paper, we show how the majority of models can’t independently predict the appropriate queries of SQL without the feedback, the guidance as well as the help of the user. And finally, we present the conclusion together with the future work.\n",
      "Submitted_date: 2021-01-24\n",
      "DOI link: https://doi.org/10.1007/978-3-030-53440-0_5\n",
      "Processing link: https://link.springer.com/article/10.1007/s10462-025-11255-1\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s10462-025-11255-1.pdf\n",
      "Title: Agent-in-the-loop to distill expert knowledge into artificial intelligence models: a survey\n",
      "Authors: Jiayuan Gao, Yingwei Zhang, Yiqiang Chen, Yuanzhe Chen, Shuchao Song & Yang Gu\n",
      "Abstract: Large-scale neural networks have revolutionized many general knowledge areas (e.g., computer vision and language processing), but are still rarely applied in many expert knowledge areas (e.g., healthcare), due to data sparsity and high annotation expenses. Human-in-the-loop machine learning (HIL-ML) incorporates expert domain knowledge into the modeling process, effectively addressing these challenges. Recently, some researchers have started using large models to substitute for certain tasks typically performed by humans. Although large models have limitations in expert knowledge areas, after being trained on trillions of examples, they have demonstrated advanced capabilities in reasoning, semantic understanding, grounding, and planning. These capabilities can serve as proxies of human, which introduces new opportunities and challenges in HIL-ML area. Based on the above, we summarize a more comprehensive framework, Agent-in-the-Loop Machine Learning (AIL-ML), where agent represents both humans and large models. AIL-ML can efficiently collaborate human and large model to construct vertical AI models with lower costs. This paper presents the first review of recent advancements in this area. First, we provide a formal definition of AIL-ML and discuss its related fields. Then, we categorize the AIL-ML methods based on data processing and model development, providing formal definitions for each, and present representative works in detail for each category. Third, we highlight relative applications of AIL-ML. Finally, we summarize the current literature and highlight future research directions.\n",
      "Submitted_date: 2025-05-02\n",
      "DOI link: https://doi.org/10.1007/s10462-025-11255-1\n",
      "Processing link: https://link.springer.com/article/10.1007/s44336-024-00009-2\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s44336-024-00009-2.pdf\n",
      "Title: A survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges\n",
      "Authors: Xinyi Li, Sai Wang, Siqi Zeng & Yu Wu\n",
      "Abstract: The pursuit of more intelligent and credible autonomous systems, akin to human society, has been a long-standing endeavor for humans. Leveraging the exceptional reasoning and planning capabilities of large language models (LLMs), LLM-based agents have been proposed and have achieved remarkable success across a wide array of tasks. Notably, LLM-based multi-agent systems (MAS) are considered a promising pathway towards realizing general artificial intelligence that is equivalent to or surpasses human-level intelligence. In this paper, we present a comprehensive survey of these studies, offering a systematic review of LLM-based MAS. Adhering to the workflow of LLM-based multi-agent systems, we synthesize a general structure encompassing five key components: profile, perception, self-action, mutual interaction, and evolution. This unified framework encapsulates much of the previous work in the field. Furthermore, we illuminate the extensive applications of LLM-based MAS in two principal areas: problem-solving and world simulation. Finally, we discuss in detail several contemporary challenges and provide insights into potential future directions in this domain.\n",
      "Submitted_date: 2024-07-15\n",
      "DOI link: https://doi.org/10.1007/s44336-024-00009-2\n",
      "Processing link: https://link.springer.com/article/10.1007/s10664-022-10275-7\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s10664-022-10275-7.pdf\n",
      "Title: JEMMA: An extensible Java dataset for ML4Code applications\n",
      "Authors: Anjan Karmakar\n",
      "Abstract: Machine Learning for Source Code (ML4Code) is an active research field in which extensive experimentation is needed to discover how to best use source code’s richly structured information. With this in mind, we introduce JEMMA: An Extensible Java Dataset for ML4Code Applications, which is a large-scale, diverse, and high-quality dataset targeted at ML4Code. Our goal with JEMMA is to lower the barrier to entry in ML4Code by providing the building blocks to experiment with source code models and tasks. JEMMA comes with a considerable amount of pre-processed information such as metadata, representations (e.g., code tokens, ASTs, graphs), and several properties (e.g., metrics, static analysis results) for 50,000 Java projects from the 50K-C dataset, with over 1.2 million classes and over 8 million methods. JEMMA is also extensible allowing users to add new properties and representations to the dataset, and evaluate tasks on them. Thus, JEMMA becomes a workbench that researchers can use to experiment with novel representations and tasks operating on source code. To demonstrate the utility of the dataset, we also report results from two empirical studies on our data, ultimately showing that significant work lies ahead in the design of context-aware source code models that can reason over a broader network of source code entities in a software project—the very task that JEMMA is designed to help with.\n",
      "Submitted_date: 2022-12-13\n",
      "DOI link: https://doi.org/10.1007/s10664-022-10275-7\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-23080-6_6\n",
      "PDF: None\n",
      "Title: Conversational QA over Knowledge Bases\n",
      "Authors: Jianfeng Gao, Chenyan Xiong, Paul Bennett & Nick Craswell\n",
      "Abstract: This chapter describes Conversational Question Answering over Knowledge Bases (C-KBQA) as a special case of CIR, where the answers to input queries are generated not from a collection of indexed documents but from a (set of) structured knowledge base(s).\n",
      "Submitted_date: 2023-03-17\n",
      "DOI link: https://doi.org/10.1007/978-3-031-23080-6_6\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-58219-7_4\n",
      "PDF: None\n",
      "Title: Question Answering When Knowledge Bases are Incomplete\n",
      "Authors: Camille Pradel & Damien Sileo\n",
      "Abstract: While systems for question answering over knowledge bases (KB) continue to progress, real world usage requires systems that are robust to incomplete KBs. Dependence on the closed world assumption is highly problematic, as in many practical cases the information is constantly evolving and KBs cannot keep up. In this paper we formalize a typology of missing information in knowledge bases, and present a dataset based on the Spider KB question answering dataset, where we deliberately remove information from several knowledge bases, in this case implemented as relational databases (The dataset and the code to reproduce experiments are available at https://github.com/camillepradel/IDK.). Our dataset, called IDK (Incomplete Data in Knowledge base question answering), allows to perform studies on how to detect and recover from such cases. The analysis shows that simple baselines fail to detect most of the unanswerable questions.\n",
      "Submitted_date: 2020-09-15\n",
      "DOI link: https://doi.org/10.1007/978-3-030-58219-7_4\n",
      "Processing link: https://link.springer.com/article/10.1007/s11432-023-3956-3\n",
      "PDF: None\n",
      "Title: Deep learning for code generation: a survey\n",
      "Authors: Huangzhao Zhang, Kechi Zhang, Zhuo Li, Jia Li, Jia Li, Yongmin Li, Yunfei Zhao, Yuqi Zhu, Ge Li & Zhi Jin\n",
      "Abstract: In the past decade, thanks to the powerfulness of deep-learning techniques, we have witnessed a whole new era of automated code generation. To sort out developments, we have conducted a comprehensive review of solutions to deep learning-based code generation. In this survey, we generally formalize the pipeline and procedure of code generation and categorize existing solutions according to taxonomy from perspectives of architecture, model-agnostic enhancing strategy, metrics, and tasks. In addition, we outline the challenges faced by current dominant large models and list several plausible directions for future research. We hope that this survey may provide handy guidance to understanding, utilizing, and developing deep learning-based code-generation techniques for researchers and practitioners.\n",
      "Submitted_date: 2023-06-08\n",
      "DOI link: https://doi.org/10.1007/s11432-023-3956-3\n",
      "Processing link: https://link.springer.com/article/10.1007/s11277-020-07896-w\n",
      "PDF: None\n",
      "Title: Trainable Framework for Information Extraction, Structuring and Summarization of Unstructured Data, Using Modified NER\n",
      "Authors: Partha Sarathy Banerjee & Baisakhi Chakraborty\n",
      "Abstract: The World Wide Web is an ever expanding source of data in today’s world. Millions of tera-bytes of data and information is getting added every second. In this information age as the data is getting generated at an exponential rate, the fact to be noted is that most of the information is already available is in the form of natural language text. The task of information extraction from mammoth data leads us to think on the quality and the form of available data. Secondly, the ever increasing data poses a challenging task of extracting useful information from the available data. The third task is to extract information as efficiently as possible. For retrieving the information there is a need to develop ingenious way to answer any kind of query put up by a user from the available unstructured data. This paper proposes a novel trainable and integrated Natural Language Information Interpretation and Representation System (NLIIRS) that accepts any available un-annotated corpus of data in the form of natural language, and performs the following tasks: finds out the useful data, extracts relevant information in usable form (structured form/tables), summarizes the data and structures the data in relational form. At the end the Question and Answering (Q&A) module shows the cognitive abilities of NLIIR system by answering the questions in natural language relevant to the text. This multispecialty system beyond just Q&A. This is a trainable system capable of handling any unstructured data to be transformed into structured and well organized information. It allows the user to ask questions in natural language. It adopts the advantages of a modified named entity recognition so as to bypass the time consuming process of parts of speech tagging while pre-processing the available corpus (data) for information extraction.\n",
      "Submitted_date: 2020-10-29\n",
      "DOI link: https://doi.org/10.1007/s11277-020-07896-w\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-15-3380-8_30\n",
      "PDF: None\n",
      "Title: Use of Ontology Learning in Information System Integration: A Literature Survey\n",
      "Authors: Chuangtao Ma & Bálint Molnár\n",
      "Abstract: Ontology-based information integration is a useful method to integrate heterogeneous data at the semantic level. However, there are some bottlenecks of the traditional method for constructing ontology, i.e., time-consuming, error-prone, and semantic loss. Ontology learning is a kind of ontology construction approach based on machine learning, it provides a new opportunity to tackle the above bottlenecks. Especially, it could be employed to construct ontologies and integrate large-scale and heterogeneous data from various information systems. This paper surveys the latest developments of ontology learning and highlights how they could be adopted and play a vital role in the integration of information systems. The recent techniques and tools of ontology learning from text and relational database are reviewed, the possibility of using ontology learning in information integration were discussed based on the mapping results of the aforementioned bottlenecks and features of ontology learning. The potential directions for using ontology learning in information systems integration were given.\n",
      "Submitted_date: 2020-03-03\n",
      "DOI link: https://doi.org/10.1007/978-981-15-3380-8_30\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-1-4842-6370-9_8\n",
      "PDF: None\n",
      "Title: Multi-model Capabilities\n",
      "Authors: Davide Mauri & Sanjay Mishra\n",
      "Abstract: Azure SQL is not a traditional relational database platform. Every modern database must enable you to use various data formats for different scenarios. Although traditional normalized relational format is battle-tested and proven as optimal technology for a wide range of different scenarios, in some cases, you might find that some other formats might be the better fit for the problem that you are solving.\n",
      "Submitted_date: 2020-11-06\n",
      "DOI link: https://doi.org/10.1007/978-1-4842-6370-9_8\n",
      "Processing link: https://link.springer.com/article/10.1007/s00778-019-00584-7\n",
      "PDF: None\n",
      "Title: Explaining Natural Language query results\n",
      "Authors: Daniel Deutch, Nave Frost & Amir Gilad\n",
      "Abstract: Multiple lines of research have developed Natural Language (NL) interfaces for formulating database queries. We build upon this work, but focus on presenting a highly detailed form of the answers in NL. The answers that we present are importantly based on the provenance of tuples in the query result, detailing not only the results but also their explanations. We develop a novel method for transforming provenance information to NL, by leveraging the original NL query structure. Furthermore, since provenance information is typically large and complex, we present two solutions for its effective presentation as NL text: one that is based on provenance factorization, with novel desiderata relevant to the NL case and one that is based on summarization. We have implemented our solution in an end-to-end system supporting questions, answers and provenance, all expressed in NL. Our experiments, including a user study, indicate the quality of our solution and its scalability.\n",
      "Submitted_date: 2018-12-03\n",
      "DOI link: https://doi.org/10.1007/s00778-019-00584-7\n",
      "Processing link: https://link.springer.com/article/10.1007/s11431-020-1692-3\n",
      "PDF: None\n",
      "Title: Recent advances and challenges in task-oriented dialog systems\n",
      "Authors: Zheng Zhang, Ryuichi Takanobu, Qi Zhu, MinLie Huang & XiaoYan Zhu\n",
      "Abstract: Due to the significance and value in human-computer interaction and natural language processing, task-oriented dialog systems are attracting more and more attention in both academic and industrial communities. In this paper, we survey recent advances and challenges in task-oriented dialog systems. We also discuss three critical topics for task-oriented dialog systems: (1) improving data efficiency to facilitate dialog modeling in low-resource settings, (2) modeling multi-turn dynamics for dialog policy learning to achieve better task-completion performance, and (3) integrating domain ontology knowledge into the dialog model. Besides, we review the recent progresses in dialog evaluation and some widely-used corpora. We believe that this survey, though incomplete, can shed a light on future research in task-oriented dialog systems.\n",
      "Submitted_date: 2020-03-09\n",
      "DOI link: https://doi.org/10.1007/s11431-020-1692-3\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-35699-6_26\n",
      "PDF: None\n",
      "Title: Neural Semantic Parsing with Anonymization for Command Understanding in General-Purpose Service Robots\n",
      "Authors: Nick Walker, Yu-Tang Peng & Maya Cakmak\n",
      "Abstract: Service robots are envisioned to undertake a wide range of tasks at the request of users. Semantic parsing is one way to convert natural language commands given to these robots into executable representations. Methods for creating semantic parsers, however, rely either on large amounts of data or on engineered lexical features and parsing rules, which has limited their application in robotics. To address this challenge, we propose an approach that leverages neural semantic parsing methods in combination with contextual word embeddings to enable the training of a semantic parser with little data and without domain specific parser engineering. Key to our approach is the use of an anonymized target representation which is more easily learned by the parser. In most cases, this simplified representation can trivially be transformed into an executable format, and in others the parse can be completed through further interaction with the user. We evaluate this approach in the context of the RoboCup@Home General Purpose Service Robot task, where we have collected a corpus of paraphrased versions of commands from the standardized command generator. Our results show that neural semantic parsers can predict the logical form of unseen commands with 89% accuracy. We release our data and the details of our models to encourage further development from the RoboCup and service robotics communities.\n",
      "Submitted_date: 2019-12-01\n",
      "DOI link: https://doi.org/10.1007/978-3-030-35699-6_26\n",
      "Processing link: https://link.springer.com/article/10.1007/s11432-023-4127-5\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s11432-023-4127-5.pdf\n",
      "Title: Deep learning-based software engineering: progress, challenges, and opportunities\n",
      "Authors: Yanjie Jiang & Lu Zhang\n",
      "Abstract: Researchers have recently achieved significant advances in deep learning techniques, which in turn has substantially advanced other research disciplines, such as natural language processing, image processing, speech recognition, and software engineering. Various deep learning techniques have been successfully employed to facilitate software engineering tasks, including code generation, software refactoring, and fault localization. Many studies have also been presented in top conferences and journals, demonstrating the applications of deep learning techniques in resolving various software engineering tasks. However, although several surveys have provided overall pictures of the application of deep learning techniques in software engineering, they focus more on learning techniques, that is, what kind of deep learning techniques are employed and how deep models are trained or fine-tuned for software engineering tasks. We still lack surveys explaining the advances of subareas in software engineering driven by deep learning techniques, as well as challenges and opportunities in each subarea. To this end, in this study, we present the first task-oriented survey on deep learning-based software engineering. It covers twelve major software engineering subareas significantly impacted by deep learning techniques. Such subareas spread out through the whole lifecycle of software development and maintenance, including requirements engineering, software development, testing, maintenance, and developer collaboration. As we believe that deep learning may provide an opportunity to revolutionize the whole discipline of software engineering, providing one survey covering as many subareas as possible in software engineering can help future research push forward the frontier of deep learning-based software engineering more systematically. For each of the selected subareas, we highlight the major advances achieved by applying deep learning techniques with pointers to the available datasets in such a subarea. We also discuss the challenges and opportunities concerning each of the surveyed software engineering subareas.\n",
      "Submitted_date: 2023-09-26\n",
      "DOI link: https://doi.org/10.1007/s11432-023-4127-5\n",
      "Processing link: https://link.springer.com/article/10.1007/s11235-011-9620-3\n",
      "PDF: None\n",
      "Title: Agent-based framework for intelligent natural language interface\n",
      "Authors: Moses Ekpenyong\n",
      "Abstract: In this contribution, we present a framework for an intelligent natural language interface (NLI) that suits the need of embedded platform, using agent-based approach. The proposed framework is motivated by an ongoing speech technology research project aimed at developing a generic synthesizer for information disseminating systems in local languages. The architecture is based on various forms of action representations with a sequence of transformations that converts users’ input (text or speech) into a suitable set of agent actions that produce response to the input. This approach incrementally minimizes the complexity and ambiguity of the natural language input by using predefined sets of interim actions at different levels, hence, increasing the robustness and reliability of the NLI.\n",
      "Submitted_date: 2011-09-01\n",
      "DOI link: https://doi.org/10.1007/s11235-011-9620-3\n",
      "Processing link: https://link.springer.com/article/10.1007/s10489-023-05221-z\n",
      "PDF: None\n",
      "Title: NL2SQL with partial missing metadata based on multi-view metadata graph compensation and reasoning\n",
      "Authors: Jie Lin, Yulong Liang, Jiyan Li, Yi Bai & Yong Wang\n",
      "Abstract: The performance of metadata-dependent NL2SQL models will be seriously decreased, while facing the incomplete or distorted metadata information. In response to this problem, we proposed a metadata compensation approach, which represents the question together with SQL query, data cell value relevance and incomplete schema data as a global metadata graph, and applies knowledge graph reasoning to complete the metadata graph. This global metadata graph is a multi-graph. An improved transR model was proposed to represent this multi-graph by integrating the contributions from multiple relationships between two nodes. Depending on the compensated metadata graph, new end-to-end and preprocess improving frameworks were respectively constructed for adapting to different metadata-dependent NL2SQL systems. The new models have been evaluated on Spider dataset with artificially simulated partial metadata relation deficiency or metadata distortion. Except ablation comparing, the new models also have been compared with some approaches of existing and have demonstrated improved performance.\n",
      "Submitted_date: 2023-12-06\n",
      "DOI link: https://doi.org/10.1007/s10489-023-05221-z\n",
      "Processing link: https://link.springer.com/chapter/10.1007/979-8-8688-0515-8_8\n",
      "PDF: None\n",
      "Title: Architecting a NL2SQL Project for Immense Enterprise Databases\n",
      "Authors: Pere Martra\n",
      "Abstract: In Chapter 6, you've already seen a small NL2SQL solution. On that occasion, the databases used were for illustrative purposes, to facilitate both coding and comprehension of all the concepts explained in the chapter.\n",
      "Submitted_date: 2024-09-19\n",
      "DOI link: https://doi.org/10.1007/979-8-8688-0515-8_8\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-96-0808-9_27\n",
      "PDF: None\n",
      "Title: BIS: NL2SQL Service Evaluation Benchmark for Business Intelligence Scenarios\n",
      "Authors: Bora Caglayan, Mingxue Wang & Puchao Zhang\n",
      "Abstract: NL2SQL (Natural Language to Structured Query Language) transformation has seen wide adoption in Business Intelligence (BI) applications in recent years. However, existing NL2SQL benchmarks are not suitable for production BI scenarios, as they are not designed for common business intelligence questions. To address this gap, we have developed a new benchmark focused on typical NL questions in industrial BI scenarios. We discuss the challenges of constructing a BI -focused benchmark and the shortcomings of existing benchmarks. Additionally, we introduce question categories in our benchmark that reflect common BI inquiries. Lastly, we propose two novel semantic similarity evaluation metrics for assessing NL2SQL capabilities in BI applications and services.\n",
      "Submitted_date: 2024-12-07\n",
      "DOI link: https://doi.org/10.1007/978-981-96-0808-9_27\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-96-5373-7_90\n",
      "PDF: None\n",
      "Title: From NL2SQL to NL2Code: Exploring A New Paradigm for Natural Language to Programming Code Generation\n",
      "Authors: Lihe Tang & Yitian Liu\n",
      "Abstract: With the continuous advancement of artificial intelligence technology, the ability to automatically convert natural language instructions into executable code has become a cutting-edge research focus. To apply NL2Code in the field of backend business code generation, NL2SQL (natural language to structured query language) and template engine-based code generation techniques have been adopted. By conducting coding experiments using the Java language, it is possible to quickly generate backend business code from natural language instructions. Therefore, the new method of generating backend business code from natural language proposed in this paper enables more efficient and accurate automatic code generation. This approach not only leverages the strengths of AI in natural language processing but also addresses the practical needs of software development, bridging the gap between non-technical user input and technical code output. The integration of NL2SQL and template engines ensures that the generated code is both functional and maintainable, marking a significant step forward in automating the software development process.\n",
      "Submitted_date: 2025-05-22\n",
      "DOI link: https://doi.org/10.1007/978-981-96-5373-7_90\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-90200-0_13\n",
      "PDF: None\n",
      "Title: LLMs for Virtualized Networking Infrastructures: An Industrial Report\n",
      "Authors: Luigi Pannocchi & Tommaso Cucinotta\n",
      "Abstract: Machine Learning and Artificial Intelligence techniques are disrupting several fields of engineering, including telecommunications. In this domain, network operators are evaluating the use of data-oriented techniques to improve the management of virtualized networking infrastructures in several aspects, including monitoring, optimization, traffic forecasting, capacity planning, anomaly detection, and others. This paper summarizes our ongoing experience in the area of adopting human-machine interfaces based on Natural Language Processing for easing and speeding-up the interactions between these systems and network operators in their everyday work.\n",
      "Submitted_date: 2025-06-11\n",
      "DOI link: https://doi.org/10.1007/978-3-031-90200-0_13\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-19-8300-9_24\n",
      "PDF: None\n",
      "Title: Structured Design Solves Multiple Tables of NL2SQL\n",
      "Authors: Xianwei Yi, Ruijie Wang, Hanyi Zhang & Shiqi Zhen\n",
      "Abstract: NL2SQL (NLP Language To SQL) is a cutting-edge research direction of natural language processing, which converts natural query statements input by users into executable SQL statements. CCKS2022 proposes a multi-database multi-table NL2SQL task for the financial domain. For this task, this paper proposes a SQL generation method based on semantic parsing. The method adopts the multi-stage iterative generation mode of “question-database name-table name-column name-SQL statement”, uses semantic parsing and semantic similarity learning methods in acquiring table names, and generates and selects SQL statements based on the same query Multi-input statement for integrated filtering. At the end of the competition, the label replacement method was adopted, that is, the tables and columns in the SQL statement were replaced with tags to reduce the difficulty of generation.\n",
      "Submitted_date: 2022-12-02\n",
      "DOI link: https://doi.org/10.1007/978-981-19-8300-9_24\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-73503-5_1\n",
      "PDF: None\n",
      "Title: Large Language Model for Querying Databases in Portuguese\n",
      "Authors: Lourenço Figueiredo & Nuno Marques\n",
      "Abstract: This study introduces a system that helps non-expert users find information easily without knowing database languages or asking technicians for help. A specific domain is explored, focusing on a subscrip- tion-based sports facility, which serves as an open-source version of a real case study. Utilizing the star schema, the available data in the database is structured to provide accessibility through Portuguese Natural Language queries. Using a Large Language Model (LLM), SQL queries are generated based on the question and the provided star schema. We created a dataset with 115 highly challenging questions drawn from real-world usage scenarios to validate the correctness of the system. Challenges found during testing, like attribute value interpretation, out-of-scope questions, and temporal interval adequacy issues, highlight the insufficiency of the star schema alone in providing the needed context for generating accurate SQL queries by the LLM. Addressing these challenges through enhanced contextual information shows significant improvement in query correctness, with validation results increasing from 57.76% to 88.79%. This study shows the potential and limitations of LLMs in generating SQL queries from Portuguese Natural Language queries.\n",
      "Submitted_date: 2024-11-16\n",
      "DOI link: https://doi.org/10.1007/978-3-031-73503-5_1\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-16-1342-5_21\n",
      "PDF: None\n",
      "Title: NL2SQL: Natural Language to SQL Query Translator\n",
      "Authors: T. J. Revanth, K. Venkat Sai, R. Ramya, Renusree Chava, V. Sushma & B. S. Ramya\n",
      "Abstract: Most of the media networks and organizations require a database to store the information and to retrieve the information from the database where structured query language (SQL) is utilized. SQL has its own sentence format to frame the questions. The capacity of end users to recover information from a database is constrained because of an absence of SQL knowledge. To recover right information from the database, this inquiry needs to be written in legitimate or right linguistic structure. Along these lines, the essential target of this proposed work is to locate the reasonable method to change over natural language to SQL and make the information available for end users. In existing NLS-SQL system, the users found difficulty in querying due to firm data dictionaries which users are not allowed to change or modify the data dictionary as per their requirements. In this paper, we propose a system that provides a user-friendly interface between end user and database. The essential feature of our paper is that users have privilege to update data dictionaries by which they can add new terms or phrases which can be mapped to their corresponding clause in SQL. In this, we undertake a lightweight strategy of changing over a characteristic language explanation into comparable SQL articulation, which when executed on a database furnishes us with the precise outcomes using natural language processing (NLP). In the proposed framework, we have used natural language processing (NLP) to enable communication between users and system without need to memorize the complex SQL commands. The project framework will now allow the amateur users who do not have any information about SQL to recover the necessary information. This framework can be used at the training and placement cell officials who deal with database study.\n",
      "Submitted_date: 2021-12-01\n",
      "DOI link: https://doi.org/10.1007/978-981-16-1342-5_21\n",
      "Processing link: https://link.springer.com/chapter/10.1007/979-8-8688-0515-8_1\n",
      "PDF: None\n",
      "Title: Introduction to Large Language Models with OpenAI\n",
      "Authors: Pere Martra\n",
      "Abstract: You will begin by understanding the workings of large language models using the OpenAI API. OpenAI enables you to use powerful models in a straightforward manner.\n",
      "Submitted_date: 2024-09-19\n",
      "DOI link: https://doi.org/10.1007/979-8-8688-0515-8_1\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-7238-4_26\n",
      "PDF: None\n",
      "Title: Gar+\n",
      "+\n",
      ": Natural Language to SQL Translation with Efficient Generate-and-Rank\n",
      "Authors: Yuankai Fan, Can Huang, Tonghui Ren, Zhenying He & X.Sean Wang\n",
      "Abstract: Web applications heavily depend on databases, yet the conventional database interface often presents challenges for efficient data utilization. It is imperative to address the considerable demand emanating from a vast array of end users seeking seamless input of their requirements and effortless retrieval of query results. Natural Language (NL) Interfaces to Databases serve to make databases accessible to end users. Mainstream approaches typically prioritize building language translation models for converting NL queries to SQL queries, while a novel generate-and-rank approach is proposed to achieve this through a procedure involving generation and ranking. Despite yielding superior translation results on the public benchmark, this generate-and-rank approach encounters efficiency issues that may impede its practical application. In this paper, we introduce Gar+\n",
      "+\n",
      ", which extends the existing generate-and-rank approach for a more efficient generation and robust ranking procedure. Specifically, Gar+\n",
      "+\n",
      " utilizes the bloom filter to accelerate the data generation process by reducing unnecessary function calls. Additionally, Gar+\n",
      "+\n",
      " provides a brand-new implementation of the ranking module, specifically the re-ranking model, empowered with enhanced language understanding ability. We evaluate the effectiveness of Gar+\n",
      "+\n",
      " on three public benchmarks, namely GEO, Spider, and Mt-teql. Gar+\n",
      "+\n",
      " achieved an overall accuracy of 66.6% on Geo, 80.6% on Spider, and 78.4% on Mt-teql, respectively.\n",
      "Submitted_date: 2024-08-28\n",
      "DOI link: https://doi.org/10.1007/978-981-97-7238-4_26\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-19-8300-9_16\n",
      "PDF: None\n",
      "Title: Learning Seq2Seq Model with Dynamic Schema Linking for NL2SQL\n",
      "Authors: Xingxing Ning, Yupeng Zhao & Jie liu\n",
      "Abstract: NL2SQL (Natural Language to SQL) is a cutting-edge problem in the field of semantic parsing and TableQA. “CCKS2022: Financial NL2SQL Evaluation” raises a challenging scenario for building NL2SQL systems in the financial domain. To deal with the problem of small-scale data and the requirement of adapting to financial scenarios, we propose an NL2SQL approach to automatically converts natural language questions into SQL queries to achieve accurate table question answering. We use cross-validation and schema linking method that fuses table-column-value information to make full use of all the training data. Then we train a Transformer-based Seq2Seq semantic parsing model with T5 as pre-training model to understand common questions in the financial field and parse the database’s tables, attributes, foreign keys and other complex relationships, and finally generate SQL queries. Experiments are conducted to test the effectiveness of our strategy. Our best ensemble model achieves EM score of 0.287 on testing set and ranks 2nd in the competition.\n",
      "Submitted_date: 2022-12-02\n",
      "DOI link: https://doi.org/10.1007/978-981-19-8300-9_16\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-8979-9_14\n",
      "PDF: None\n",
      "Title: Application and Research on a Large Model Training Method Based on Instruction Fine-Tuning in Domain-Specific Tasks\n",
      "Authors: Yawei Zhang, Changsheng Li, Xindong Wang & Bin Zhang\n",
      "Abstract: The potential of large-scale models to enhance industrial productivity and catalyze societal progress is undeniable. However, inherent challenges-such as lengthy training cycles and the demand for advanced computational resources-remain daunting. Given recent advancements in computational adaptability, this paper introduces a systematic approach to effectively fine-tune these models for domain-specific tasks. Our method encompasses three key phases: (1) a thorough analysis of domain-specific business needs and data acquisition; (2) precise task segmentation, designing standardized instruction formats to construct a fine-tuning dataset, and subsequently fine-tuning the large-scale models; (3) rigorous model validation using a test dataset. Through these steps, we effectively fine-tuned our training using 5,000 data instances and validated our results with an additional 1,000 test instances. To complement our study, we provide a comparative analysis of different training techniques and assess the fine-tuning results on four prominent open-source models. The conclusions drawn offer valuable insights for the future application of large-scale models in specialized domains and pave the way for further research and applications.\n",
      "Submitted_date: 2023-12-15\n",
      "DOI link: https://doi.org/10.1007/978-981-99-8979-9_14\n",
      "Processing link: https://link.springer.com/chapter/10.1007/979-8-8688-0974-3_10\n",
      "PDF: None\n",
      "Title: Beyond RDBMS\n",
      "Authors: Bob Ward\n",
      "Abstract: In the summer of 2023, I was trying to think of a way to describe some of the great capabilities we have in Azure SQL that is “in the box” but also features that are not normally in a traditional Relational Database Management System (RDBMS). The word “beyond” just came to me as a way to describe this. So, I started sketching down on paper (I’m old school) two columns:\n",
      "Submitted_date: 2024-10-11\n",
      "DOI link: https://doi.org/10.1007/979-8-8688-0974-3_10\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-00129-1_49\n",
      "PDF: None\n",
      "Title: Data-Based Insights for the Masses: Scaling Natural Language Querying to Middleware Data\n",
      "Authors: Kausik Lakkaraju, Sai Teja Paladi & Biplav Srivastava\n",
      "Abstract: In this demonstration, we focus on middleware data obtained from devices like the network routers and power meters which may be of interest to a technician fixing a customer complaint or a user trying to self-diagnose their utility usage. The users in our case are often unaware of both the data details and database querying language which is in contrast to typical natural language to structured query (NL2SQL) situations where the business analyst knows their domain data but not the querying techniques. We adapt the rule-based NL2SQL approach to our problem and in particular, focus on queries about users, devices and spatio-temporal properties that are unique to this setting. We demonstrate an Alexa-based system, implemented using open-source Rasa, that can answer router usage queries in a home setting and easily extended for power usage or other utilities.\n",
      "Submitted_date: 2022-04-08\n",
      "DOI link: https://doi.org/10.1007/978-3-031-00129-1_49\n",
      "Processing link: https://link.springer.com/article/10.1007/s41019-022-00191-7\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s41019-022-00191-7.pdf\n",
      "Title: Intent-Aware Data Visualization Recommendation\n",
      "Authors: Atsuki Maruta & Makoto P. Kato\n",
      "Abstract: This paper proposes a visualization recommender system for tabular data given visualization intents (e.g., “population trends in Italy” and “smartphone market share”). The proposed method predicts the most suitable visualization type (e.g., line, pie, or bar chart) and visualized columns (columns used for visualization) based on statistical features extracted from the tabular data as well as semantic features derived from the visualization intent. To predict the appropriate visualization type, we propose a bi-directional attention (BiDA) model that identifies important table columns using the visualization intent and important parts of the intent using the table headers. To determine the visualized columns, we employ a pre-trained neural language model to encode both visualization intents and table columns and predict which columns are the most likely to be used for visualization. Since there was no available dataset for this task, we created a new dataset consisting of over 100 K tables and their appropriate visualization. Experiments revealed that our proposed methods accurately predicted suitable visualization types and visualized columns.\n",
      "Submitted_date: 2022-03-07\n",
      "DOI link: https://doi.org/10.1007/s41019-022-00191-7\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-50577-5_20\n",
      "PDF: None\n",
      "Title: Research on Database Language Query Method Based on Cloud Computing Platform\n",
      "Authors: Shao Gong\n",
      "Abstract: Some database language query methods have the problems of time cost and resource occupation. Under this background, a database language query method based on cloud computing platform is designed. Convert the formally described information into data that can be stored, extract the semantic Web hierarchy, and establish relationships with real objects, identify the potential association features of database words and sentences, restore the original orthogonal semantic structure space, build ontology annotation model based on cloud computing platform, transplant HanLP’s neural network dependency syntax analysis tool, and design language query methods. Test results: The average time cost and resource utilization of the database language query method designed this time are 1720 ms and 42.36% respectively, which shows that the database language query method designed this time is more effective under the technical support of the cloud computing platform.\n",
      "Submitted_date: 2024-02-21\n",
      "DOI link: https://doi.org/10.1007/978-3-031-50577-5_20\n",
      "Processing link: https://link.springer.com/chapter/10.1007/979-8-8688-0515-8_7\n",
      "PDF: None\n",
      "Title: Creating and Publishing Your Own LLM\n",
      "Authors: Pere Martra\n",
      "Abstract: This chapter serves as a continuation of Chapter 5, which focused on various fine-tuning techniques for a language model. As you may recall, we explored different methods such as LoRA, QLoRA, and prompt tuning.\n",
      "Submitted_date: 2024-09-19\n",
      "DOI link: https://doi.org/10.1007/979-8-8688-0515-8_7\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-16-8558-3_26\n",
      "PDF: None\n",
      "Title: Natural Language Query for Power Grid Information Model\n",
      "Authors: Bing Wu\n",
      "Abstract: A building information model often provides the functional and physical data of an electrical facility for the downstream construction, operation and maintenance of the built power grid infrastructure. Therefore, the rapid and convenient query of the required information from the design model is crucial for all the project participants. However, the query of the design data from a BIM model is frequently burdensome and tedious. Moreover, the Grid Information Modeling (GIM) schema, developed by the China State Grid for describing electrical equipment with more engineering details, exaggerates the difficulty of querying the design model. This study applies the Natural Language Interface to Database (NLIDB) approach for querying data from the Neo4j graph database that fuses both IFC data for architectural or structural design and GIM data for electrical equipment. Meanwhile, this study also develops a tool to automatically convert the natural language questions into Cypher queries. In addition, a knowledge graph is also developed for linking the semantic elements extracted from the natural language questions with the IFC semantics stored in the Neo4j database.\n",
      "Submitted_date: 2022-03-22\n",
      "DOI link: https://doi.org/10.1007/978-981-16-8558-3_26\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-52167-7_7\n",
      "PDF: None\n",
      "Title: Inferring Logical Clauses for Answering Complex Multi-hop Open Domain Questions\n",
      "Authors: Boris Galitsky\n",
      "Abstract: We enable a conventional Question Answering (Q/A) system with formal reasoning that relies on clauses learned from texts and also on the complex question decomposition into simple queries that can run against various sources, from local index to intranet to the web. We integrate the reasoning, multi-hop querying and machine reading comprehension (MRC) so that a value can be extracted from a search result of one simple query and substituted into another simple query, till the answer is obtained via the recomposition of simple queries. The integrated approach boosts the Q/A performance in a number of domains and approaches the state-of-the-art for some of them, designed to train a deep learning (DL) Q/A system to answer complex, multi-hop queries. We assess the contribution of two reasoning components: ontology building from text, and entity association, as well as multi-hop query decomposer and MRC and observe that these components are necessary and complement each other answering complex questions. We apply a similar multi-hop framework to the problem of a natural language (NL) access to a database. The conclusion is that the proposed architecture with the focus on formal reasoning is well suited for industrial applications where performance is guaranteed in cases of no or limited training sets.\n",
      "Submitted_date: 2020-12-08\n",
      "DOI link: https://doi.org/10.1007/978-3-030-52167-7_7\n",
      "Processing link: https://link.springer.com/chapter/10.1007/979-8-8688-0515-8_5\n",
      "PDF: None\n",
      "Title: Fine-Tuning Models\n",
      "Authors: Pere Martra\n",
      "Abstract: This chapter will introduce you to a completely new world. So far, you've used large language models, seen how to ask them questions, and learned how to influence them through prompts. You've also created RAG applications where the language model uses the information you provide to generate its response.\n",
      "Submitted_date: 2024-09-19\n",
      "DOI link: https://doi.org/10.1007/979-8-8688-0515-8_5\n",
      "Processing link: https://link.springer.com/chapter/10.1007/979-8-8688-0974-3_7\n",
      "PDF: None\n",
      "Title: Monitoring and Tuning Performance for Azure SQL\n",
      "Authors: Bob Ward\n",
      "Abstract: You now have seen how to secure your Azure SQL deployment. Another aspect to ensure you have the best possible database for your application is understanding how to scale, configure, monitor, and tune performance. If you know SQL Server, here is some good news. The engine that powers Azure SQL is the same one for SQL Server! This means that about any performance capability you need exists for Azure SQL. It also means that many of the same tasks and skills you use for SQL Server apply to Azure SQL. In this chapter, we will explore all the capabilities and tasks you normally use to monitor and tune performance for a SQL Server and compare it with Azure SQL. You will also learn some specific capabilities and techniques unique to Azure to give you the best possible performance.\n",
      "Submitted_date: 2024-10-11\n",
      "DOI link: https://doi.org/10.1007/979-8-8688-0974-3_7\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-16-0401-0_22\n",
      "PDF: None\n",
      "Title: NLP2SQL Using Semi-supervised Learning\n",
      "Authors: H. Vathsala\n",
      "Abstract: Human Computer interaction has been moving towards Natural language in the modern age. SQL (Structured Query Language) is the chief database query language used today. There are many flavors of SQL but all of them have the same basic underlying structure. This paper attempts to use the Natural Language inputs to query the databases, which is achieved by translating the natural language (which in our case is English) input into the SQL (specific to MySQL database) query language. Here we use a semi-supervised learning with Memory augmented policy optimization approach to solve this problem. This method uses the context of the natural language questions through database schema, and hence its not just generation of SQL code. We have used the WikiSQL dataset for all our experiments. The proposed method gives a 2.3% higher accuracy than the state of the art semi-supervised method on an average.\n",
      "Submitted_date: 2021-02-11\n",
      "DOI link: https://doi.org/10.1007/978-981-16-0401-0_22\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-04299-8_4\n",
      "PDF: None\n",
      "Title: Developing Conversational Natural Language Interface to a Database\n",
      "Authors: Boris Galitsky\n",
      "Abstract: In this Chapter we focus on a problem of a natural language access to a database, well-known and highly desired to be solved. We start with the modern approaches based on deep learning and analyze lessons learned from unusable database access systems. This chapter can serve as a brief introduction to neural networks for learning logic representations. Then a number of hybrid approaches are presented and their strong points are analyzed. Finally, we describe our approach that relies on parsing, thesaurus and disambiguation via chatbot communication mode. The conclusion is that a reliable and flexible database access via NL needs to employ a broad spectrum of linguistic, knowledge representation and learning techniques. We conclude this chapter by surveying the general technology trends related to NL2SQL, observing how AI and ML are seeping into virtually everything and represent a major battleground for technology providers.\n",
      "Submitted_date: 2019-04-05\n",
      "DOI link: https://doi.org/10.1007/978-3-030-04299-8_4\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-16-1986-1_3\n",
      "PDF: None\n",
      "Title: Trends and Highlights in China\n",
      "Authors: None\n",
      "Abstract: Great progress has been witnessed in KG technology and applications. KG is now one of the most representative technologies of knowledge engineering in the era of big data and has been put into large-scale implementation in Search, Question Answering, and other simple scenarios. China’s strong concern and continued commitment to innovation in KG technology and applications in the past few years have resulted in a lot of progress and breakthroughs. Key issues and highlights will be elaborated on from both a technology and application perspective.\n",
      "Submitted_date: 2021-06-10\n",
      "DOI link: https://doi.org/10.1007/978-981-16-1986-1_3\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-0-387-73563-4_6\n",
      "PDF: None\n",
      "Title: Data Fitting Techniques with Applications to Mineral Dissolution Kinetics\n",
      "Authors: Joel Z. Bandstra\n",
      "Abstract: A common problem in chemical kinetics is the development of a rate law that describes the dependence of the reaction rate on the surrounding conditions such as concentrations of reacting species or temperature of the reacting media (see Chap. 1). The most direct approach to solving this problem is to measure reaction rates under systematically varied conditions and then to perform mathematical analyses on these data to determine the form of the rate law and to generate estimates of any unknown constants, or parameters, that make up the proposed rate law. Other chapters in this book provide information both for designing kinetics experiments and for selecting appropriate rate laws for a variety of geochemical reactions. In this chapter we describe the mathematical analyses — known collectively as curve fitting or regression analysis — that can be used to select a rate equation that matches a given data set, to generate estimates for any unknown parameters in the rate equation (e.g., rate constants or reaction orders), and to quantify the uncertainty associated with the estimated values for the parameters. As we traverse this entirely quantitative process we will attempt to describe the underlying, qualitative process of looking at kinetic data: plots to make, features of these plots to examine, and conceptual sketches to draw.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-0-387-73563-4_6\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-44213-1_32\n",
      "PDF: None\n",
      "Title: Enhancing Text2SQL Generation with Syntactic Information and Multi-task Learning\n",
      "Authors: Haochen Li & Minghua Nuo\n",
      "Abstract: The Text2SQL task aims to convert natural language (NL) questions into SQL queries. Most rule-based traditional syntactic parsers are often ignoring the syntactic information in the question and unseen database schemas, which lead to lower generalization ability. To breakthrough these limitations, we propose a novel model Syn-RGAT that leverages syntactic information of questions. Specifically, our model jointly encodes database schemas and questions by mapping them into a graph, which is then passed through a relation-aware graph attention network (RGAT). For the questions, map it as a syntax information graph and use syntactic augmentation module to learn all the nodes representations of the graph. Then the outputs of RGAT and syntactic augmentation module are integrated. Additionally, to assist the model distinguish between different database schemas, we introduce a graph pruning task and form a multi-task framework which shares the encoder of Text2SQL task. We use a heuristic learning method to combine graph pruning tasks with Text2SQL. In experimental part, Syn-RGAT outperforms all baseline models on the Spider dataset, and we further improve the performance more than 8% with BERT.\n",
      "Submitted_date: 2023-09-22\n",
      "DOI link: https://doi.org/10.1007/978-3-031-44213-1_32\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-1256-8_10\n",
      "PDF: None\n",
      "Title: Research on the Text2SQL Method Based on Schema Linking Enhanced\n",
      "Authors: Jun Tie, Boer Zhu, Chong Sun & Ziqi Fan\n",
      "Abstract: In recent years, natural language generation of SQL sentences (Text2SQL) has received a lot of attention as an important research direction natural language processing (NLP). Text2SQL makes it easier for users to query complex databases without learning SQL sentences and the underlying database schema. The current mainstream Text2SQL method is the grammar-based IRNet, which attempts to present an efficient method with explanations from the perspective of constructing reasonable grammars and provides a good solution for solving complex nested queries. Still, it makes simple use of external database ontology knowledge, resulting in natural language problems in which words do not correspond well to tables and columns in the database. To address this problem, a new method that considers the entity relationships between natural language problems and data in the database - SLESQL is proposed, which extends some of the functionality of IRNet by using schema linking enhanced Experiments show that SLESQL achieves 6.8% improvement in accuracy over IRNet on the publicly available dataset Spider.\n",
      "Submitted_date: 2023-04-02\n",
      "DOI link: https://doi.org/10.1007/978-981-99-1256-8_10\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-92970-0_11\n",
      "PDF: None\n",
      "Title: Interactive Sensemaking with SurveySense: Enhancing Survey Insights Through Human-AI Collaboration on an LLM-Based Platform\n",
      "Authors: Jia Hui Bong, Siyuan Liu & Xiuyi Fan\n",
      "Abstract: Surveys are essential in various fields to gather feedback and information. Efficient data processing is thus crucial for quick, informed decision-making. As the volume of data continues to grow, automation in survey analysis has become increasingly important. This paper proposes a new software tool, SurveySense, which aims to streamline the survey analysis process. A study involving 26 participants was also conducted to evaluate the tool across six dimensions - Insight Quality, Efficiency, Consistency, User satisfaction and Trustworthiness, Human-AI Collaboration and Usability and Interaction. Results showed that SurveySense would be a helpful assistant for survey analysis but human inputs remain crucial in the process to further refine the AI-generated insights to align with personal goals.\n",
      "Submitted_date: 2025-05-25\n",
      "DOI link: https://doi.org/10.1007/978-3-031-92970-0_11\n",
      "Processing link: https://link.springer.com/article/10.1007/s10639-025-13633-2\n",
      "PDF: None\n",
      "Title: Analyzing the teaching and learning environments through student feedback at scale: a multi-agent LLMs framework\n",
      "Authors: Chang Cai, Minyang Chow & Xiuyi Fan\n",
      "Abstract: Analyzing the teaching and learning environment (TLE) through student feedback is essential for identifying curricular gaps and improving teaching practices. However, traditional feedback analysis methods, particularly for qualitative data, are often time-consuming and prone to human bias. Large Language Models (LLMs) offer a promising solution by facilitating multimodal data analysis. This study presents a novel framework that utilizes multi-agent LLMs to analyze feedback from 7,160 medical residents, integrating both quantitative and qualitative data to generate comprehensive reports. Quantitative analysis was conducted using SPSS for descriptive statistics, trend analysis, and non-parametric tests, while LLMs handled sentiment analysis and topic identification for qualitative feedback, with results visualized through word clouds. The findings from both qualitative and quantitative analyses were integrated as multimodal data, comprising text and images. The multi-agent system, comprising report generation and modification agents, analyzed multimodal data to produce and refine reports. This iterative process significantly improved report quality in terms of balance, clarity, semantics, readability, and coherence. The findings suggest that multi-agent LLMs improve the efficiency and depth of feedback analysis while providing a scalable solution for generating customized and comprehensive reports. This approach has far-reaching implications for educational data analysis.\n",
      "Submitted_date: 2024-11-06\n",
      "DOI link: https://doi.org/10.1007/s10639-025-13633-2\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-78093-6_9\n",
      "PDF: None\n",
      "Title: Online Digital Investigative Journalism Using SociaLens\n",
      "Authors: Hasan M. Jamil & Sajratul Y. Rubaiat\n",
      "Abstract: Media companies witnessed a significant transformation with the rise of the internet, bigdata, machine learning (ML) and AI. Recent emergence of large language models (LLM) have added another aspect to this transformation. Researchers believe that with the help of these technologies, investigative digital journalism will enter a new era. Using a smart set of data gathering and analysis tools, journalists will be able to create data driven contents and insights in unprecedented ways. In this paper, we introduce a versatile and autonomous investigative journalism tool, called SociaLens, for identifying and extracting query specific data from online sources, responding to probing queries and drawing conclusions entailed by large volumes of data using ML analytics fully autonomously. We envision its use in investigative journalism, law enforcement and social policy planning. The proposed system capitalizes on the integration of ML technology with LLMs and advanced bigdata search techniques. We illustrate the functionality of SociaLens using a focused case study on rape incidents in a developing country and demonstrate that journalists can gain nuanced insights without requiring coding expertise they might lack. SociaLens is designed as a ChatBot that is capable of contextual conversation, find and collect data relevant to queries, initiate ML tasks to respond to queries, generate textual and visual reports, all fully autonomously within the ChatBot environment.\n",
      "Submitted_date: 2024-12-04\n",
      "DOI link: https://doi.org/10.1007/978-3-031-78093-6_9\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-5615-5_28\n",
      "PDF: None\n",
      "Title: Open-Domain Question Answering over Tables with Large Language Models\n",
      "Authors: Xinyi Liang, Rui Hu, Yu Liu & Konglin Zhu\n",
      "Abstract: Open-domain question answering (ODQA) over tables has received attention in recent years. This task requires retrieving evidence from huge amounts of semi-structured tabular data to answer natural language questions. Previous open-domain table QA methods typically focus on customized models, which require a significant amount of annotated training data. Furthermore, most existing models struggle to handle different types of complex problems due to limited reasoning ability and flexibility. Some of them attempt to expand the training and pre-training modules, but this approach increases costs and limits practicality. To alleviate the above challenges, we introduce Generating Identifiers and Selecting chunks for Tables (GIST), which leverages the potential of large language models (LLMs) for the open-domain table QA task. With the assistance of designed prompts and a powerful LLM, GIST first generate question identifiers, and process them for table retrieval. Then we select the candidate evidence to reduce interference caused by the vast amount of data and support subsequent reasoning. Finally, the LLM is prompted to extract the correct answer. Experimental results illustrate that our method achieves competitive end-to-end QA performance on two open-domain table QA datasets. In particular, our method significantly outperforms the current state-of-the-art on NQ-TABLES.\n",
      "Submitted_date: 2024-08-03\n",
      "DOI link: https://doi.org/10.1007/978-981-97-5615-5_28\n",
      "Processing link: https://link.springer.com/article/10.1007/s41019-023-00235-6\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s41019-023-00235-6.pdf\n",
      "Title: DB-GPT: Large Language Model Meets Database\n",
      "Authors: Xuanhe Zhou, Zhaoyan Sun & Guoliang Li\n",
      "Abstract: Large language models (LLMs) have shown superior performance in various areas. And LLMs have the potential to revolutionize data management by serving as the \"brain\" of next-generation database systems. However, there are several challenges that utilize LLMs to optimize databases. First, it is challenging to provide appropriate prompts (e.g., instructions and demonstration examples) to enable LLMs to understand the database optimization problems. Second, LLMs only capture the logical database characters (e.g., SQL semantics) but are not aware of physical characters (e.g., data distributions), and it requires to fine-tune LLMs to capture both physical and logical information. Third, LLMs are not well trained for databases with strict constraints (e.g., query plan equivalence) and privacy-preserving requirements, and it is challenging to train database-specific LLMs while ensuring database privacy. To overcome these challenges, this vision paper proposes a LLM-based database framework (DB-GPT), including automatic prompt generation, DB-specific model fine-tuning, and DB-specific model design and pre-training. Preliminary experiments show that DB-GPT achieves relatively good performance in database tasks like query rewrite and index tuning. The source code and datasets are available at github.com/TsinghuaDatabaseGroup/DB-GPT.\n",
      "Submitted_date: 2023-06-06\n",
      "DOI link: https://doi.org/10.1007/s41019-023-00235-6\n",
      "Processing link: https://link.springer.com/article/10.1007/s10723-024-09768-0\n",
      "PDF: None\n",
      "Title: An Effective Prediction of Resource Using Machine Learning in Edge Environments for the Smart Healthcare Industry\n",
      "Authors: Guangyu Xu\n",
      "Abstract: Recent modern computing and trends in digital transformation provide a smart healthcare system for predicting diseases at an early stage. In healthcare services, Internet of Things (IoT) based models play a vital role in enhancing data processing and detection. As IoT grows, processing data requires more space. Transferring the patient reports takes too much time and energy, which causes high latency and energy. To overcome this, Edge computing is the solution. The data is analysed in the edge layer to improve the utilization. This paper proposed effective prediction of resource allocation and prediction models using IoT and Edge, which are suitable for healthcare applications. The proposed system consists of three modules: data preprocessing using filtering approaches, Resource allocation using the Deep Q network, and prediction phase using an optimised DL model called DBN-LSTM with frog leap optimization. The DL model is trained using the training health dataset, and the target field is predicted. It has been tested using the sensed data from the IoT layer, and the patient health status is expected to take appropriate actions. With timely prediction using edge devices, doctors and patients conveniently take necessary actions. The primary objective of this system is to secure low latency by improving the quality of service (QoS) metrics such as makespan, ARU, LBL, TAT, and accuracy. The deep reinforcement learning approach is employed due to its considerable acceptance for resource allocation. Compared to the state-of-the-art approaches, the proposed system obtained reduced makespan by increasing the average resource utilization and load balancing, which is suitable for accurate real-time analysis of patient health status.\n",
      "Submitted_date: 2023-08-07\n",
      "DOI link: https://doi.org/10.1007/s10723-024-09768-0\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-19-7596-7_14\n",
      "PDF: None\n",
      "Title: A Survey on Table Question Answering: Recent Advances\n",
      "Authors: Nengzheng Jin, Joanna Siebert, Dongfang Li & Qingcai Chen\n",
      "Abstract: Table Question Answering (Table QA) refers to providing precise answers from tables to answer a user’s question. In recent years, there have been a lot of works on table QA, but there is a lack of comprehensive surveys on this research topic. Hence, we aim to provide an overview of available datasets and representative methods in table QA. We classify existing methods for table QA into five categories according to their techniques, which include semantic-parsing-based, generative, extractive, matching-based, and retriever-reader-based methods. Moreover, because table QA is still a challenging task for existing methods, we also identify and outline several key challenges and discuss the potential future directions of table QA.\n",
      "Submitted_date: 2022-11-19\n",
      "DOI link: https://doi.org/10.1007/978-981-19-7596-7_14\n",
      "Processing link: https://link.springer.com/article/10.1007/s10707-023-00496-3\n",
      "PDF: None\n",
      "Title: Multilingual spatial domain natural language interface to databases\n",
      "Authors: Wenlu Wang\n",
      "Abstract: A natural language interface (NLI) to databases is an interface that translates a natural language question to a structured query that is executable by database management systems (DBMS). However, an NLI that is trained in the general domain is hard to apply in the spatial domain due to the idiosyncrasy and expressiveness of the spatial semantics. Moreover, there are a wide range of database servers available, and a unilingual NLI model limits its practical usage. In this article, we propose to not only address the spatial domain generalization challenge, but also support multilingual back-end, i.e., supporting different query languages, such as SQL and Prolog. For the challenge of spatial semantics, we propose a spatial comprehension model that is able to recognize the meaning of spatial entities based on the semantics of context and effectively resolve the ambiguity given the spatial semantics. The spatial semantics learned from the spatial comprehension model is then injected to the natural language question to ease the burden of capturing the spatial-specific semantics. We also propose to add a prefix symbol to support the multilingual back-end (e.g., query languages). With our spatial comprehension model and symbol injections, our NLI for the spatial domain, named SpatialNLI, is able to capture the semantic structure of the question and translate it to the corresponding syntax of an executable query accurately. We also experimentally ascertain that SpatialNLI outperforms state-of-the-art methods.\n",
      "Submitted_date: 2022-01-25\n",
      "DOI link: https://doi.org/10.1007/s10707-023-00496-3\n",
      "Processing link: https://link.springer.com/article/10.1007/s13042-024-02269-2\n",
      "PDF: None\n",
      "Title: Federated learning-guided intrusion detection and neural key exchange for safeguarding patient data on the internet of medical things\n",
      "Authors: Chongzhou Zhong\n",
      "Abstract: To improve the security of the Internet of Medical Things (IoMT) in healthcare, this paper offers a Federated Learning (FL)-guided Intrusion Detection System (IDS) and an Artificial Neural Network (ANN)-based key exchange mechanism inside a blockchain framework. The IDS are essential for spotting network anomalies and taking preventative action to guarantee the secure and dependable functioning of IoMT systems. The suggested method integrates FL-IDS with a blockchain-based ANN-based key exchange mechanism, providing several important benefits: (1) FL-based IDS creates a shared ledger that aggregates nearby weights and transmits historical weights that have been averaged, lowering computing effort, eliminating poisoning attacks, and improving data visibility and integrity throughout the shared database. (2) The system uses edge-based detection techniques to protect the cloud in the case of a security breach, enabling quicker threat recognition with less computational and processing resource usage. FL’s effectiveness with fewer data samples plays a part in this benefit. (3) The bidirectional alignment of ANNs ensures a strong security framework and facilitates the production of keys inside the IoMT network on the blockchain. (4) Mutual learning approaches synchronize ANNs, making it easier for IoMT devices to distribute synchronized keys. (5) XGBoost and ANN models were put to the test using BoT-IoT datasets to gauge how successful the suggested method is. The findings show that ANN demonstrates greater performance and dependability when dealing with heterogeneous data available in IoMT, such as ICU (Intensive Care Unit) data in the medical profession, compared to alternative approaches studied in this study. Overall, this method demonstrates increased security measures and performance, making it an appealing option for protecting IoMT systems, especially in demanding medical settings like ICUs.\n",
      "Submitted_date: 2023-12-23\n",
      "DOI link: https://doi.org/10.1007/s13042-024-02269-2\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-5652-4_33\n",
      "PDF: None\n",
      "Title: Conversion of Natural Language to SQL Query\n",
      "Authors: Chaitanya Nirfarake, Hrishikesh Vaze, Atharva Wagh, Zeeshan Mujawar & Preeti Kale\n",
      "Abstract: This paper outlines a method for efficiently automating the conversion of Natural Language Query to Structured Query Language (SQL) queries. SQL is a standard tool for relational database management systems. The correct SQL query with proper keywords and syntax must be entered to obtain or manipulate data from such databases. Therefore, we put forward Natural Language Sequential Query Language “NLSQL”, a system in natural language processing to get around this problem and convert natural language queries to SQL queries. To revamp prevailing advancements in this field, we propose three times state-of-the-art parsing and implementation for JOIN queries with our system’s support for multiple tables in a rule-based system. Along with basic queries, complex commands can also be handled by this system. After experimenting and testing this work on a number of natural language queries we were able to achieve convincing results.\n",
      "Submitted_date: 2023-11-01\n",
      "DOI link: https://doi.org/10.1007/978-981-99-5652-4_33\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-99-3250-4_28\n",
      "PDF: None\n",
      "Title: Survey on Natural Language Interfaces to Databases\n",
      "Authors: Prachi Mehta, Vedant Mehta, Harshwardhan Pardeshi & Pramod Bide\n",
      "Abstract: In today’s world, data holds utmost importance and value and all industries like health care, education, government, law, finance, etc., require data to perform statistical analysis to make business decisions. To get access to all this data, users need to heavily interact with the database. To extract accurate data, complex SQL queries involving JOINS, nesting has to be written. But not all users are proficient at SQL because of which it takes time to extract and process the correct data efficiently, which leads to wastage of resources. In order to smooth this process, we aim to provide and perform survey on a natural language abstraction layer over the database so that users can focus on the application and not the data. The system would accept a query in multilingual natural language (Way Humans Communicate with each other) through speech or text. This would be then converted to a SQL query which would be fired on the database.\n",
      "Submitted_date: 2023-08-04\n",
      "DOI link: https://doi.org/10.1007/978-981-99-3250-4_28\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-97-7356-5_33\n",
      "PDF: None\n",
      "Title: SQL Queries Using Voice Commands to Be Executed\n",
      "Authors: R. S. M. Lakshmi Patibandla\n",
      "Abstract: Add tables, delete tables, update tuples, and remove entries all require SQL queries. SQL query execution could appear straightforward, yet even a small mistake could result in serious issues. Keeping track of the queries and ensuring that they are handled flawlessly is a time-consuming, exhausting procedure. Without it, a bad query execution would result in bad data handling and eventual data loss. Speaking a query out loud and clearly and letting the computer handle it are two additional simple alternatives to typing it in. This is accomplished by our software using built-in Python methods and straightforward techniques. Furthermore, the time complexity might be greatly decreased with basic knowledge of NLP methods and how they operate, and comparable outcomes could be seen with low topic knowledge and high productivity.\n",
      "Submitted_date: 2024-12-29\n",
      "DOI link: https://doi.org/10.1007/978-981-97-7356-5_33\n",
      "Processing link: https://link.springer.com/chapter/10.1007/979-8-8688-0974-3_2\n",
      "PDF: None\n",
      "Title: What Is Azure SQL?\n",
      "Authors: Bob Ward\n",
      "Abstract: “Bob, what is the cloud?” I vividly remember my beautiful and talented wife Ginger asking me this question when I walked in our kitchen one evening. I paused for a second, getting ready to present my incredible and thoughtful answer, and said “Well, you see the cloud is….” Fifteen minutes later (as Ginger recalls – I thought it was just a few minutes), Ginger politely interrupted me and said “Uh…I was kind of look for the CliffsNotes answer?” I was taken aback. How can anyone define the cloud as something so simple when it is such a complex topic and provides so much. I couldn’t let this go, so I spent the next few days researching a simpler answer that still defined the cloud. But I also wanted to make sure Ginger knew the answer to the question “What is Azure?”\n",
      "Submitted_date: 2024-10-11\n",
      "DOI link: https://doi.org/10.1007/979-8-8688-0974-3_2\n",
      "Processing link: https://link.springer.com/chapter/10.34157/978-3-648-17372-5_2\n",
      "PDF: None\n",
      "Title: Interview zum Thema »Zukunft der Unternehmensplanung mit SAP«\n",
      "Authors: None\n",
      "Abstract: Matthias Krämer ist Senior Vice President | Head of SAP HANA Database & Analytics – Planning & Analytics\n",
      "Submitted_date: None\n",
      "DOI link: None\n",
      "Processing link: https://link.springer.com/article/10.1007/s42044-021-00095-1\n",
      "PDF: None\n",
      "Title: An intelligent natural language query processor for a relational database\n",
      "Authors: S. S. Vinod Chandra\n",
      "Abstract: Every single byte of data is stored in either a structured or unstructured database. In this era of data exploration, retrieving and processing this information is tedious, as databases are ubiquitous. Basic knowledge in query processing languages like SQL, DMX, or QUEL is essential for retrieving such information from a database. Most people, however, are unaware of such query processing languages and find it difficult to write queries because of their lack of knowledge about the structure and format. Queries vary depending on the database used and how results need to be displayed. This can be addressed using an intelligent database system (IDBS) with natural language processing (NLP) capability. An intelligent natural language interface for a database (NLIDB) allows users to query the database in their spoken language. This paper describes an intelligent NLIDB that takes English language queries as input and converts them into corresponding SQL queries for retrieving information. The NLP procedures used here recognize the tokens and predict the possibility of generating clauses including SELECT, WHERE, and FROM. A query translation algorithm is used to map the identified tokens to SQL tokens. To generate SQL queries, a template is used. A query predictor based on maximum entropy generates the SQL queries when the query translator fails. Thus the model was trained with generated queries and different combinations of chunk tags, and their restraints were predicted. The proposed NLP technique is implemented in the maximum entropy model. The model either predicts SQL templates or generates SQL queries. This technique yields 100% correct results for the template-based system. The system offers a maximum probable result which matches the user query for the prediction module. The system consistently generates accurate results for a natural language query in template mode or SQL query mode. Easy retrieval of data from a huge database by making use of local language is of high relevance. Adding other languages and training the model can be seen as a scope of future work.\n",
      "Submitted_date: 2021-06-03\n",
      "DOI link: https://doi.org/10.1007/s42044-021-00095-1\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-031-29476-1_7\n",
      "PDF: None\n",
      "Title: Formalization of Natural Language into PPTL Specification via Neural Machine Translation\n",
      "Authors: Chunyi Li, Jiajun Chang, Xiaobing Wang, Liang Zhao & Wenjie Mao\n",
      "Abstract: Propositional Projection Temporal Logic (PPTL) has been widely used in formal verification, and its expressiveness is suitable for the description of security requirements. However, the expression and application of temporal logic formulas rely on a strong mathematical background, which is difficult for non-domain experts, thus bridging the chasm between natural language descriptions and formal languages is urgently needed. This paper proposes an innovative architecture for neural machine automatic translation named NL2PPTL, which transforms natural language into PPTL specification via utilizing data preprocessing, encoder-decoder network and stack sequentially. To evaluate the performance of our method, the experimental verification is realized on real datasets. The experiment conducted shows that our method has effectiveness on temporal logic specification generation.\n",
      "Submitted_date: 2023-03-25\n",
      "DOI link: https://doi.org/10.1007/978-3-031-29476-1_7\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-19-2347-0_65\n",
      "PDF: None\n",
      "Title: Natural Language Interface in Dogri to Database\n",
      "Authors: Shubhnandan S. Jamwal & Vijay Singh Sen\n",
      "Abstract: Database is one of the most common terms used in today’s digital era in almost every domain. Digital applications use database for convenient storage and retrieval of data in an organized manner. Database management system is used to perform data operations while ensuring consistency, security, sharing, and privacy of data besides backup and recovery of the data. However, application of database requires sound technical skills since a particular language in a specific format is required by the system to obtain desired data. Dogri language is mainly spoken in Jammu and Kashmir and in some of its neighboring border district of states Himachal Pradesh and Punjab. It is added in the 8th schedule of Indian constitution and is one of the 22 official languages of India. In this paper, a novel attempt has been made to transform the Dogri language query to its equivalent SQL query format. Emphasis has been made on the fundamental and mostly used select queries that the proposed system accepts as input which is then transformed in SQL-formatted query. Since user is independent of following any predefined format for querying the database, some preprocessing operations on data are inevitable. Once data preprocessing like tokenization and stopword removal was done, Dogri language query was partitioned into clauses. These clauses were mapped to the most likely clause in SQL format. Clauses were finally sorted in a manner that a well-formatted SQL query is obtained. Natural languages are highly ambiguous, about five versions of each query were taken, and around 150 queries were tested. The proposed model obtained an accuracy of 88% in transforming appropriately Dogri language query to their equivalent SQL-formatted query.\n",
      "Submitted_date: 2022-08-02\n",
      "DOI link: https://doi.org/10.1007/978-981-19-2347-0_65\n",
      "Processing link: https://link.springer.com/article/10.1186/s40537-020-00383-w\n",
      "PDF: https://link.springer.com/content/pdf/10.1186/s40537-020-00383-w.pdf\n",
      "Title: Querying knowledge graphs in natural language\n",
      "Authors: Shiqi Liang\n",
      "Abstract: Knowledge graphs are a powerful concept for querying large amounts of data. These knowledge graphs are typically enormous and are often not easily accessible to end-users because they require specialized knowledge in query languages such as SPARQL. Moreover, end-users need a deep understanding of the structure of the underlying data models often based on the Resource Description Framework (RDF). This drawback has led to the development of Question-Answering (QA) systems that enable end-users to express their information needs in natural language. While existing systems simplify user access, there is still room for improvement in the accuracy of these systems. In this paper we propose a new QA system for translating natural language questions into SPARQL queries. The key idea is to break up the translation process into 5 smaller, more manageable sub-tasks and use ensemble machine learning methods as well as Tree-LSTM-based neural network models to automatically learn and translate a natural language question into a SPARQL query. The performance of our proposed QA system is empirically evaluated using the two renowned benchmarks-the 7th Question Answering over Linked Data Challenge (QALD-7) and the Large-Scale Complex Question Answering Dataset (LC-QuAD). Experimental results show that our QA system outperforms the state-of-art systems by 15% on the QALD-7 dataset and by 48% on the LC-QuAD dataset, respectively. In addition, we make our source code available.\n",
      "Submitted_date: 2020-09-11\n",
      "DOI link: None\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-91560-5_18\n",
      "PDF: None\n",
      "Title: Intent-Aware Visualization Recommendation for Tabular Data\n",
      "Authors: Atsuki Maruta & Makoto P. Kato\n",
      "Abstract: This paper proposes a visualization recommender system for tabular data with a visualization intent (e.g., “population trends in Italy” and “smartphone market share”). The proposed method predicts the most suitable visualization type (e.g., line, pie, and bar charts) and visualized columns (columns used for visualization) based on statistical features extracted from the tabular data, as well as semantic features derived from the visualization intent. To predict an appropriate visualization type, we propose a bi-directional attention (BiDA) model that identifies important table columns by the visualization intent, and important parts of the intent by table headers. To identify visualized columns, we employ a pre-trained neural language model to encode both visualization intents and table columns, and estimate which columns are the most likely to be used for visualization. Since there is no available dataset for this task, we developed a new dataset consisting of over 100K tables and their appropriate visualization. The experiments revealed that our proposed methods accurately estimated suitable visualization types as well as visualized columns. The code is available at https://github.com/kasys-lab/intent-viz .\n",
      "Submitted_date: 2022-01-01\n",
      "DOI link: https://doi.org/10.1007/978-3-030-91560-5_18\n",
      "Processing link: https://link.springer.com/article/10.1007/s13740-021-00128-9\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s13740-021-00128-9.pdf\n",
      "Title: What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs\n",
      "Authors: Nadine Steinmetz & Kai-Uwe Sattler\n",
      "Abstract: Question Answering based on Knowledge Graphs (KGQA) still faces difficult challenges when transforming natural language (NL) to SPARQL queries. Simple questions only referring to one triple are answerable by most QA systems, but more complex questions requiring complex queries containing subqueries or several functions are still a tough challenge within this field of research. Evaluation results of QA systems therefore also might depend on the benchmark dataset the system has been tested on. For the purpose to give an overview and reveal specific characteristics, we examined currently available KGQA datasets regarding several challenging aspects. This paper presents a detailed look into the datasets and compares them in terms of challenges a KGQA system is facing.\n",
      "Submitted_date: 2020-04-11\n",
      "DOI link: https://doi.org/10.1007/s13740-021-00128-9\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-13-6052-7_34\n",
      "PDF: None\n",
      "Title: Natural Language Based SQL Query Verification Against Relational Schema\n",
      "Authors: Shoaib Saleem Khan, Abid Saeed, Yasir Majeed & Muhammad Kamran\n",
      "Abstract: Writing SQL queries for database is a complex and skill requiring task especially for the new users. The situation becomes more critical when a low skilled person want to access and analyze his data from a relational database. These scenarios require expertise and skills in terms of understanding and writing the accurate and functional queries. However, these complex tasks can be simplified by providing an easy interface to the users. In order to resolve all such issues, automated software tool is needed, which facilitates both users and software engineers. In this paper we present a novel approach with name Que-Gen (Query Generator) that generates SQL queries based on the specification provided in National English Language. Users need to write the requirements in simple English in a few statements. After a semantic analysis and mapping of the associated information. Que-Gen generates the intended SQL queries that can be executed directly on the database. An experimental study has been conducted to analyze the performance and the accuracy of the purposed tool.\n",
      "Submitted_date: 2019-03-12\n",
      "DOI link: https://doi.org/10.1007/978-981-13-6052-7_34\n",
      "Processing link: https://link.springer.com/article/10.2991/ijcis.d.200310.002\n",
      "PDF: https://link.springer.com/content/pdf/10.2991/ijcis.d.200310.002.pdf\n",
      "Title: Information Retrieval Based on Knowledge-Enhanced Word Embedding Through Dialog: A Case Study\n",
      "Authors: Jin Ren, Hengsheng Wang & Tong Liu\n",
      "Abstract: The aim of this paper is to provide a systematic route of information retrieval from a knowledge-based database (or domain knowledge) through a dialog system of natural language interaction. The application is about a comprehensive building at a university, with classrooms, laboratory rooms, meeting rooms, research rooms and offices, and is to present related information the user asks for. First, the domain knowledge is expressed with predicate expressions based on the ontology structure; then the vocabulary is presented distributedly with word embedding enhanced with the domain knowledge; queries from the user are then converted into the intent (general) and slot elements (specific) with the help of trained recurrent neural network (RNN). The system works smoothly. The key point is integrating the two methods of knowledge-based and data-driven natural language processing into one system, and the domain knowledge is in the central part which is incorporated into the word embedding to make it specifically fit the natural language in this application.\n",
      "Submitted_date: 2019-04-15\n",
      "DOI link: None\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-34058-2_22\n",
      "PDF: None\n",
      "Title: SchenQL: A Concept of a Domain-Specific Query Language on Bibliographic Metadata\n",
      "Authors: Christin Katharina Kreutz, Michael Wolz & Ralf Schenkel\n",
      "Abstract: Information access needs to be uncomplicated, users rather use incorrect data which is easily received than correct information which is harder to obtain. Querying bibliographic metadata from digital libraries mainly supports simple textual queries. A user’s demand for answering more sophisticated queries could be fulfilled by the usage of SQL. As such means are highly complex and challenging even for trained programmers, a domain-specific query language is needed to provide a straightforward way to access data.\n",
      "In this paper we present SchenQL, a simple query language focused on bibliographic metadata in the area of computer science while using the vocabulary of domain-experts. By facilitating a plain syntax and fundamental aggregate functions, we propose an easy-to-learn domain-specific query language capable of search and exploration. It is suitable for domain-experts as well as casual users while still providing the possibility to answer complicated queries.\n",
      "Submitted_date: 2019-10-29\n",
      "DOI link: https://doi.org/10.1007/978-3-030-34058-2_22\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-319-71008-2_31\n",
      "PDF: None\n",
      "Title: Translation of Natural Language Queries to SQL that Involve Aggregate Functions, Grouping and Subqueries for a Natural Language Interface to Databases\n",
      "Authors: Rodolfo A. Pazos R., Andres A. Verastegui, José A. Martínez F. & Juana Gaspar H.\n",
      "Abstract: Currently, huge amounts of information are stored in databases (DBs). In order to facilitate access to information to all users, natural language interfaces to databases (NLIDBs) have been developed. To this end, these interfaces translate natural language queries to a DB query language. For businesses, the main application of NLIDBs is for decision making by facilitating access to information in a flexible manner. For a NLIDB to be considered complete, it must deal with queries that involve aggregate functions: COUNT, MIN, MAX, SUM and AVG. The prototype developed at the Instituto Tecnológico de Cd. Madero (ITCM) can translate queries in natural language to SQL; however, it did not have a module for dealing with aggregate functions, grouping and subqueries. In this paper a new module of this NLIDB for dealing with aggregate functions, grouping and subqueries is described, and experimental results are presented, which show that this interface has a performance (recall) better than that of C-Phrase.\n",
      "Submitted_date: 2018-01-11\n",
      "DOI link: https://doi.org/10.1007/978-3-319-71008-2_31\n",
      "Processing link: https://link.springer.com/article/10.1007/s41870-018-0095-2\n",
      "PDF: None\n",
      "Title: A system to transform natural language queries into SQL queries\n",
      "Authors: Arun Solanki & Ashutosh Kumar\n",
      "Abstract: In the present scenario, every user is not familiar with the use of Structured Query Language (SQL). So, user is not able to understand or write complex queries in SQL. An enhanced application with intelligent interface is needed to improve communication between the naïve user and the databases application. The database is an efficient structure for handling data. To understand the structure of the database, a user has to learn SQL. The non-expert users who are not familiar with the use of SQL, need a system by which the user can interact with the database in their natural language. The system must have the ability to understand the natural language and interact with the database accordingly. In this research, an improved system with three-tier architecture is developed. Pattern matching and semantic matching techniques are used to develop the system which transforms natural language into SQL queries. The SQL query is generated with the production rules and the predefined data dictionary. The predefined data dictionary contains semantics for attributes and relation among attributes. The input given by the user is transformed by the system into SQL query by passing through steps like tokenization, escape word removal, classification of elements and query formation. Finally, the output is in the form of a SQL query. The results are compared with the existing system. The results given by the proposed system are better than the existing system. The proposed system has better recall value, accuracy, and precision in comparison to existing systems.\n",
      "Submitted_date: 2017-06-02\n",
      "DOI link: https://doi.org/10.1007/s41870-018-0095-2\n",
      "Processing link: https://link.springer.com/article/10.1186/s40064-016-2164-y\n",
      "PDF: https://link.springer.com/content/pdf/10.1186/s40064-016-2164-y.pdf\n",
      "Title: Comparative study on the customization of natural language interfaces to databases\n",
      "Authors: Rodolfo A. Pazos R., Marco A. Aguirre L., Juan J. González B., José A. Martínez F. & Andrés A. Verástegui O.\n",
      "Abstract: In the last decades the popularity of natural language interfaces to databases (NLIDBs) has increased, because in many cases information obtained from them is used for making important business decisions. Unfortunately, the complexity of their customization by database administrators make them difficult to use. In order for a NLIDB to obtain a high percentage of correctly translated queries, it is necessary that it is correctly customized for the database to be queried. In most cases the performance reported in NLIDB literature is the highest possible; i.e., the performance obtained when the interfaces were customized by the implementers. However, for end users it is more important the performance that the interface can yield when the NLIDB is customized by someone different from the implementers. Unfortunately, there exist very few articles that report NLIDB performance when the NLIDBs are not customized by the implementers. This article presents a semantically-enriched data dictionary (which permits solving many of the problems that occur when translating from natural language to SQL) and an experiment in which two groups of undergraduate students customized our NLIDB and English language frontend (ELF), considered one of the best available commercial NLIDBs. The experimental results show that, when customized by the first group, our NLIDB obtained a 44.69 % of correctly answered queries and ELF 11.83 % for the ATIS database, and when customized by the second group, our NLIDB attained 77.05 % and ELF 13.48 %. The performance attained by our NLIDB, when customized by ourselves was 90 %.\n",
      "Submitted_date: 2015-11-27\n",
      "DOI link: None\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-319-46562-3_29\n",
      "PDF: None\n",
      "Title: Natural Language Interface to Relational Database (NLI-RDB) Through Object Relational Mapping (ORM)\n",
      "Authors: Abdullah Alghamdi, Majdi Owda & Keeley Crockett\n",
      "Abstract: This paper proposes a novel approach for building a Natural Language Interface to a Relational Database (NLI-RDB) using Conversational Agent (CA), Information Extraction (IE) and Object Relational Mapping (ORM) framework. The CA will help in disambiguating the user’s queries and guiding the user interaction. IE will play an important role in named entities extraction in order to map Natural Language queries into database queries. The ORM framework i.e. the Hibernate framework resolves the impedance mismatch between the Object Oriented Paradigms (OOP) and Relational Databases (RDBs) i.e. OOP concepts differ from RDB concepts, thus it reduces the complexity in generating SQL statements. Also, by utilizing ORM framework, the RDBs entities are mapped into real world objects, which bring the RDBs a step closer to the user. In addition, the ORM framework simplify the interaction between OOP and RDBs. The developed NLI-RDB system allows the user to interact with objects directly in natural language and through navigation, rather than by using SQL statements. This direct interaction tends to be easier and more acceptable for humans whom are nor technically orientated and have no SQL knowledge. The NLI-RDB system also offers friendly and interactive user interface in order to refine the query generated automatically. The NLI-RDB system has been evaluated by a group of participants through a combination of qualitative and quantitative measures. The experimental results show good performance of the prototype and excellent user’s satisfaction.\n",
      "Submitted_date: 2016-09-07\n",
      "DOI link: https://doi.org/10.1007/978-3-319-46562-3_29\n",
      "Processing link: https://link.springer.com/article/10.1007/s00766-015-0224-4\n",
      "PDF: None\n",
      "Title: TiQi: answering unstructured natural language trace queries\n",
      "Authors: Piotr Pruski, Sugandha Lohar, William Goss, Alexander Rasin & Jane Cleland-Huang\n",
      "Abstract: Software traceability is a required element in the development and certification of safety-critical software systems. However, trace links, which are created at significant cost and effort, are often underutilized in practice due primarily to the fact that project stakeholders often lack the skills needed to formulate complex trace queries. To mitigate this problem, we present a solution which transforms spoken or written natural language queries into structured query language (SQL). TiQi includes a general database query mechanism and a domain-specific model populated with trace query concepts, project-specific terminology, token disambiguators, and query transformation rules. We report results from four different experiments exploring user preferences for natural language queries, accuracy of the generated trace queries, efficacy of the underlying disambiguators, and stability of the trace query concepts. Experiments are conducted against two different datasets and show that users have a preference for written NL queries. Queries were transformed at accuracy rates ranging from 47 to 93 %.\n",
      "Submitted_date: 2014-10-25\n",
      "DOI link: https://doi.org/10.1007/s00766-015-0224-4\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-642-33021-6_36\n",
      "PDF: None\n",
      "Title: Natural Language Interfaces to Databases: An Analysis of the State of the Art\n",
      "Authors: Rodolfo A. Pazos R., Juan J. González B., Marco A. Aguirre L., José A. Martínez F. & Héctor J. Fraire H.\n",
      "Abstract: People constantly make decisions based on information, most of which is stored in databases. Accessing this information requires the use of query languages to databases such as SQL. In order to avoid the difficulty of using these languages for users who are not computing experts, Natural Language Interfaces for Databases (NLIDB) have been developed, which permit to query databases through queries formulated in natural language. Although since the 60s many NLIDBs have been developed, their performance has not been satisfactory, there still remain very difficult problems that have not been solved by NLIDB technology, and there does not yet exist a standardized method of evaluation that permits to compare the performance of different NLIDBs. This chapter presents an analysis of NLIDBs, which includes their classification, techniques, advantages, disadvantages, and a proposal for a proper evaluation of them.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-642-33021-6_36\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-642-45260-4_5\n",
      "PDF: None\n",
      "Title: Automatic Generation and Reranking of SQL-Derived Answers to NL Questions\n",
      "Authors: Alessandra Giordani & Alessandro Moschitti\n",
      "Abstract: In this paper, given a relational database, we automatically translate a natural language question into an SQL query retrieving the correct answer. We exploit the structure of the DB to generate a set of candidate SQL queries, which we rerank with a SVM-ranker based on tree kernels. In particular we use linguistic dependencies in the natural language question and the DB metadata to build a set of plausible SELECT, WHERE and FROM clauses enriched with meaningful joins. Then, we combine all the clauses to get the set of all possible SQL queries, producing candidate queries to answer the question. This approach can be recursively applied to deal with complex questions, requiring nested queries. We sort the candidates in terms of scores of correctness using a weighting scheme applied to the query generation rules. Then, we use a SVM ranker trained with structural kernels to reorder the list of question and query pairs, where both members are represented as syntactic trees. The f-measure of our model on standard benchmarks is in line with the best models (85% on the first question), which use external and expensive hand-crafted resources such as the semantic interpretation. Moreover, we can provide a set of candidate answers with a Recall of the answer of about 92% and 96% on the first 2 and 5 candidates, respectively.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-642-45260-4_5\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-642-31178-9_16\n",
      "PDF: None\n",
      "Title: Generating SQL Queries Using Natural Language Syntactic Dependencies and Metadata\n",
      "Authors: Alessandra Giordani & Alessandro Moschitti\n",
      "Abstract: This research concerns with translating natural language questions into SQL queries by exploiting the MySQL framework for both hypothesis construction and thesis verification in the task of question answering. We use linguistic dependencies and metadata to build sets of possible SELECT and WHERE clauses. Then we exploit again the metadata to build FROM clauses enriched with meaningful joins. Finally, we combine all the clauses to get the set of all possible SQL queries, producing an answer to the question. Our algorithm can be recursively applied to deal with complex questions, requiring nested SELECT instructions. Additionally, it proposes a weighting scheme to order all the generated queries in terms of probability of correctness.\n",
      "Our preliminary results are encouraging as they show that our system generates the right SQL query among the first five in the 92% of the cases. This result can be greatly improved by re-ranking the queries with a machine learning methods.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-642-31178-9_16\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-642-25324-9_24\n",
      "PDF: None\n",
      "Title: Semantic Model for Improving the Performance of Natural Language Interfaces to Databases\n",
      "Authors: Rodolfo A. Pazos R., Juan J. González B. & Marco A. Aguirre L.\n",
      "Abstract: Despite the fact that since the late 60s many Natural Language Interfaces to Databases (NLIDBs) have been developed, up to now many problems continue, which prevent the translation process from natural language to SQL to be totally successful. Some of the main problems that have been encountered relate to 1) achieving domain independence, 2) the use of words or phrases of different syntactic categories for referring to tables and columns, and 3) semantic ellipsis. This paper introduces a new method for modeling databases that includes relevant information for improving the performance of NLIDBs. This method will be useful for solving many problems found in the translation from natural language to SQL, using a database model that contains linguistic information that provides more semantic information than that found in conventional database models (such as the extended entity-relationship model) and those used in previous NLIDBs.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-642-25324-9_24\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-642-21455-4_14\n",
      "PDF: None\n",
      "Title: Kernel-Based Machines for Abstract and Easy Modeling of Automatic Learning\n",
      "Authors: Alessandro Moschitti\n",
      "Abstract: The modeling of system semantics (in several ICT domains) by means of pattern analysis or relational learning is a product of latest results in statistical learning theory. For example, the modeling of natural language semantics expressed by text, images, speech in information search (e.g. Google, Yahoo,..) or DNA sequence labeling in Bioinformatics represent distinguished cases of successful use of statistical machine learning. The reason of this success is due to the ability to overcome the concrete limitations of logic/rule-based approaches to semantic modeling: although, from a knowledge engineer perspective, rules are natural methods to encode system semantics, noise, ambiguity and errors affecting dynamic systems, prevent such approached from being effective, e.g. they are not flexible enough.\n",
      "In contrast, statistical relational learning, applied to representations of system states, i.e. training examples, can produce semantic models of system behavior based on a large number attributes. As the values of the latter are automatically learned, they reflect the flexibility of statistical settings and the overall model is robust to unexpected system condition changes. Unfortunately, while attribute weight and their relations with other attributes can be automatically learned from examples, their design for representing the target object (e.g. a system state) has to be manually carry out. This requires expertise, intuition and deep knowledge about the expected system behavior. A typical difficult task is for example the conversion of structures into attribute-value representations.\n",
      "Kernel Methods are powerful techniques designed within the statistical learning theory. They can be used in learning algorithms in place of attributes, thus simplifying object representation. More specifically, kernel functions can define structural and semantic similarities between objects (e.g. states) at abstract level, replacing the similarity defined in terms of attribute overlap.\n",
      "In this chapter, we provide the basic notions of machine learning along with latest theoretical results obtained in recent years. First, we show traditional and simple machine learning algorithms based on attribute-value representations and probability notions such as the Naive Bayes and the Decision Tree classifiers. Second, we introduce the PAC learning theory and the Perceptron algorithm to provide the readers with essential concepts of modern machine learning. Finally, we use the above background to illustrate a simplified theory of Support Vector Machines, which, along with the kernel methods, are the ultimate product of the statistical learning theory.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-642-21455-4_14\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-642-15390-7_21\n",
      "PDF: None\n",
      "Title: Dialogue Manager for a NLIDB for Solving the Semantic Ellipsis Problem in Query Formulation\n",
      "Authors: Rodolfo A. Pazos R., José A. Martínez F. & Juan J. Gonzalez B.\n",
      "Abstract: A query written in natural language (NL) may involve several linguistic problems that cause a query not being interpreted or translated correctly into SQL. One of these problems is implicit information or semantic ellipsis, which can be understood as the omission of important words in the wording of a query written in NL. An exhaustive survey on NLIDB works has revealed that most of these works has not systematically dealt with semantic ellipsis. In experiments conducted on commercial NLIDBs, very poor results have been obtained (7% to 16.9%) when dealing with query corpora that involve semantic ellipsis. In this paper we propose a dialogue manager (DM) for a NLIDB for solving semantic ellipsis problems. The operation of this DM is based on a typification of elliptical problems found in queries, which permits to systematically deal with this problem. Additionally, the typification has two important characteristics: domain independence, which permits the typification to be applied to queries of different databases, and generality, which means that it holds for different languages such as English, French, Italian, Spanish, etc. These characteristics are inherited to the dialogue processes implemented in the DM, since they are based on this typification. In experiments conducted with this DM and a NLIDB on a corpus of elliptical queries, an increase of correctly answered queries of 30-35% was attained.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-642-15390-7_21\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-540-89694-4_44\n",
      "PDF: None\n",
      "Title: An Evolutionary Method for Natural Language to SQL Translation\n",
      "Authors: Alexandre Afonso & Leonardo Brito\n",
      "Abstract: In this paper, we propose a new methodology where complex natural language requests from a user to a relational database are broken into simple sentences through an Evolutionary Computing method. Such basic sentences are then translated by another module, which tries to perform a pattern matching between a model filled by local grammars and the basic sentences generated by the Evolutionary Programming algorithm. The output of this system is a set of SQL queries to a specific database. The main feature is its combinatorial approach, as an alternative for the use of methods that employs many linguistic levels (lexicon, syntax rules and semantics) and intermediate languages. The proposed methodology is applied to Brazilian Portuguese. In our test bed, a 92% translation correctness was achieved.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-540-89694-4_44\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-540-69858-6_46\n",
      "PDF: None\n",
      "Title: Mapping Natural Language into SQL in a NLIDB\n",
      "Authors: Alessandra Giordani\n",
      "Abstract: Since the 1970’s, there has been growing interest in understanding and answering human language questions. Despite this, little progress has been made in developing an interface that any untrained user can use to query very large databases using natural language. In this research, the design of a novel system is discussed. Tree-like structures are built for every question and each query, and a tree kernel, which represents trees in terms of their substructures, is used to define feature spaces. A machine learning algorithm is proposed that takes pairs of trees as training input and derives the unknown final SQL query by matching propositional and relational substructures.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-540-69858-6_46\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-540-72393-6_74\n",
      "PDF: None\n",
      "Title: An Improve to Human Computer Interaction, Recovering Data from Databases Through Spoken Natural Language\n",
      "Authors: Omar Florez-Choque & Ernesto Cuadros-Vargas\n",
      "Abstract: The fastest and most straightforward way of communication for mankind is the voice. Therefore, the best way to interact with computers should be the voice too. That is why at the moment men are searching new ways to interact with computers. This interaction is improved if the words spoken by the speaker are organized in Natural Language.\n",
      "In this article, it is proposed a model to recover information from databases through queries in Spanish Natural Language using the voice as the way of communication. This model incorporates a Hybrid Intelligent System based on Genetic Algorithms and a Kohonen Self-Organizing Map (SOM) to recognize the present phonemes in a word through time. This approach allows us to remake up a word with speaker independence. Furthermore, it is proposed the use of a compiler with type 2 grammar according to the Chomsky Hierarchy to support the syntactic and semantic structure in Spanish language. Our experiments suggest that the Spoken Natural Language improves notably the Human-Computer interaction when compared with traditional input methods such as: mouse or keybord.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-540-72393-6_74\n",
      "Processing link: https://link.springer.com/chapter/10.1007/11925231_88\n",
      "PDF: None\n",
      "Title: Issues in Translating from Natural Language to SQL in a Domain-Independent Natural Language Interface to Databases\n",
      "Authors: Rodolfo A. Pazos Rangel & O. Joaquín Pérez\n",
      "Abstract: This paper deals with a domain-independent natural language interface to databases (NLIDB) for the Spanish language. This NLIDB had been previously tested for the Northwind and Pubs domains and had attained good performance (86% success rate). However, domain independence complicates the task of achieving high translation success, and to this end the ATIS (Air Travel Information System) database, which has been used by several natural language interfaces, was selected to conduct a new evaluation. The purpose of this evaluation was to asses the efficiency of the interface after the reconfiguration for another domain and to detect the problems that affect translation success. For the tests a corpus of queries was gathered and the results obtained showed that the interface can easily be reconfigured and that attained a 50% success rate. When the found problems concerning query translation were analyzed, wording deficiencies of some user queries and several errors in the synonym dictionary were discovered. After correcting these problems a second test was conducted, in which the interface attained a 61.4% success rate. These experiments showed that user training is necessary as well as a dialogue system that permits to clarify a query when it is deficiently formulated.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/11925231_88\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-540-27779-8_9\n",
      "PDF: None\n",
      "Title: Schema-Based Natural Language Semantic Mapping\n",
      "Authors: Niculae Stratica & Bipin C. Desai\n",
      "Abstract: This paper addresses the problem of mapping Natural Language to SQL queries. It assumes that the input is in English language and details a methodology to build a SQL query based on the input sentence, a dictionary and a set of production rules. The dictionary consists of semantic sets and index files. A semantic set is created for each table or attribute name and contains synonyms, hyponyms and hypernyms as retrieved by WordNet and complemented manually. The index files contain pointers to records in the database, ordered by value and by data type. The dictionary and the production rules form a context-free grammar for producing the SQL queries. The context ambiguities are addressed through the use of the derivationally related forms based on WordNet. Building the run time semantic sets of the input tokens helps solving the ambiguities related to the database schema. The proposed method introduces two functional entities: a pre-processor and a runtime engine. The pre-processor reads the database schema and uses WordNet to create the semantic sets and the set of production rules. It also reads the database records and creates the index files. The run time engine matches the input tokens to the dictionary and uses the rules to create the corresponding SQL query.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-540-27779-8_9\n",
      "Processing link: https://link.springer.com/chapter/10.1007/3-540-46154-X_16\n",
      "PDF: None\n",
      "Title: Spanish Natural Language Interface for a Relational Database Querying System\n",
      "Authors: Rodolfo A. Pazos Range\n",
      "Abstract: The fast growth of Internet is creating a society where the demand on information storage, organization, access, and analysis services is continuously growing. This constantly increases the number of inexperienced users that need to access databases in a simple way. Together with the emergence of voice interfaces, such a situation foretells a promising future for database querying systems using natural language interfaces. We describe the architecture of a relational database querying system using a natural language (Spanish) interface, giving a brief explanation of the implementation of each of the constituent modules: lexical parser, syntax checker, and semantic analyzer.\n",
      "Submitted_date: 2002-08-23\n",
      "DOI link: https://doi.org/10.1007/3-540-46154-X_16\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-1-4471-3423-7_24\n",
      "PDF: None\n",
      "Title: The Experiences of New Users of a Natural Language Interface to a Relational Database in a Controlled Setting\n",
      "Authors: John E. Bell\n",
      "Abstract: This paper describes the experiences encountered by new users of a natural language interface for ad hoc query of a relational database. During the study, subjects with wide ranging computer experience performed queries of varying complexity using a commercial natural language system. Their experiences are compared with those of similar subjects working on artificial language and graphical user interfaces doing the same queries. The study revealed strengths and weaknesses of the natural language interface studied as well as natural language interfaces for database query in general, and it showed that interaction with the natural language interface was qualitatively different than interaction with either of the other systems while the overall performance was quantitatively very similar.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-1-4471-3423-7_24\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-981-16-5529-6_6\n",
      "PDF: None\n",
      "Title: Generation of Structured Query Language from Natural Language Using Recurrent Neural Networks\n",
      "Authors: Lubna Mohammed Kunhi & Jyothi Shetty\n",
      "Abstract: With the rise of the digital era, data has become a prominent resource. A humongous amount of data is generated every single day. This data, typically raw, is said to be in an unstructured format and is converted into a structured format to derive meaning and value and often stored in huge databases. The data is then queried upon using querying languages like SQL. This is where the generation of SQL from natural language comes into the picture. Natural language processing is a technology used to help computers understand the natural language of humans. While it is easier for humans to communicate in natural language, it is almost impossible for a computer to master. By providing an efficient method to convert NL to SQL, this paper provides an overview of the various approaches in existence, and further goes on to develop a mechanism for the same using recurrent neural networks. The proposed approach makes use of the Spider dataset and has been differentiated to account for the accuracy for various clauses including JOIN operations in an SQL query.\n",
      "Submitted_date: 2022-01-11\n",
      "DOI link: https://doi.org/10.1007/978-981-16-5529-6_6\n",
      "Processing link: https://link.springer.com/article/10.1007/s00778-025-00921-z\n",
      "PDF: None\n",
      "Title: Auto-tables: synthesizing multi-step transformations to relationalize tables without using examples\n",
      "Authors: Peng Li\n",
      "Abstract: Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables “in the wild”. Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Tableau forums. We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.\n",
      "Submitted_date: 2024-09-21\n",
      "DOI link: https://doi.org/10.1007/s00778-025-00921-z\n",
      "Processing link: https://link.springer.com/article/10.1038/s41598-021-98019-3\n",
      "PDF: https://link.springer.com/content/pdf/10.1038/s41598-021-98019-3.pdf\n",
      "Title: Translating synthetic natural language to database queries with a polyglot deep learning framework\n",
      "Authors: Adrián Bazaga, Nupur Gunwant & Gos Micklem\n",
      "Abstract: The number of databases as well as their size and complexity is increasing. This creates a barrier to use especially for non-experts, who have to come to grips with the nature of the data, the way it has been represented in the database, and the specific query languages or user interfaces by which data are accessed. These difficulties worsen in research settings, where it is common to work with many different databases. One approach to improving this situation is to allow users to pose their queries in natural language. In this work we describe a machine learning framework, Polyglotter, that in a general way supports the mapping of natural language searches to database queries. Importantly, it does not require the creation of manually annotated data for training and therefore can be applied easily to multiple domains. The framework is polyglot in the sense that it supports multiple different database engines that are accessed with a variety of query languages, including SQL and Cypher. Furthermore Polyglotter supports multi-class queries. Good performance is achieved on both toy and real databases, as well as a human-annotated WikiSQL query set. Thus Polyglotter may help database maintainers make their resources more accessible.\n",
      "Submitted_date: 2021-05-12\n",
      "DOI link: None\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-35445-9_49\n",
      "PDF: None\n",
      "Title: Development of a Virtual View for Processing Complex Natural Language Queries\n",
      "Authors: José A. Martínez F., Rodolfo A. Pazos R. & Juana Gaspar H.\n",
      "Abstract: Natural language interfaces to databases (NLIDBs) allow inexperienced users to formulate questions in natural language (NL) to DB management systems. A survey of the literature on NLIDBs shows that the prevalence of the interfaces developed to date are limited by two aspects: it is difficult to port them from one database (DB) to another, and they fail on some questions, either because they misinterpret the questions or they are unable to respond even when the answer exists. Among the problems that occur in NL questions, there is a problem that may arise when querying complex DBs: semantically implied entities. This problem is related to the semantic meaning of a query, when upon referring to an entity (DB table), another entity (or entities) is semantically implied to which the first entity is related, and usually the user ignores the relationship between the two (or more) entities. This chapter describes a method for using a virtual view in a NLIDB, which allows processing queries that involve this problem more efficiently than a previous version of the interface that uses materialized views. Specifically, experimental results show that the new version reduces by 23% the time for translating questions from NL to SQL.\n",
      "Submitted_date: 2020-02-28\n",
      "DOI link: https://doi.org/10.1007/978-3-030-35445-9_49\n",
      "Processing link: https://link.springer.com/article/10.1007/s00778-019-00567-8\n",
      "PDF: https://link.springer.com/content/pdf/10.1007/s00778-019-00567-8.pdf\n",
      "Title: A comparative survey of recent natural language interfaces for databases\n",
      "Authors: Katrin Affolter & Kurt Stockinger\n",
      "Abstract: Over the last few years, natural language interfaces (NLI) for databases have gained significant traction both in academia and industry. These systems use very different approaches as described in recent survey papers. However, these systems have not been systematically compared against a set of benchmark questions in order to rigorously evaluate their functionalities and expressive power. In this paper, we give an overview over 24 recently developed NLIs for databases. Each of the systems is evaluated using a curated list of ten sample questions to show their strengths and weaknesses. We categorize the NLIs into four groups based on the methodology they are using: keyword-, pattern-, parsing- and grammar-based NLI. Overall, we learned that keyword-based systems are enough to answer simple questions. To solve more complex questions involving subqueries, the system needs to apply some sort of parsing to identify structural dependencies. Grammar-based systems are overall the most powerful ones, but are highly dependent on their manually designed rules. In addition to providing a systematic analysis of the major systems, we derive lessons learned that are vital for designing NLIs that can answer a wide range of user questions.\n",
      "Submitted_date: 2018-09-18\n",
      "DOI link: https://doi.org/10.1007/s00778-019-00567-8\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-030-21074-8_15\n",
      "PDF: None\n",
      "Title: Deep Reader: Information Extraction from Document Images via Relation Extraction and Natural Language\n",
      "Authors: D. Vishwanath, Rohit Rahul, Gunjan Sehgal,  Swati, Arindam Chowdhury, Monika Sharma, Lovekesh Vig & Gautam Shroff\n",
      "Abstract: Recent advancements in the area of Computer Vision with state-of-art Neural Networks has given a boost to Optical Character Recognition (OCR) accuracies. However, extracting characters/text alone is often insufficient for relevant information extraction as documents also have a visual structure that is not captured by OCR. Extracting information from tables, charts, footnotes, boxes, headings and retrieving the corresponding structured representation for the document remains a challenge and finds application in a large number of real-world use cases. In this paper, we propose a novel enterprise based end-to-end framework called DeepReader which facilitates information extraction from document images via identification of visual entities and populating a meta relational model across different entities in the document image. The model schema allows for an easy to understand abstraction of the entities detected by the deep vision models and the relationships between them. DeepReader has a suite of state-of-the-art vision algorithms which are applied to recognize handwritten and printed text, eliminate noisy effects, identify the type of documents and detect visual entities like tables, lines and boxes. Deep Reader maps the extracted entities into a rich relational schema so as to capture all the relevant relationships between entities (words, textboxes, lines etc.) detected in the document. Relevant information and fields can then be extracted from the document by writing SQL queries on top of the relationship tables. A natural language based interface is added on top of the relationship schema so that a non-technical user, specifying the queries in natural language, can fetch the information with minimal effort. In this paper, we also demonstrate many different capabilities of Deep Reader and report results on a real-world use case.\n",
      "Submitted_date: 2019-06-19\n",
      "DOI link: https://doi.org/10.1007/978-3-030-21074-8_15\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-319-71008-2_33\n",
      "PDF: None\n",
      "Title: Issues in Querying Databases with Design Anomalies Using Natural Language Interfaces\n",
      "Authors: Rodolfo A. Pazos R., José A. Martínez F., Alan G. Aguirre L. & Marco A. Aguirre L.\n",
      "Abstract: Accessing information is a vital activity in businesses; therefore, databases (DBs) have become necessary tools for storing their information. However, for accessing the information stored in a database, it is necessary to use a DB query language, such as SQL. Natural language interfaces to databases (NLIDBs) allow inexperienced users to obtain information from a DB using natural language expressions without the need of using a DB query language. Despite the relative effectiveness of NLIDBs, most of the approaches proposed for designing NLIDBs ignore the possibility that the DB to be queried could be poorly designed; i.e., it could have design anomalies. Unfortunately, various experiments (described in this paper) show that DB anomalies degrade the performance (recall) of NLIDBs. The purpose of this paper is to analyze the most common DB design anomalies for proposing solutions to this problem and avoid performance degradation of NLIDBs when accessing such DBs.\n",
      "Submitted_date: 2018-01-11\n",
      "DOI link: https://doi.org/10.1007/978-3-319-71008-2_33\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-319-30282-9_6\n",
      "PDF: None\n",
      "Title: Evaluating the Interpretation of Natural Language Trace Queries\n",
      "Authors: Sugandha Lohar, Jane Cleland-Huang & Alexander Rasin\n",
      "Abstract: [Context and Motivation:] In current practice, existing traceability data is often underutilized due to lack of accessibility and difficulties users have in constructing the complex SQL queries needed to address realistic Software Engineering questions. In our prior work we therefore presented TiQi – a natural language (NL) interface for querying software projects. TiQi has been shown to transform a set of trace queries collected from IT experts at accuracy rates ranging from 47 % to 93 %. [Question/problem:] However, users need to quickly determine whether TiQi has correctly understood the NL query. [Principal ideas/results:] TiQi needs to communicate the transformed query back to the user and provide support for disambiguation and correction. In this paper we report on three studies we conducted to compare the effectiveness of four query representation techniques. [Contribution:] We show that simultaneously displaying a visual query representation, SQL, and a sample of the data results enabled users to most accurately evaluate the correctness of the transformed query.\n",
      "Submitted_date: 2016-03-04\n",
      "DOI link: https://doi.org/10.1007/978-3-319-30282-9_6\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-319-05170-3_44\n",
      "PDF: None\n",
      "Title: Features and Pitfalls that Users Should Seek in Natural Language Interfaces to Databases\n",
      "Authors: Rodolfo A. Pazos Rangel, Marco A. Aguirre & Juan J. González\n",
      "Abstract: Natural Language Interfaces to Databases (NLIDBs) are tools that can be useful in making decisions, allowing different types of users to get information they need using natural language communication. Despite their important features and that for more than 50 years NLIDBs have been developed, their acceptance by end users is very low due to extremely complex problems inherent to natural language, their customization and internal operation, which has produced poor performance regarding queries correctly translated. This chapter presents a study on the main desirable features that NLIDBs should have as well as their pitfalls, describing some study cases that occur in some interfaces to illustrate the flaws of their approach.\n",
      "Submitted_date: 2014-03-27\n",
      "DOI link: https://doi.org/10.1007/978-3-319-05170-3_44\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-90-481-9112-3_87\n",
      "PDF: None\n",
      "Title: An Approach for Developing Natural Language Interface to Databases Using Data Synonyms Tree and Syntax State Table\n",
      "Authors: Safwan shatnawi & Rajeh Khamis\n",
      "Abstract: The basic idea addressed in this research is developing a generic, dynamic, and domain independent natural language interface to databases. The approach consists of two phases; configuration phase and operation phase. The former builds data synonyms tree based on the database being implemented. The idea behind this tree is matching the natural language words with database elements. The tree hierarchy contains the database tables, attributes, attribute descriptions, and all possible synonyms for each description. The latter phase contains a technique that implements syntax state table to extract the SQL components from the natural language user request. As a result the corresponding SQL statement is generated without interference of human experts.\n",
      "Submitted_date: 2010-05-20\n",
      "DOI link: https://doi.org/10.1007/978-90-481-9112-3_87\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-540-87391-4_81\n",
      "PDF: None\n",
      "Title: Shedding Light on a Troublesome Issue in NLIDBS\n",
      "Authors: Rodolfo Pazos, René Santaolalaya S., Juan C. Rojas P. & Joaquín Pérez O.\n",
      "Abstract: A natural language interface to databases (NLIDB) without help mechanisms that permit clarifying queries is prone to incorrect query translation. In this paper we draw attention to a problem in NLIDBs that has been overlooked and has not been dealt with systematically: word economy; i.e., the omission of words when expressing a query in natural language (NL). In order to get an idea of the magnitude of this problem, we conducted experiments on EnglishQuery when applied to a corpora of economized-wording queries. The results show that the percentage of correctly answered queries is 18%, which is substantially lower than those obtained with corpora of regular queries (53%–83%). In this paper we describe a typification of problems found in economized-wording queries, which has been used to implement domain-independent dialog processes for an NLIDB in Spanish. The incorporation of dialog processes in an NLIDB permits users to clarify queries in NL, thus improving the percentage of correctly answered queries. This paper presents the tests of a dialog manager that deals with four types of query problems, which permits to improve the percentage of correctly answered queries from 60% to 91%. Due to the generality of our approach, we claim that it can be applied to other domain-dependent or domain-independent NLIDBs, as well as other NLs such as English, French, Italian, etc.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-540-87391-4_81\n",
      "Processing link: https://link.springer.com/chapter/10.1007/978-3-540-73351-5_2\n",
      "PDF: None\n",
      "Title: An Efficient Denotational Semantics for Natural Language Database Queries\n",
      "Authors: Richard A. Frost & Randy J. Fortier\n",
      "Abstract: Early work on natural language database query processing focused on theories of compositional semantics. Recent work concentrates on the translation of NL queries to SQL where semantics is primarily used in an ad hoc manner to guide syntactic translation. Here, we argue that there remains a need for an efficiently-implementable denotational semantics for NL DB queries, and show how this can be achieved by integrating a relatively little-known semantics for transitive verbs with a new efficiently-implementable semantics for negation.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/978-3-540-73351-5_2\n",
      "Processing link: https://link.springer.com/chapter/10.1007/1-4020-3304-4_28\n",
      "PDF: None\n",
      "Title: A Framework for Developing Conversational User Interfaces\n",
      "Authors: James Glass, Eugene Weinstein, Scott Cyphers & Joseph Polifroni\n",
      "Abstract: In this work we report our efforts to facilitate the creation of mixed-initiative conversational interfaces for novice and experienced developers of human language technology. Our focus has been on a framework that allows developers to easily specify the basic concepts of their applications, and rapidly prototype conversational interfaces for a variety of configurations. In this paper we describe the current knowledge representation, the compilation processes for speech understanding, generation, and dialogue turn management, as well as the user interfaces created for novice users and more experienced developers. Finally, we report our experiences with several user groups in which developers used this framework to prototype a variety of conversational interfaces.\n",
      "Submitted_date: None\n",
      "DOI link: https://doi.org/10.1007/1-4020-3304-4_28\n",
      "Processing link: https://link.springer.com/chapter/10.1007/3-540-54563-8_76\n",
      "PDF: None\n",
      "Title: Multilevel interface to a distributed database system\n",
      "Authors: Bipin C. Desai\n",
      "Abstract: In this paper we present the issues involved in the design of a Multilevel Interface to a Heterogeneous Distributed Database Management Systems. The system provides interaction with the database using natural language, general formal language and a target formal language. Issues of syntactic analyses, semantic analyses, knowledge representation, query interpretation and generation of query in a formal query language are discussed in the paper.\n",
      "Submitted_date: 2005-05-28\n",
      "DOI link: https://doi.org/10.1007/3-540-54563-8_76\n",
      "Processing link: https://link.springer.com/article/10.1007/s11432-024-4222-0\n",
      "PDF: None\n",
      "Title: The rise and potential of large language model based agents: a survey\n",
      "Authors: Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Xipeng Qiu, Xuanjing Huang & Qi Zhang\n",
      "Abstract: For a long time, researchers have sought artificial intelligence (AI) that matches or exceeds human intelligence. AI agents, which are artificial entities capable of sensing the environment, making decisions, and taking actions, are seen as a means to achieve this goal. Extensive efforts have been made to develop AI agents, with a primary focus on refining algorithms or training strategies to enhance specific skills or particular task performance. The field, however, lacks a sufficiently general and powerful model to serve as a foundation for building general agents adaptable to diverse scenarios. With their versatile capabilities, large language models (LLMs) pave a promising path for the development of general AI agents, and substantial progress has been made in the realm of LLM-based agents. In this article, we conduct a comprehensive survey on LLM-based agents, covering their construction frameworks, application scenarios, and the exploration of societies built upon LLM-based agents. We also conclude some potential future directions and open problems in this flourishing field.\n",
      "Submitted_date: 2024-09-07\n",
      "DOI link: https://doi.org/10.1007/s11432-024-4222-0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from DrissionPage import ChromiumPage\n",
    "\n",
    "paper = pd.read_csv('springer/all_springer_papers.csv')\n",
    "\n",
    "titles = []\n",
    "authors = []\n",
    "pdf_links = []\n",
    "abstracts = [] \n",
    "dois = []\n",
    "submitted_dates = []\n",
    "\n",
    "page = ChromiumPage()\n",
    "for link in paper['link']:\n",
    "\n",
    "    print(f\"Processing link: {link}\")\n",
    "\n",
    "    title, author, pdf_link, abstract, doi, submitted_date = extract_detail(page, link)\n",
    "    titles.append(title)\n",
    "    authors.append(author)\n",
    "    pdf_links.append(pdf_link)\n",
    "    abstracts.append(abstract)\n",
    "    dois.append(doi)\n",
    "    submitted_dates.append(submitted_date)\n",
    "\n",
    "# Create a DataFrame with the extracted details\n",
    "paper['pdf_link'] = pdf_links\n",
    "paper['title'] = titles\n",
    "paper['authors'] = authors\n",
    "paper['abstract'] = abstracts\n",
    "paper['submitted'] = submitted_dates\n",
    "paper['doi'] = dois\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "paper.to_csv('all_springer_papers.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a8d0295",
   "metadata": {},
   "outputs": [],
   "source": [
    "page.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb543d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
