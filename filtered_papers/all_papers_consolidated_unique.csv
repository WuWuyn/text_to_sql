link,title,authors,pdf_link,abstract,doi,submitted
https://ieeexplore.ieee.org/document/11024318/,To Mock or Not to Mock: Divergence in Mocking Practices Between LLM and Developers,['Hanbin Qin'],,"Mock objects are essential for isolating unit tests and reducing dependencies in software testing. However, deciding what to mock requires careful judgment to balance isolation and maintainability. This study evaluates OpenAI's GPT-4o for automating mock decisions by comparing its outputs with developer choices. The findings reveal that while the LLM excels in identifying dependencies, their broader isolation strategy often results in Over-mocking compared to the developers. These insights suggest the potential for LLM-based tools to generate test cases with accurate and well-balanced mocking strategies.",https://doi.org/10.1109/ICSE-Companion66252.2025.00077,27 April 2025 - 03 May 2025
https://ieeexplore.ieee.org/document/10988981/,Leveraging Large Language Models for Explicit Wait Management in End-to-End Web Testing,"['Dario Olianas', 'Maurizio Leotta', 'Filippo Ricca']",,"End-to-end (E2E) testing is an approach in which an application is automatically tested through scripts that simulate the actions a user would perform. Properly managing asynchronous interactions is crucial in this approach to avoid test failures and flakiness. In the Selenium WebDriver framework, this is typically addressed by using thread sleeps (which pause the test for a fixed time) or explicit waits (function calls that pause the test execution until a specified condition is met). Explicit waits require the selection of both a condition to wait for (e.g., element visibility, element clickability) and an element on which that condition applies. Since thread sleeps are unreliable and replacing them with appropriate explicit waits is a time consuming task, in this work, we leverage a Large Language Model (LLM) to assist testers in selecting the most appropriate explicit waits. We defined a structured procedure (a series of prompts) for engaging with the LLM and validated this approach empirically on three test suites affected by asynchronous waiting issues, as well as on 12 synthetic examples. Additionally, we compared our approach with SleepReplacer, the current state-of-the-art tool for replacing thread sleeps with explicit waits in E2E web test suites. The results show that the LLM-based approach can automatically replace the majority of thread sleeps in a test suite on the first attempt, outperforming SleepReplacer.",https://doi.org/10.1109/ICST62969.2025.10988981,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10904141/,Exploring the Integration of Generative AI Tools in Software Testing Education: A Case Study on ChatGPT and Copilot for Preparatory Testing Artifacts in Postgraduate Learning,"['Susmita Haldar', 'Mary Pierce', 'Luiz Fernando Capretz']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10904141,"Software testing education is important for building qualified testing professionals. To ensure that software testing graduates are ready for real-world challenges, it is necessary to integrate modern tools and technologies into the curriculum. With the emergence of Large Language Models (LLMs), their potential use in software engineering has become a focus, but their application in software testing education remains largely unexplored. This study, conducted in the Capstone Project course of a postgraduate software testing program, was carried out over two semesters with two distinct groups of students. A custom-built Travel Application limited to a web platform was used in the first semester. In the second semester, a new set of students worked with an open-source application, offering a larger-scale, multi-platform experience across web, desktop, and mobile platforms. Students initially created preparatory testing artifacts manually as a group deliverable. Following this, they were assigned an individual assignment to generate the same artifacts using LLM tools such as ChatGPT 3.5 in the first semester and Microsoft Copilot in the second. This process directly compared manually created artifacts and those generated using LLMs, leveraging AI for faster outputs. After completion, they responded to a set of assigned questions. The students’ responses were assessed using an integrated methodology, including quantitative and qualitative assessments, sentiment analysis to understand emotions, and a thematic approach to extract deeper insights. The findings revealed that while LLMs can assist and augment manual testing efforts, they cannot entirely replace the need for manual testing. By incorporating innovative technology into the curriculum, this study highlights how Generative AI can support active learning, connect theoretical concepts with practical applications, and align educational practices with industry needs.",https://doi.org/10.1109/ACCESS.2025.3545882,
https://ieeexplore.ieee.org/document/10989038/,Evaluation of the Choice of LLM in a Multi-Agent Solution for GUI-Test Generation,"['Stevan Tomic', 'Emil Alégroth', 'Maycel Isaac']",,"Automated testing, particularly for GUI-based systems, remains a costly and labor-intensive process and prone to errors. Despite advancements in automation, manual testing still dominates in industrial practice, resulting in delays, higher costs, and increased error rates. Large Language Models (LLMs) have shown great potential to automate tasks traditionally requiring human intervention, leveraging their cognitive-like abilities for test generation and evaluation. In this study, we present PathFinder, a Multi-Agent LLM (MALLM) framework that incorporates four agents responsible for (a) perception and summarization, (b) decision-making, (c) input handling and extraction, and (d) validation, which work collaboratively to automate exploratory web-based GUI testing. The goal of this study is to assess how different LLMs, applied to different agents, affect the efficacy of automated exploratory GUI testing. We evaluate PathFinder with three models, Mistral-Nemo, Gemma2, and Llama3.1, on four e-commerce websites. Thus, 27 permutations of the LLMs, across three agents (excluding the validation agent), to test the hypothesis that a solution with multiple agents, each using different LLMs, is more efficacious (efficient and effective) than a multi-agent solution where all agents use the same LLM. The results indicate that the choice of LLM constellation (combination of LLMs) significantly impacts efficacy, suggesting that a single LLM across agents may yield the best balance of efficacy (measured by F1-score). Hypothesis to explain this result include, but are not limited to: improved decision-making consistency and reduced task coordination discrepancies. The contributions of this study are an architecture for MALLM-based GUI testing, empirical results on its performance, and novel insights into how LLM selection impacts the efficacy of automated testing.",https://doi.org/10.1109/ICST62969.2025.10989038,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10876166/,A Retrospective on Whole Test Suite Generation: On the Role of SBST in the Age of LLMs,"['Gordon Fraser', 'Andrea Arcuri']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10876166,"This paper presents a retrospective of the article “Whole Test Suite Generation”, published in the IEEE Transactions on Software Engineering, in 2012. We summarize its main contributions, and discuss how this work impacted the research field of Search-Based Software Testing (SBST) in the last 12 years. The novel techniques presented in the paper were implemented in the tool EvoSuite, which has been so far the state-of-the-art in unit test generation for Java programs using SBST. SBST has shown practical and impactful applications, creating the foundations to open the doors to tackle several other software testing problems besides unit testing, like for example system testing of Web APIs with EvoMaster. We conclude our retrospective with our reflections on what lies ahead, especially considering the important role that SBST still plays even in the age of Large Language Models (LLMs).",https://doi.org/10.1109/TSE.2025.3539458,
https://ieeexplore.ieee.org/document/10910287/,Comparing the Adaptability of a Genetic Algorithm and an LLM-Based Framework for Automated Software Test Data Generation: In the Context of Web Applications,"['Sashini Wanigasekara', 'Dinesh Asanka', 'Chathura Rajapakse', 'Dilani Wickramaarachchi', 'Abhiru Wijesinghe']",,"In the fast-paced world of software development, ensuring software quality is paramount. Software Quality Assurance (SQA) plays a vital role, primarily through testing, which can be carried out manually or automatically. Yet, creating comprehensive test data (TD) for web applications can be a formidable task. Manual test data generation (TDG) is time-consuming and error prone. Automation of TDG has become increasingly important in the realm of software quality assurance as it enables efficient and effective testing of software systems. The need for an appropriate framework for automated TDG is critical to achieve comprehensive and reliable test coverage. Automated TDG offers significant advantages, including time and resource savings, improved test coverage, and seamless integration into the software development process. The core aim of this research is to bridge the gap between manual and existing automated methods, resulting in time and cost savings, heightened testing efficiency, and elevated software quality. Research objectives encompass comparing the adaptability of an AGA based automated TDG model and a LLM based automated TDG model to a web application. The results from the LLM model for triangle classification program was found to be potentially acceptable and accurate than the AGA model's results. This research discusses the challenges encountered when implementing and using the AGA-based framework in the web application context and how an LLM model could overcome the challenges. The study highlights the benefits of using the LLM approach, demonstrating its relevance and accuracy in generating test data compared to the Genetic Algorithm-based model. The practical implications for software quality assurance practices are discussed, emphasizing the enhanced efficiency and effectiveness of the LLM model in improving software quality.",https://doi.org/10.1109/ICDDS62937.2024.10910287,05-07 December 2024
https://ieeexplore.ieee.org/document/10989025/,LLMs in the Heart of Differential Testing: A Case Study on a Medical Rule Engine,"['Erblin Isaku', 'Christoph Laaber', 'Hassan Sartaj', 'Shaukat Ali', 'Thomas Schwitalla', 'Jan F. Nygård']",,"The Cancer Registry of Norway (CRN) uses an automated cancer registration support system (CaReSS) to support core cancer registry activities, i.e., data capture, data curation, and producing data products and statistics for various stakeholders. GURI is a core component of CaReSS, which is responsible for validating incoming data with medical rules. Such medical rules are manually implemented by medical experts based on medical standards, regulations, and research. Since large language models (LLMs) have been trained on a large amount of public information, including these documents, they can be employed to generate tests for GURI. Thus, we propose an LLM-based test generation and differential testing approach (LLMeDiff) to test GURI. We experimented with four different LLMs, two medical rule engine implementations, and 58 real medical rules to investigate the hallucination, success, time efficiency, and robustness of the LLMs to generate tests, and these tests' ability to find potential issues in GURI. Our results showed that GPT-3.5 hallucinates the least, is the most successful, and is generally the most robust; however, it has the worst time efficiency. Our differential testing revealed 22 medical rules where implementation inconsistencies were discovered (e.g., regarding handling rule versions). Finally, we provide insights for practitioners and researchers based on the results.",https://doi.org/10.1109/ICST62969.2025.10989025,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10660826/,Towards LLM-Assisted System Testing for Microservices,"['Mustafa Almutawa', 'Qusai Ghabrah', 'Marco Canini']",,"As modern applications are being designed in a distributed, Microservices Architecture (MSA), it becomes increasingly difficult to debug and test those systems. Typically, it is the role of software testing engineers or Quality Assurance (QA) engineers to write software tests to ensure the reliability of applications, but such a task can be labor-intensive and time-consuming. In this paper, we explore the potential of Large Language Models (LLMs) in assisting software engineers in generating test cases for software systems, with a particular focus on performing end-to-end (black-box) system testing on web-based MSA applications. We present our experience building Kashef, a software testing tool that utilizes the advanced capabilities of current LLMs in code generation and reasoning, and builds on top of the concept of communicative agents.",https://doi.org/10.1109/ICDCSW63686.2024.00011,23-23 July 2024
https://ieeexplore.ieee.org/document/10962456/,SleepReplacer-GPT: AI-Based Thread Sleep Replacement in Selenium WebDriver Tests,"['Dario Olianas', 'Maurizio Leotta', 'Filippo Ricca']",,"Ensuring the quality of modern web applications through end-to-end (E2E) testing is crucial, especially for dynamic systems like single-page applications. Managing asynchronous calls effectively is a key challenge, often addressed using thread sleeps or explicit waits. While thread sleeps are simple to use, they cause inefficiencies and flakiness, whereas explicit waits are more efficient but demand careful implementation.This work explores extending SleepReplacer, a tool that automatically replaces thread sleeps with explicit waits in Selenium WebDriver test suites. We aim to enhance its capabilities by integrating it with ChatGPT, enabling intelligent and automated replacement of thread sleeps with optimal explicit waits. This integration aims to improve code quality and reduce flakiness.We developed a structured procedure for interacting with ChatGPT and validated it on three test suites and synthetic examples covering diverse cases.Results show that the LLM-based approach correctly replaces thread sleeps with explicit waits on the first attempt, consistently outperforming SleepReplacer. These findings support integrating ChatGPT with SleepReplacer to create a smarter, more efficient tool for managing asynchronous behavior in test suites.",https://doi.org/10.1109/ICSTW64639.2025.10962456,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10831260/,Try-Then-Eval: Equipping an LLM-based Agent with a Two-Phase Mechanism to Solve Computer Tasks,"['Duy Cao', 'Phu Nguyen', 'Vy Le', 'Long Nguyen', 'Vu Nguyen']",,"Building an autonomous intelligent agent capable of carrying out web automation tasks from descriptions in natural language offers a wide range of applications, including software testing, virtual assistants, and task automation in general. However, recent studies addressing this problem often require manually constructing of prior human demonstrations. In this paper, we approach the problem by leveraging the idea of reinforcement learning (RL) with the two-phase mechanism to form an agent using LLMs for automating computer tasks without relying on human demonstrations. We evaluate our LLM-based agent using the MiniWob++ dataset of web-based application tasks, showing that our approach achieves 85% success rate without prior demonstrations. The results also demonstrate the agent's capability of self-improvement through training.",https://doi.org/10.1109/SMC54092.2024.10831260,06-10 October 2024
https://ieeexplore.ieee.org/document/10764850/,SoVAR: Building Generalizable Scenarios from Accident Reports for Autonomous Driving Testing,"['An Guo', 'Yuan Zhou', 'Haoxiang Tian', 'Chunrong Fang', 'Yunjian Sun', 'Weisong Sun']",,"Autonomous driving systems (ADSs) have undergone remarkable development and are increasingly employed in safety-critical applications. However, recently reported data on fatal accidents involving ADSs suggests that the desired level of safety has not yet been fully achieved. Consequently, there is a growing need for more comprehensive and targeted testing approaches to ensure safe driving. Scenarios from real-world accident reports provide valuable resources for ADS testing, including critical scenarios and high-quality seeds. However, existing scenario reconstruction methods from accident reports often exhibit limited accuracy in information extraction. Moreover, due to the diversity and complexity of road environments, matching current accident information with the simulation map data for reconstruction poses significant challenges.In this paper, we design and implement SoVAR, a tool for automatically generating road-generalizable scenarios from accident reports. SoVAR utilizes well-designed prompts with linguistic patterns to guide the large language model (LLM) in extracting accident information from textual data. Subsequently, it formulates and solves accident-related constraints in conjunction with the extracted accident information to generate accident trajectories. Finally, SoVAR reconstructs accident scenarios on various map structures and converts them into test scenarios to evaluate its capability to detect defects in industrial ADSs. We experiment with SoVAR, using the accident reports from the National Highway Traffic Safety Administration’s (NHTSA) database to generate test scenarios for the industrial-grade ADS Apollo. The experimental findings demonstrate that SoVAR can effectively generate generalized accident scenarios across different road structures. Furthermore, the results confirm that SoVAR identified 5 distinct safety violation types that contributed to the crash of Baidu Apollo.CCS Concepts• Software and its engineering → Software testing an...",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11007529/,Automated Testing for Service-Oriented Architecture: Leveraging Large Language Models for Enhanced Service Composition,"['Mahsun Altin', 'Behcet Mutlu', 'Deniz Kilinc', 'Altan Cakir']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11007529,"This article explores the application of Large Language Models (LLMs), including proprietary models such as OpenAI’s ChatGPT 4o and ChatGPT 4o-mini, Anthropic’s Claude 3.5 Sonnet and Claude 3.7 Sonnet, and Google’s Gemini 1.5 Pro, Gemini 2.0 Flash, and Gemini 2.0 Flash-Lite, as well as open-source alternatives including Qwen2.5-14B-Instruct-1M, and commercially accessed models such as DeepSeek R1 and DeepSeek V3, which were tested via APIs despite having open-source variants, to automate validation and verification in Application Programming Interface (API) testing within a Service-Oriented Architecture (SOA). Our system compares internal responses from the Enuygun Web Server against third-party API outputs in both JSON and XML formats, validating critical parameters such as flight prices, baggage allowances, and seat availability. We generated 100 diverse test scenarios across varying complexities (1-4 flight results) by randomly altering request and response parameters. Experimental results show that Google Gemini 2.0 Flash achieved high accuracy (up to 99.98%) with the lowest completion time (85.34 seconds), while Qwen2.5-14B-Instruct-1M exhibited limited capability in processing complex formats. Models such as OpenAI’s ChatGPT and Anthropic’s Claude Sonnet models also demonstrated strong performance in single-flight validation scenarios, making them suitable for low-latency, high-precision tasks. Our findings indicate that some open-source models can offer promising cost-effective alternatives, though performance significantly varies. This integration of LLMs reduced manual workload, improved test scalability, and enabled real-time validation across large-scale datasets. As LLM technologies mature, we anticipate further advances in automation, accuracy, and efficiency in software validation systems.",https://doi.org/10.1109/ACCESS.2025.3571994,
https://ieeexplore.ieee.org/document/10893343/,Enhancing User Story Generation in Agile Software Development Through Open AI and Prompt Engineering,"['Vijayalakshmi Ramasamy', 'Suganya Ramamoorthy', 'Gursimran Singh Walia', 'Eli Kulpinski', 'Aaron Antreassian']",,"This innovative practice full paper explores the use of AI technologies in user story generation. With the emergence of agile software development, generating comprehensive user stories that capture all necessary functionalities and perspectives has become crucial for software development. Every computing program in the United States requires a semester-or year-long senior capstone project, which requires student teams to gather and document technical requirements. Effective user story generation is crucial for successfully implementing software projects. However, user stories written in natural language can be prone to inherent defects such as incompleteness and incorrectness, which may creep in during the downstream development activities like software designs, construction, and testing. One of the challenges faced by software engineering educators is to teach students how to elicit and document requirements, which serve as a blueprint for software development. Advanced AI technologies have increased the popularity of large language models (LLMs) trained on large multimodal datasets. Therefore, utilizing LLM-based techniques can assist educators in helping students discover aspects of user stories that may have been overlooked or missed during the manual analysis of requirements from various stakeholders. The main goal of this research study is to investigate the potential application of OpenAI techniques in software development courses at two academic institutions to enhance software design and development processes, aiming to improve innovation and efficiency in team project-based educational settings. The data used for the study constitute student teams generating user stories by traditional methods (control) vs. student teams using OpenAI agents (treatment) such as gpt-4-turbo for generating user stories. The overarching research questions include: RQ-l) What aspects of user stories generated using OpenAI prompt engineering differ significantly from those gene...",https://doi.org/10.1109/FIE61694.2024.10893343,13-16 October 2024
https://ieeexplore.ieee.org/document/10366647/,"LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities","['Shengcheng Yu', 'Chunrong Fang', 'Yuchen Ling', 'Chentian Wu', 'Zhenyu Chen']",,"This paper investigates the application of large language models (LLM) in the domain of mobile application test script generation. Test script generation is a vital component of software testing, enabling efficient and reliable automation of repetitive test tasks. However, existing generation approaches often encounter limitations, such as difficulties in accurately capturing and reproducing test scripts across diverse devices, platforms, and applications. These challenges arise due to differences in screen sizes, input modalities, platform behaviors, API inconsistencies, and application architectures. Overcoming these limitations is crucial for achieving robust and comprehensive test automation.By leveraging the capabilities of LLMs, we aim to address these challenges and explore its potential as a versatile tool for test automation. We investigate how well LLMs can adapt to diverse devices and systems while accurately capturing and generating test scripts. Additionally, we evaluate its cross-platform generation capabilities by assessing its ability to handle operating system variations and platform-specific behaviors. Furthermore, we explore the application of LLMs in cross-app migration, where it generates test scripts across different applications and software environments based on existing scripts.Throughout the investigation, we analyze its adaptability to various user interfaces, app architectures, and interaction patterns, ensuring accurate script generation and compatibility. The findings of this research contribute to the understanding of LLMs’ capabilities in test automation. Ultimately, this research aims to enhance software testing practices, empowering app developers to achieve higher levels of software quality and development efficiency.",https://doi.org/10.1109/QRS60937.2023.00029,22-26 October 2023
https://ieeexplore.ieee.org/document/10989041/,Integrating LLM-Based Text Generation with Dynamic Context Retrieval for GUI Testing,"['Juyeon Yoon', 'Seah Kim', 'Somin Kim', 'Sukchul Jung', 'Shin Yoo']",,"Automated GUI testing plays a crucial role for smartphone vendors who have to ensure that the widely used mobile apps-that are not essentially developed by the vendors-are compatible with new devices and system updates. While existing testing techniques can automatically generate event sequences to reach different GUI views, inputs such as strings and numbers remain difficult to generate, as their generation often involves semantic understanding of the app functionality. Recently, Large Language Models (LLMs) have been successfully adopted to generate string inputs that are semantically relevant to the test case. This paper evaluates the LLM-based input generation in the industrial context of vendor testing of both in-house and 3rd party mobile apps. We present DROIDFILLER, an LLM based input generation technique that builds upon existing work with more sophisticated prompt engineering and customisable context retrieval. DROIDFILLER is empirically evaluated using a total of 120 textfields collected from a total of 45 apps, including both in-house and 3rd party ones. The results show that DROIDFILLER can outperform both vanilla LLM based input generation as well as the existing resource pool approach. We integrate DROIDFILLER into the existing GUI testing framework used at Samsung, evaluate its performance, and discuss the challenges and considerations for practical adoption of LLM-based input generation in the industry.",https://doi.org/10.1109/ICST62969.2025.10989041,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10844085/,LLM-Guided Crowdsourced Test Report Clustering,"['Ying Li', 'Ye Zhong', 'Lijuan Yang', 'Yanbo Wang', 'Penghua Zhu']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10844085,"This paper proposes a clustering method for crowdsourced test reports based on a large language model to solve the limitations of existing methods in processing repeated reports and utilizing multi-modal information. Existing crowdsourced test report clustering methods have significant shortcomings in handling duplicate reports, ignoring the semantic information of screenshots, and underutilizing the relationship between text and images. The emergence of LLM provides a new way to solve these problems. By integrating the semantic understanding ability of LLM, key information can be extracted from the test report more accurately, and the semantic relationship between screenshots and text descriptions can be used to guide the clustering process, thus improving the accuracy and effectiveness of clustering. The method in this paper uses a pre-trained LLM (such as GPT-4) to encode the text in the test report, and uses a visual model such as CLIP to encode the application screenshots, converting the text descriptions and images into high-dimensional semantic vectors. The cosine similarity is then used to calculate the similarity between the vectors, and semantic binding rules are constructed to guide the clustering process, ensuring that semantically related reports are assigned to the same cluster and semantically different reports are assigned to different clusters. Through experimental verification, this method is significantly superior to traditional methods in several evaluation indicators, demonstrating its great potential in improving the efficiency and quality of crowdsourced test report processing. In the future, this method is expected to be widely used in the process of software testing and maintenance, and further promote technological progress.",https://doi.org/10.1109/ACCESS.2025.3530960,
https://ieeexplore.ieee.org/document/10548767/,Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model,"['Zhe Liu', 'Chunyang Chen', 'Junjie Wang', 'Mengzhuo Chen', 'Boyu Wu', 'Zhilin Tian']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10548767,"Mobile applications have become a ubiquitous part of our daily life, providing users with access to various services and utilities. Text input, as an important interaction channel between users and applications, plays an important role in core functionality such as search queries, authentication, messaging, etc. However, certain special text (e.g., -18 for Font Size) can cause the app to crash, and generating diversified unusual inputs for fully testing the app is highly demanded. Nevertheless, this is also challenging due to the combination of explosion dilemma, high context sensitivity, and complex constraint relations. This paper proposes InputBlaster which leverages the LLM to automatically generate unusual text inputs for mobile app crash detection. It formulates the unusual inputs generation problem as a task of producing a set of test generators, each of which can yield a batch of unusual text inputs under the same mutation rule. In detail, InputBlaster leverages LLM to produce the test generators together with the mutation rules serving as the reasoning chain, and utilizes the in-context learning schema to demonstrate the LLM with examples for boosting the performance. InputBlaster is evaluated on 36 text input widgets with cash bugs involving 31 popular Android apps, and results show that it achieves 78% bug detection rate, with 136% higher than the best baseline. Besides, we integrate it with the automated GUI testing tool and detect 37 unseen crashes in real-world apps.",https://doi.org/10.1145/3597503.3639118,14-20 April 2024
https://ieeexplore.ieee.org/document/10638556/,Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing,"['Robson Santos', 'Italo Santos', 'Cleyton Magalhaes', 'Ronnie de Souza Santos']",,"A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates content, including grammatical sentences, human-like paragraphs, and syntactically code snippets. LLMs can play a pivotal role in soft-ware development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts-specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.",https://doi.org/10.1109/ICST60714.2024.00039,27-31 May 2024
https://ieeexplore.ieee.org/document/10989033/,"Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation","['Azat Abdullin', 'Pouria Derakhshanfar', 'Annibale Panichella']",,"Generating tests automatically is a key and ongoing area of focus in software engineering research. The emergence of Large Language Models (LLMs) has opened up new op-portunities, given their ability to perform a wide spectrum of tasks. However, the effectiveness of LLM -based approaches compared to traditional techniques such as search-based software testing (SBST) and symbolic execution remains uncertain. In this paper, we perform an extensive study of automatic test generation approaches based on three tools: EvoSuite for SBST, Kex for symbolic execution, and TestSpark for LLM-based test generation. We evaluate tools' performance on the GitBug Java dataset and compare them using various execution-based and feature-based metrics. Our results show that while LLM-based test generation is promising, it falls behind traditional methods w.r.t. coverage. However, it significantly outperforms them in mutation scores, suggesting that LLMs provide a deeper semantic understanding of code. LLM-based approach performed worse than SBST and symbolic execution-based approaches w.r.t. fault detection capabilities. Additionally, our feature-based analysis shows that all tools are affected by the complexity and internal dependencies of the class under test (CUT), with LLM-based approaches being especially sensitive to the CUT size.",https://doi.org/10.1109/ICST62969.2025.10989033,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/11042526/,Static Analysis and LLM for Comprehensive Java Unit Test Generation,['Wei Wei'],,"Software testing is crucial in ensuring the reliability and correctness of software applications. However, generating comprehensive test cases manually can be time-consuming and error-prone. This paper introduces SAGEN, a tool designed to automate Java unit test generation by leveraging static analysis of Syntax Trees (AST) and large language models (LLMs). SAGEN identifies literal values and their ranges, generating test cases that improve coverage and quality. In our experiments, SAGEN outperforms traditional test case generation tools such as EvoSuite and Randoop. It demonstrates a 10%
 improvement in code coverage and a 13%
 enhancement in test case quality. Furthermore, SAGEN achieves a compile pass rate of 89.7%
, proving its effectiveness in producing both high-quality and reliable test cases.",https://doi.org/10.1109/AEMCSE65292.2025.11042526,09-11 May 2025
https://ieeexplore.ieee.org/document/10440574/,"Software Testing With Large Language Models: Survey, Landscape, and Vision","['Junjie Wang', 'Yuchao Huang', 'Chunyang Chen', 'Zhe Liu', 'Song Wang', 'Qing Wang']",,"Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.",https://doi.org/10.1109/TSE.2024.3368208,
https://ieeexplore.ieee.org/document/10765048/,Using LLM for Mining and Testing Constraints in API Testing,"['Minh-Hieu Huynh', 'Quoc-Tri Le', 'Tien N. Nguyen', 'Vu Nguyen']",,CCS Concepts• Software and its engineering → Software testing and debugging,,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11029812/,NIODebugger: A Novel Approach to Repair Non-Idempotent-Outcome Tests with LLM-Based Agent,['Kaiyao Ke'],,"Flaky tests, characterized by inconsistent results across repeated executions, present significant challenges in software testing, especially during regression testing. Recently, there has been emerging research interest in non-idempotentoutcome (NIO) flaky tests-tests that pass on the initial run but fail on subsequent executions within the same environment. Despite progress in utilizing Large Language Models (LLMs) to address flaky tests, existing methods have not tackled NIO flaky tests. The limited context window of LLMs restricts their ability to incorporate relevant source code beyond the test method itself, often overlooking crucial information needed to address state pollution, which is the root cause of NIO flakiness. This paper introduces NIODebugger, the first framework to utilize an LLM-based agent to repair flaky tests. NIODebugger features a three-phase design: detection, exploration, and fixing. In the detection phase, dynamic analysis collects stack traces and custom test execution logs from multiple test runs, which helps in understanding accumulative state pollution. During the exploration phase, the LLM-based agent provides instructions for extracting relevant source code associated with test flakiness. In the fixing phase, NIODebugger repairs the tests using the information gathered from the previous phases. NIODebugger can be integrated with multiple LLMs, achieving patching success rates ranging from 11.63% to 58.72%. Its best-performing variant, NIODebugger-GPT-4, successfully generated correct patches for 101 out of 172 previously unknown NIO tests across 20 largescale open-source projects. We submitted pull requests for all generated patches; 58 have been merged, only 1 was rejected, and the remaining 42 are pending. The Java implementation of NIODebugger is provided as a Maven plugin accessible at https://github.com/kaiyaok2/NIOInspector.",https://doi.org/10.1109/ICSE55347.2025.00226,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10429345/,Evaluating LLM’s Code Reading Abilities in Big Data Contexts using Metamorphic Testing,"['Ziyu Li', 'Zhendu Li', 'Kaiming Xiao', 'Xuan Li']",,"With the explosive growth of Big Data, understanding complex data-centering algorithms and software has become essential. Large Language Models (LLMs) especially ChatGPT models have been increasingly deployed in Big Data environments to improve workflow in various tasks, including code reading. The current testing method on LLMs’ code reading ability focuses more on code structural understanding and sentiment understanding, all tests are conducted on different prompts with different assumptions. This paper analyzes current LLM code-reading testing methods and presents an innovative evaluation of Metamorphic Testing to evaluate LLMs’ code-reading abilities. We proposed two new metamorphic relations specified f or code reading challenges and evaluated the ChatGPT-3.5 on its code understanding capabilities. Our study offers insights into LLMs’ capabilities to correctly understand diverse code bases and maintain validity.",https://doi.org/10.1109/BigDIA60676.2023.10429345,15-17 December 2023
https://ieeexplore.ieee.org/document/10962507/,LLM Prompt Engineering for Automated White-Box Integration Test Generation in REST APIs,"['André Mesquita Rincon', 'Auri Marcelo Rizzo Vincenzi', 'João Pascoal Faria']",,"This study explores prompt engineering for automated white-box integration testing of RESTful APIs using Large Language Models (LLMs). Four versions of prompts were designed and tested across three OpenAI models (GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o) to assess their impact on code coverage, token consumption, execution time, and financial cost. The results indicate that different prompt versions, especially with more advanced models, achieved up to 90% coverage, although at higher costs. Additionally, combining test sets from different models increased coverage, reaching 96% in some cases. We also compared the results with EvoMaster, a specialized tool for generating tests for REST APIs, where LLM-generated tests achieved comparable or higher coverage in the benchmark projects. Despite higher execution costs, LLMs demonstrated superior adaptability and flexibility in test generation.",https://doi.org/10.1109/ICSTW64639.2025.10962507,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10988971/,Evaluating Correct-Consistency and Robustness in Code-Generating LLMs,['Shahin Honarvar'],,"Ensuring the reliability of large language models (LLMs) for code generation is crucial for their safe integration into software engineering practices, especially in safety-critical domains. Despite advances in LLM tuning, they frequently generate incorrect code, raising concerns about robustness and trustworthiness. This PhD research introduces Turbulence, a novel benchmark and evaluation framework designed to assess both the correct-consistency and robustness of LLMs through a structured coding question neighbourhood approach. By evaluating model performance across sets of semantically related but non-equivalent coding tasks, Turbulence identifies discontinuities in LLM generalisation, revealing patterns of success and failure that standard correctness evaluations often overlook. Applied to 22 instruction-tuned LLMs across Python coding question neighbourhoods, the benchmark highlights significant variability in correctness, including error patterns persisting even under deterministic settings. Future work will extend the question neighbourhood concept to Capture The Flag (CTF) challenges, enabling a deeper analysis of model reasoning capabilities in progressively complex tasks. This extension has attracted interest from the UK AI Safety Institute, which recognises the frame-work's potential for advancing rigorous evaluation methodologies in the context of safe and trusted AI for software engineering.",https://doi.org/10.1109/ICST62969.2025.10988971,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10989005/,Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code,"['Shahin Honarvar', 'Mark van der Wilk', 'Alastair F. Donaldson']",,"We present a method for systematically evaluating the correctness and robustness of instruction-tuned large language models (LLMs) for code generation via a new benchmark, Turbulence. Turbulence consists of a large set of natural language question templates, each of which is a programming problem, parameterised so that it can be asked in many different forms. Each question template has an associated test oracle that judges whether a code solution returned by an LLM is correct. Thus, from a single question template, it is possible to ask an LLM a neighbourhood of very similar programming questions, and assess the correctness of the result returned for each question. This allows gaps in an LLM's code generation abilities to be identified, including anomalies where the LLM correctly solves almost all questions in a neighbourhood but fails for particular parameter instantiations. We present experiments against five LLMs from OpenAI, Cohere and Meta, each at two temperature configurations. Our findings show that, across the board, Turbulence is able to reveal gaps in LLM reasoning ability. This goes beyond merely highlighting that LLMs sometimes produce wrong code (which is no surprise): by systematically identifying cases where LLMs are able to solve some problems in a neighbourhood but do not manage to generalise to solve the whole neighbourhood, our method is effective at highlighting robustness issues. We present data and examples that shed light on the kinds of mistakes that LLMs make when they return incorrect code results.",https://doi.org/10.1109/ICST62969.2025.10989005,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10675844/,“No Free Lunch” when using Large Language Models to Verify Self-Generated Programs,"['Sol Zilberman', 'H. C. Betty Cheng']",,"Large Language Models (LLMs) have shown great success in a wide range of text-generation tasks including the synthesis of code from natural language descriptions. As LLMbased techniques continue to grow in popularity, especially amongst entry-level developers, LLM-generated code has the potential to be deployed in a diverse set of application domains. While LLMs can generate syntactically correct code output, recent work has shown the presence of nonsensical and faulty reasoning in LLM-generated text. As such, overreliance on LLMs for software generation may potentially result in the deployment of faulty software leading to critical system failures. This study explores the capabilities of a single LLM to generate both software and corresponding test suites from the same initial program descriptions, which can be considered analogous to an individual developer coding and unit testing for a given piece of software. We present an empirical framework and evaluation methodology to assess the usefulness of LLM-generated test cases for verifying programs generated by the same LLM. Our findings indicate that LLMs frequently generate irrelevant tests that suffer from numerous quality concerns.",https://doi.org/10.1109/ICSTW60967.2024.00018,27-31 May 2024
https://ieeexplore.ieee.org/document/10967317/,Optimizing LLMs for Code Generation: Which Hyperparameter Settings Yield the Best Results?,"['Chetan Arora', 'Ahnaf Ibn Sayeed', 'Sherlock Licorish', 'Fanyu Wang', 'Christoph Treude']",,"Large Language Models (LLMs), such as GPT models, are increasingly used in software engineering for various tasks, such as code generation, requirements management, and debugging. While automating these tasks has garnered significant attention, a systematic study on the impact of varying hyperparameters on code generation outcomes remains unexplored. This study aims to assess LLMs' code generation performance by exhaustively exploring the impact of various hyperparameters. Hyperparameters for LLMs are adjustable settings that affect the model's behaviour and performance. Specifically, we investigated how changes to the hyperparameters-temperature, top probability (top_p), frequency penalty, and presence penalty-affect code generation outcomes. We systematically adjusted all hyperparameters together, exploring every possible combination by making small increments to each hyperparameter at a time. This exhaustive approach was applied to 13 Python code generation tasks, yielding one of four outcomes for each hyperparameter combination: no output from the LLM, non-executable code, code that fails unit tests, or correct and functional code. We analysed these outcomes for a total of 14,742 generated Python code segments, focusing on correctness, to determine how the hyperparameters influence the LLM to arrive at each outcome. Using correlation coefficient and regression tree analyses, we ascertained which hyperparameters influence which aspect of the LLM. Our results indicate that optimal performance is achieved with a temperature below 0.5, top probability below 0.75, frequency penalty above -1 and below 1.5, and presence penalty above -1. We make our dataset and results available to facilitate replication.",https://doi.org/10.1109/APSEC65559.2024.00039,03-06 December 2024
https://ieeexplore.ieee.org/document/10989036/,Impact of Large Language Models of Code on Fault Localization,"['Suhwan Ji', 'Sanghwa Lee', 'Changsup Lee', 'Yo-Sub Han', 'Hyeonseung Im']",,"Identifying the point of error is imperative in software debugging. Traditional fault localization (FL) techniques rely on executing the program and using the code coverage matrix in tandem with test case results to calculate a suspiciousness score for each method or line. Recently, learning-based FL techniques have harnessed machine learning models to extract meaningful features from the code coverage matrix and improve FL performance. These techniques, however, require compilable source code, existing test cases, and specialized tools for generating the code coverage matrix for each programming language of interest. In this paper, we propose, for the first time, a simple but effective sequence generation approach for fine-tuning large language models of code (LLMCs) for FL tasks. LLMCs have recently received much attention for various software engineering problems. In line with these, we leverage the innate understanding of code that LLMCs have acquired through pre-training on large code corpora. Specifically, we fine-tune 13 representative encoder, encoder-decoder, and decoder-based LLMCs (across 7 different architectures) for FL tasks. Unlike previous approaches, LLM Cs can analyze code sequences that do not compile. Still, they have a limitation on the length of the input data. Therefore, for a fair comparison with existing FL techniques, we extract methods with errors from the project-level benchmark, Defects4J, and analyze them at the line level. Experimental results show that LLMCs fine-tuned with our approach successfully pinpoint error positions in 50.6%, 64.2%, and 72.3% of 1,291 methods in Defects4J for Top-1/3/5 prediction, outperforming the best learning-based state-of-the-art technique by up to 1.35, 1.12, and 1.08 times, respectively. We also conduct an in-depth investigation of key factors that may affect the FL performance of LLMCs. Our findings suggest promising research directions for FL and automated program repair tasks using LLMCs.",https://doi.org/10.1109/ICST62969.2025.10989036,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10669832/,Assured LLM-Based Software Engineering,"['Nadia Alshahwan', 'Mark Harman', 'Inna Harper', 'Alexandru Marginean', 'Shubho Sengupta', 'Eddy Wang']",,"In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code(1)does not regress the properties of the original code ?(2)improves the original in a verifiable and measurable way ?To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of LLM’s propensity to hallucinate. It allows us to generate code using LLMs, independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.",,15-15 April 2024
https://ieeexplore.ieee.org/document/10988926/,AugmenTest: Enhancing Tests with LLM-Driven Oracles,"['Shaker Mahmud Khandaker', 'Fitsum Kifetew', 'Davide Prandi', 'Angelo Susi']",,"Automated test generation is crucial for ensuring the reliability and robustness of software applications while at the same time reducing the effort needed. While significant progress has been made in test generation research, generating valid test oracles still remains an open problem. To address this challenge, we present AugmenTest, an approach leveraging Large Language Models (LLMs) to infer correct test oracles based on available documentation of the software under test. Unlike most existing methods that rely on code, AugmenTest utilizes the semantic capabilities of LLMs to infer the intended behavior of a method from documentation and developer comments, without looking at the code. AugmenTest includes four variants: Simple Prompt, Extended Prompt, RAG with a generic prompt (without the context of class or method under test), and RAG with Simple Prompt, each offering different levels of contextual information to the LLMs. To evaluate our work, we selected 142 Java classes and generated multiple mutants for each. We then generated tests from these mutants, focusing only on tests that passed on the mutant but failed on the original class, to ensure that the tests effectively captured bugs. This resulted in 203 unique tests with distinct bugs, which were then used to evaluate AugmenTest. Results show that in the most conservative scenario, AugmenTest's Extended Prompt consistently outperformed the Simple Prompt, achieving a success rate of 30% for generating correct assertions. In comparison, the state-of-the-art TOGA approach achieved 8.2%. Contrary to our expectations, the RAG-based approaches did not lead to improvements, with performance of 18.2% success rate for the most conservative scenario. Our study demonstrates the potential of LLMs in improving the reliability of automated test generation tools, while also highlighting areas for future enhancement.",https://doi.org/10.1109/ICST62969.2025.10988926,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10988995/,An Analysis of LLM Fine-Tuning and Few-Shot Learning for Flaky Test Detection and Classification,"['Riddhi More', 'Jeremy S. Bradbury']",,"Flaky tests exhibit non-deterministic behavior during execution and they may pass or fail without any changes to the program under test. Detecting and classifying these flaky tests is crucial for maintaining the robustness of automated test suites and ensuring the overall reliability and confidence in the testing. However, flaky test detection and classification is challenging due to the variability in test behavior, which can depend on environmental conditions and subtle code interactions. Large Language Models (LLMs) offer promising approaches to address this challenge, with fine-tuning and few-shot learning (FSL) emerging as viable techniques. With enough data fine-tuning a pre-trained LLM can achieve high accuracy, making it suitable for organizations with more resources. Alternatively, we introduce FlakyXbert, an FSL approach that employs a Siamese network architecture to train efficiently with limited data. To understand the performance and cost differences between these two methods, we compare fine-tuning on larger datasets with FSL in scenarios restricted by smaller datasets. Our evaluation involves two existing flaky test datasets, FlakyCat and IDoFT. Our results suggest that while fine-tuning can achieve high accuracy, FSL provides a cost-effective approach with competitive accuracy, which is especially beneficial for organizations or projects with limited historical data available for training. These findings underscore the viability of both fine-tuning and FSL in flaky test detection and classification with each suited to different organizational needs and resource availability.",https://doi.org/10.1109/ICST62969.2025.10988995,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10989020/,Improving the Readability of Automatically Generated Tests Using Large Language Models,"['Matteo Biagiola', 'Gianluca Ghislotti', 'Paolo Tonella']",,"Search-based test generators are effective at producing unit tests with high coverage. However, such automatically generated tests have no meaningful test and variable names, making them hard to understand and interpret by developers. On the other hand, large language models (LLMs) can generate highly readable test cases, but they are not able to match the effectiveness of search-based generators, in terms of achieved code coverage. In this paper, we propose to combine the effectiveness of search-based generators with the readability of LLM generated tests. Our approach focuses on improving test and variable names produced by search-based tools, while keeping their semantics (i.e., their coverage) unchanged. Our evaluation on nine industrial and open source LLMs show that our readability improvement transformations are overall semantically-preserving and stable across multiple repetitions. Moreover, a human study with ten professional developers, show that our LLM-improved tests are as readable as developer-written tests, regardless of the LLM employed.",https://doi.org/10.1109/ICST62969.2025.10989020,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10704582/,FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair,"['Sakina Fatima', 'Hadi Hemmati', 'Lionel C. Briand']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10704582,"Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting development effort. While machine learning models have been used to predict flakiness and its root causes, there is much less work on providing support to fix the problem. To address this gap, in this paper, we focus on predicting the type of fix that is required to remove flakiness and then repair the test code on that basis. We do this for a subset of flaky tests where the root cause of flakiness is in the test itself and not in the production code. One key idea is to guide the repair process with additional knowledge about the test's flakiness in the form of its predicted fix category. Thus, we first propose a framework that automatically generates labeled datasets for 13 fix categories and trains models to predict the fix category of a flaky test by analyzing the test code only. Our experimental results using code models and few-shot learning show that we can correctly predict most of the fix categories. To show the usefulness of such fix category labels for automatically repairing flakiness, we augment the prompts of GPT 3.5 Turbo, a Large Language Model (LLM), with such extra knowledge to request repair suggestions. The results show that our suggested fix category labels, complemented with in-context learning, significantly enhance the capability of GPT 3.5 Turbo in generating fixes for flaky tests. Based on the execution and analysis of a sample of GPT-repaired flaky tests, we estimate that a large percentage of such repairs, (roughly between 51% and 83%) can be expected to pass. For the failing repaired tests, on average, 16% of the test code needs to be further changed for them to pass.",https://doi.org/10.1109/TSE.2024.3472476,
https://ieeexplore.ieee.org/document/10962503/,From Implemented to Expected Behaviors: Leveraging Regression Oracles for Non-regression Fault Detection using LLMs,"['Stefano Ruberto', 'Judith Perera', 'Gunel Jahangirova', 'Valerio Terragni']",,"Automated test generation tools often produce assertions that reflect implemented behavior, limiting their usage to regression testing. In this paper, we propose LLMProphet, a black-box approach that applies Few-Shot Learning with LLMs, using automatically generated regression tests as context to identify non-regression faults without relying on source code. By employing iterative cross-validation and a leave-one-out strategy, LLMProphet identifies regression assertions that are misaligned with expected behaviors. We outline LLMProphet’s workflow, feasibility, and preliminary findings, demonstrating its potential for LLM-driven fault detection.",https://doi.org/10.1109/ICSTW64639.2025.10962503,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/11029738/,Rug: Turbo Llm for Rust Unit Test Generation,"['Xiang Cheng', 'Fan Sang', 'Yizhuo Zhai', 'Xiaokuan Zhang', 'Taesoo Kim']",,"Unit testing improves software quality by evaluating isolated sections of the program. This approach alleviates the need for comprehensive program-wide testing and confines the potential error scope within the software. However, unit test development is time-consuming, requiring developers to create appropriate test contexts and determine input values to cover different code regions. This problem is particularly pronounced in Rust due to its intricate type system, making traditional unit test generation tools ineffective in Rust projects. Recently, large language models (LLMs) have demonstrated their proficiency in understanding programming language and completing software engineering tasks. However, merely prompting LLMs with a basic prompt like “generate unit test for the following source code” often results in code with compilation errors. In addition, LLM-generated unit tests often have limited test coverage. To bridge this gap and harness the capabilities of LLM, we design and implement RUG, an end-to-end solution to automatically generate the unit test for Rust projects. To help LLM's generated test pass Rust strict compilation checks, RUG designs a semantic-aware bottom-up approach to divide the context construction problem into dependent sub-problems. It solves these sub-problems sequentially using an LLM and merges them to a complete context. To increase test coverage, RUG integrates coverage-guided fuzzing with LLM to prepare fuzzing harnesses. Applying RUG on 17 real-world Rust programs (average 24,937LoC
), we show that RUG can achieve a high code coverage, up to 71.37%
, closely comparable to human effort (73.18%)
. We submitted 113 unit tests generated by RUG covering the new code: 53 of them have been accepted, 17 rejected, and 43 are pending for review.",https://doi.org/10.1109/ICSE55347.2025.00097,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10765025/,Automated Validation of COBOL to Java Transformation,"['Atul Kumar', 'Diptikalyan Saha', 'Toshikai Yasue', 'Kohichi Ono', 'Saravanan Krishnan', 'Sandeep Hans']",,"Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterprise-level code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs. Demo Video: https://youtu.be/aqF_agNP-lUCCS CONCEPTS• Software and its engineering → Software testing and debugging.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10556138/,Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs,"['Ziyu Li', 'Donghwan Shin']",,"Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development ...",,14-15 April 2024
https://ieeexplore.ieee.org/document/10893407/,WIP: ARTful Insights from a Pilot Study on GPT-Based Automatic Code Reviews in Undergraduate Computer Science Programs,"['Aaron S. Crandall', 'Bryan J. Fischer', 'Johannah L. Crandall']",,"This work in progress research paper describes a pilot study using a Large Language Model (LLM) Generative Pre-Trained Transformer-based (GPT) system that generates industry-style code reviews for student feedback on software development projects in Computer Science 2nd, 3rd, and 4th+ semester classes (CS2, CS3, CS4+) at an ABET accredited baccalaureate institution. Code reviews are a valuable, but work-intensive, component of the software engineering process and provide important training to undergraduate students in the form of mentor-peer knowledge transfer. Participants in this study engaged in iterative experiential learning using the Automatic Review Tool (ART), an artificial intelligence tool to support software engineering as an Automatic Static Analysis Tool in the Continuous Integration pipeline alongside software testing harnesses and code style checkers. This pilot study was based on earlier results from a full computer science second semester (CS2) class (n=74)
 to develop an ART-generated code review intervention pilot study with a small group of students in CS2 / 3 and CS4. The project underway uses an experiential learning and iterative feedback process to answer research questions including “Does ART provide accurate and actionable code reviews for students” and “Which levels of students are best prepared to receive and use ART-based code reviews?” During this pilot study, the project used a mixed methods research approach with a series of surveys, code review interventions, and numerical analysis of the code reviews' accuracy. Results showed a reasonable degree of code review accuracy by ART and the students learned code review skills from interaction with the ART-based reviews they received. Ongoing work includes increasing the scale of data collection, using this work to refine and focus the ART-based reviews onto the categories of feedback that students find the most valuable, and building out a more modular tool for wider release in t...",https://doi.org/10.1109/FIE61694.2024.10893407,13-16 October 2024
https://ieeexplore.ieee.org/document/10669636/,Can ChatGPT Repair Non-Order-Dependent Flaky Tests?,"['Yang Chen', 'Reyhaneh Jabbarvand']",,"Regression testing helps developers check whether the latest code changes break software functionality. Flaky tests, which can non-deterministically pass or fail on the same code version, may mislead developers’ concerns, resulting in missing some bugs or spending time pinpointing bugs that do not exist. Existing flakiness detection and mitigation techniques have primarily focused on general order-dependent (OD) and implementation-dependent (ID) flaky tests. There is also a dearth of research on repairing test flakiness, out of which, mostly have focused on repairing OD flaky tests, and a few have explored repairing a subcategory of non-order-dependent (NOD) flaky tests that are caused by asynchronous waits. As a result, there is a demand for devising techniques to reproduce, detect, and repair NOD flaky tests. Large language models (LLMs) have shown great effectiveness in several programming tasks. To explore the potential of LLMs in addressing NOD flakiness, this paper investigates the possibility of using ChatGPT to repair different categories of NOD flaky tests. Our comprehensive study on 118 from the IDoFT dataset shows that ChatGPT, despite as a leading LLM with notable success in multiple code generation tasks, is ineffective in repairing NOD test flakiness, even by following the best practices for prompt crafting. We investigated the reasons behind the failure of using ChatGPT in repairing NOD tests, which provided us valuable insights about the next step to advance the field of NOD test flakiness repair.CCS CONCEPTS• Software and its engineering → Software testing and debugging.",,14-14 April 2024
https://ieeexplore.ieee.org/document/10765010/,HITS: High-coverage LLM-based Unit Test Generation via Method Slicing,"['Zejun Wang', 'Kaibo Liu', 'Ge Li', 'Zhi Jin']",,"Large language models (LLMs) have behaved well in generating unit tests for Java projects. However, the performance for covering the complex focal methods within the projects is poor. Complex methods comprise many conditions and loops, requiring the test cases to be various enough to cover all lines and branches. However, existing test generation methods with LLMs provide the whole method-to-test to the LLM without assistance on input analysis. The LLM has difficulty inferring the test inputs to cover all conditions, resulting in missing lines and branches. To tackle the problem, we propose decomposing the focal methods into slices and asking the LLM to generate test cases slice by slice. Our method simplifies the analysis scope, making it easier for the LLM to cover more lines and branches in each slice. We build a dataset comprising complex focal methods collected from the projects used by existing state-of-the-art approaches. Our experiment results show that our method significantly outperforms current test case generation methods with LLMs and the typical SBST method Evosuite regarding both line and branch coverage scores.CCS CONCEPTS• Software and its engineering → Software testing and debugging; • Computing methodologies → Natural language processing.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10764814/,MR-Adopt: Automatic Deduction of Input Transformation Function for Metamorphic Testing,"['Congying Xu', 'Songqiang Chen', 'Jiarong Wu', 'Shing-Chi Cheung', 'Valerio Terragni', 'Hengcheng Zhu']",,"While a recent study reveals that many developer-written test cases can encode a reusable Metamorphic Relation (MR), over 70% of them directly hard-code the source input and follow-up input in the encoded relation. Such encoded MRs, which do not contain an explicit input transformation to transform the source inputs to corresponding follow-up inputs, cannot be reused with new source inputs to enhance test adequacy.In this paper, we propose MR-Adopt (Automatic Deduction Of inPut Transformation) to automatically deduce the input transformation from the hard-coded source and follow-up inputs, aiming to enable the encoded MRs to be reused with new source inputs. With typically only one pair of source and follow-up inputs available in an MR-encoded test case as the example, we leveraged LLMs to understand the intention of the test case and generate additional examples of source-followup input pairs. This helps to guide the generation of input transformations generalizable to multiple source inputs. Besides, to mitigate the issue that LLMs generate erroneous code, we refine LLM-generated transformations by removing MR-irrelevant code elements with data-flow analysis. Finally, we assess candidate transformations based on encoded output relations and select the best transformation as the result. Evaluation results show that MR-Adopt can generate input transformations applicable to all experimental source inputs for 72.00% of encoded MRs, which is 33.33% more than using vanilla GPT-3.5. By incorporating MR-Adopt-generated input transformations, encoded MR-based test cases can effectively enhance the test adequacy, increasing the line coverage and mutation score by 10.62% and 18.91%, respectively.CCS CONCEPTS• Software and its engineering → Software testing and debugging.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11026897/,Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation,"['Benjamin Steenhoek', 'Michele Tufano', 'Neel Sundaresan', 'Alexey Svyatkovskiy']",,"Software testing is a crucial but time-consuming aspect of software development, and recently, Large Language Models (LLMs) have gained popularity for automated test case generation. However, because LLMs are trained on vast amounts of open-source code, they often generate test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose Reinforcement Learning from Static Quality Metrics (RLSQM), wherein we utilize Reinforcement Learning to generate high-quality unit tests based on static analysis-based quality metrics. First, we analyzed LLM-generated tests and show that LLMs frequently do generate undesirable test smells — up to 37% of the time. Then, we implemented lightweight static analysis-based reward model and trained LLMs using this reward model to optimize for five code quality metrics. Our experimental results demonstrate that the RL-optimized Codex model consistently generated higher-quality test cases than the base LLM, improving quality metrics by up to 23%, and generated nearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all code quality metrics, in spite of training a substantially cheaper Codex model. We provide insights into how reliably utilize RL to improve test generation quality and show that RLSQM is a significant step towards enhancing the overall efficiency and reliability of automated software testing. Our data are available at this link: https://doi.org/10.6084/m9.figshare.25983166.",https://doi.org/10.1109/DeepTest66595.2025.00011,03-03 May 2025
https://ieeexplore.ieee.org/document/10837857/,Understanding Defects in Generated Codes by Language Models,"['Ali Mohammadi Esfahani', 'Nafiseh Kahani', 'Samuel A. Ajila']",,"This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the accuracy and functionality of the output remains a significant challenge. By using a structured defect classification method to understand their nature and origins this study categorizes and analyzes 367 identified defects from code snippets generated by LLMs, with a significant proportion being functionality and algorithm errors. These error categories indicate key areas where LLMs frequently fail, underscoring the need for targeted improvements. To enhance the accuracy of code generation, this paper implemented five prompt engineering techniques, including Scratchpad Prompting, Program of Thoughts Prompting, Chain-of-Thought Prompting, Chain of Code Prompting, and Structured Chain-of-Thought Prompting. These techniques were applied to refine the input prompts, aiming to reduce ambiguities and improve the models' accuracy rate. The research findings suggest that precise and structured prompting significantly miti-gates common defects, thereby increasing the reliability of LLM-aenerated code.",https://doi.org/10.1109/CASCON62161.2024.10837857,11-13 November 2024
https://ieeexplore.ieee.org/document/11029822/,LLM Based Input Space Partitioning Testing for Library APIs,"['Jiageng Li', 'Zhen Dong', 'Chong Wang', 'Haozhen You', 'Cen Zhang', 'Yang Liu']",,"Automated library APIs testing is difficult as it requires exploring a vast space of parameter inputs that may involve objects with complex data types. Existing search based approaches, with limited knowledge of relations between object states and program branches, often suffer from the low efficiency issue, i.e., tending to generate invalid inputs. Symbolic execution based approaches can effectively identify such relations, but fail to scale to large programs. In this work, we present an LLM-based input space partitioning testing approach, LISP, for library APIs. The approach lever-ages LLMs to understand the code of a library API under test and perform input space partitioning based on its understanding and rich common knowledge. Specifically, we provide the signature and code of the API under test to LLMs, with the expectation of obtaining a text description of each input space partition of the API under test. Then, we generate inputs through employing the generated text description to sample inputs from each partition, ultimately resulting in test suites that systematically explore the program behavior of the API. We evaluate LISP on more than 2,205 library API meth-ods taken from 10 popular open-source Java libraries (e.g., a
pa
che/commons-lang with 2.6k stars, guava with 48.8k stars on GitHub). Our experiment results show that LISP is effective in library API testing. It significantly outperforms state-of-the-art tool EvoSuite in terms of edge coverage. On average, LISP achieves 67.82 % branch coverage, surpassing EvoSuite by 1.21 times. In total, LISP triggers 404 exceptions or errors in the experiments, and discovers 13 previously unknown vulnerabilities during evaluation, which have been assigned CVE IDs.",https://doi.org/10.1109/ICSE55347.2025.00153,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10569684/,Guidelines for Effective Use of ChatGPT in Introductory Programming Education,"['Tlou RAMABU', 'Tumelo MALEBANE']",,"In the wake of highly capable Large Language Models (LLM) like ChatGPT, educational institutions have been navigating how to position themselves within this Artificial Intelligence (AI) era. There have been various suggestions and attempts to exclude ChatGPT in the education sector due to its AI abilities to give accurate responses to students, plagiarism and over reliance on the tool. However, there are also attempts to formally incorporate ChatGPT in education, such as in the field of economics, computer sciences or Mathematics. Without proper guidelines on the uses of ChatGPT in education, these AI technologies can be disruptive, uncontrollable and pose a risk to academic integrity. Based on the synthesis of ideas in the literature and ChatGPT experimental tests, this paper presents relevant guidelines for effective use of ChatGPT in the introductory programming education.",https://doi.org/10.23919/IST-Africa63983.2024.10569684,20-24 May 2024
https://ieeexplore.ieee.org/document/10962485/,Mutation Testing via Iterative Large Language Model-Driven Scientific Debugging,"['Philipp Straubinger', 'Marvin Kreis', 'Stephan Lukasczyk', 'Gordon Fraser']",,"Large Language Models (LLMs) can generate plausible test code. Intuitively they generate this by imitating tests seen in their training data, rather than reasoning about execution semantics. However, such reasoning is important when applying mutation testing, where individual tests need to demonstrate differences in program behavior between a program and specific artificial defects (mutants). In this paper, we evaluate whether Scientific Debugging, which has been shown to help LLMs when debugging, can also help them to generate tests for mutants. In the resulting approach, LLMs form hypotheses about how to kill specific mutants, and then iteratively generate and refine tests until they succeed, all with detailed explanations for each step. We compare this method to three baselines: (1) directly asking the LLM to generate tests, (2) repeatedly querying the LLM when tests fail, and (3) search-based test generation with Pynguin. Our experiments evaluate these methods based on several factors, including mutation score, code coverage, success rate, and the ability to identify equivalent mutants. The results demonstrate that LLMs, although requiring higher computational cost, consistently outperform Pynguin in generating tests with better fault detection and coverage. Importantly, we observe that the iterative refinement of test cases is important for achieving high-quality test suites.",https://doi.org/10.1109/ICSTW64639.2025.10962485,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10942512/,Challenges to Using Large Language Models in Code Generation and Repair,"['Liliana Pasquale', 'Antonino Sabetta', 'Marcelo d’Amorim', 'Péter Hegedűs', 'Mehdi Tarrit Mirakhorli', 'Hamed Okhravi']",,"Large language models (LLMs) hold great promise in solving many challenges arising from software complexity, including the possibility of automating code generation and repair. Although we cannot deny the groundbreaking nature of LLM-based code repair, we must be realistic in positioning current results.",https://doi.org/10.1109/MSEC.2025.3530488,
https://ieeexplore.ieee.org/document/10765018/,Enhancing Software Design and Developer Experience Via LLMs,['Simin Sun'],,"This research explores the transformative potential of generative AI in software development. Generative AI is revolutionizing the field by offering capabilities to automatically generate, refactor, and test code. Through the use of action research, new methods and tools based on generative AI models are studied and developed. The initial focus is on the models’ ability to comprehend high-level design concepts. Subsequently, the research moves into the augmented generation of software artifacts. Finally, organization-specific or task-specific methods are introduced to enhance software developers’ productivity and experience.CCS CONCEPTS• Software and its engineering → Software development process management; Software development methods;",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10449667/,Large Language Models for Software Engineering: Survey and Open Problems,"['Angela Fan', 'Beliz Gokkaya', 'Mark Harman', 'Mitya Lyubarskiy', 'Shubho Sengupta', 'Shin Yoo']",,"This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.",https://doi.org/10.1109/ICSE-FoSE59343.2023.00008,14-20 May 2023
https://ieeexplore.ieee.org/document/10988968/,Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities,"['Avishree Khare', 'Saikat Dutta', 'Ziyang Li', 'Alaia Solko-Breslin', 'Rajeev Alur', 'Mayur Naik']",,"Security vulnerabilities in modern software are prevalent and harmful. While automated vulnerability detection techniques have made promising progress, their scalability and applicability remain challenging. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted recent works to explore if LLMs can be used to detect security vulnerabilities. In this paper, we perform a more comprehensive study by examining a larger and more diverse set of datasets, languages, and LLMs, and qualitatively evaluating detection performance across prompts and vulnerability classes. Concretely, we evaluate the effectiveness of 16 pre-trained LLMs on 5,000 code samples-1,000 randomly selected each from five diverse security datasets. These balanced datasets encompass synthetic and real-world projects in Java and C/C++ and cover 25 distinct vulnerability classes. Our results show that LLMs across all scales and families show modest effectiveness in end-to-end reasoning about vul-nerabilities, obtaining an average accuracy of 62.8% and F1 score of 0.71 across all datasets. LLMs are significantly better at detecting vulnerabilities that typically only need intra-procedural reasoning, such as OS Command Injection and NULL Pointer Dereference. Moreover, LLMs report higher accuracies on these vulnerabilities than popular static analysis tools, such as CodeQL. We find that advanced prompting strategies that involve step-by-step analysis significantly improve performance of LLMs on real-world datasets in terms of F1 score (by up to 0.18 on average). Interestingly, we observe that LLMs show promising abilities at performing parts of the analysis correctly, such as identifying vulnerability-related specifications (e.g., sources and sinks) and leveraging natural language information to understand code behavior (e.g., to check if code is sanitized). We believe our insights can motivate future work on LLM-augmented vulnerability detect...",https://doi.org/10.1109/ICST62969.2025.10988968,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10705238/,A Framework for Security Testing of Large Language Models,['Kiril Traykov'],,"The purpose of this paper is to present a framework for testing of large language models (LLMs) for security vulnerabilities before their implementation to production environment. The paper discusses the latest developments in the Artificial Intelligence (AI) and Generative Artificial Intelligence (Generative AI) adoption in the industry, the expectations for further accelerated adoption and evolving regulatory landscape. An overview of the most significant risks and vulnerabilities of the LLMs such as prompt injection and denial of service have been presented with their mitigation strategies. A testing approach and testing framework have been developed and implemented with simple chatbot app. The test scenarios have been executed and results have been obtained for three open-source LLMs from which two pass the test and one failed and demonstrated the application of the proposed testing framework. Source code of the application and test script are published open source for reproducibility and reuse. In conclusion the with the confirmation of the results the limitation of the reliance on semantic similarity for the responses of the models was discussed together with three areas for further development: expanding the test scenarios to significant risks, integration with popular cloud continuous development platforms and integrating blockchain for transparent publication of the final test results.",https://doi.org/10.1109/IS61756.2024.10705238,29-31 August 2024
https://ieeexplore.ieee.org/document/11024524/,Predicting the Root Cause of Flaky Tests Based on Test Smells,"['Jing Wang', 'Weixi Zhang', 'Weiwei Wang', 'Ruilian Zhao', 'Ying Shang']",,"Flaky tests refer to test cases that exhibit inconsistent behaviors across multiple executions, potentially passing or failing unpredictably. They are frequently associated with suboptimal design practices that testers may utilize when crafting test cases, which undermine the quality of software testing. So, identifying the root causes of flaky tests is crucial for fixing them. Currently, inspired by the success of the Large Language Models (LLMs), researchers leverage the pre-trained language model to embed flaky test code as vectors and predict its root cause category based on vector similarity measures. However, such code embeddings generated by LLM mainly focus on capturing general semantic features but lack sufficient comprehension of the behavioral patterns involved in test scenarios, resulting in poor root cause identification. Test smells, which reflect poor coding practices or habits when writing test cases, provide complementary information in the root cause identification of test flakiness. Therefore, this paper proposes a root cause identification method for flaky tests based on test smells. Test smells are used to abstract and express behavioral patterns of test codes, and general semantic features extracted by vector embeddings to enhance the feature representation of flaky tests. Furthermore, to capture the complex nonlinear relationships between test smell features and code embeddings, a Feedforward Neural Network is constructed to categorize the root cause of test flakiness. To validate the effectiveness of our method, we performed evaluations on a dataset consisting of 451 Java flaky test cases. The experimental results indicate that our method achieves an F1-score of 80%, which is 7% higher than that of the baseline model that does not incorporate test smells.",https://doi.org/10.1109/ICSR66718.2025.00015,27-28 April 2025
https://ieeexplore.ieee.org/document/10731701/,Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation,"['Gavin Black', 'Varghese Mathew Vaidyan', 'Gurcan Comert']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10731701,"Fuzzing is a crucial technique for detecting software defects by dynamically generating and testing program inputs. This study introduces a framework designed to assess the application of Large Language Models (LLMs) to automate the generation of effective seed inputs for fuzzing, particularly in the Python programming environment where traditional approaches are less effective. Utilizing the Atheris fuzzing framework, we created over 38,000 seed inputs from LLMs targeted at 50 Python functions from widely-used libraries. Our findings underscore the critical role of LLM selection in seed effectiveness. In certain cases, seeds generated by LLMs rivaled or surpassed traditional fuzzing campaigns, with a corpus of fewer than 100 LLM-generated entries outperforming over 100,000 conventionally produced inputs. These seeds significantly improved code coverage and instruction count during fuzzing sessions, illustrating the efficacy of our framework in facilitating an automated, scalable approach to evaluating LLM effectiveness. The results, validated through linear regression analysis, demonstrate that selecting the appropriate LLM based on its training and capabilities is essential for optimizing fuzzing efficiency and facilitates the testing of future LLM versions.",https://doi.org/10.1109/ACCESS.2024.3484947,
https://ieeexplore.ieee.org/document/10638554/,On the Coupling between Vulnerabilities and LLM-Generated Mutants: A Study on Vul4J Dataset,"['Aayush Garg', 'Renzo Degiovanni', 'Mike Papadakis', 'Yves Le Traon']",,"With the release of powerful language models trained on large code corpus (e.g., CodeBERT, trained on 6.4 million programs), a new family of mutation testing tools has arisen that promises to generate more “natural” mutants, where the mutated code aims at following the implicit rules and coding conventions produced by programmers. In this paper, we empirically study the observable behavior of CodeBERT-generated mutants and to what extent are these coupled with software vulnerabilities. To do so, we carefully analyze 45 reproducible vulnerabilities from the Vul4J dataset to determine whether the mutants and vulnerabilities fail the same tests and whether the failures are for the same reasons or not. Hence, we define different degrees of vulnerability-coupling classes. Strongly coupled mutants fail the same tests for the same reasons as the vulnerabilities, while test coupled mutants fail the same tests but for some different reason as the vulnerabilities. Partial coupling classes are also considered. Overall, CodeBERT-generated mutants strongly coupled with 32 out of these 45 vulnerabilities (i.e. The mutants fail on the same tests for the same reasons), while another 7 vulnerabilities are test-coupled by CodeBERT mutants (i.e. The mutants fail on the same tests but not for the same reasons). Interestingly, CodeBERT mutants are diverse enough to couple vulnerabilities from 14 out of the 15 types of vulnerabilities explored, i.e., CWEs (Common Weakness Enumeration). Finally, we observe that strongly coupled mutants are scarce (1.17 % of the killable mutants), test coupled mutants represent 7.2 %, and 64.9 % of the killable mutants are not coupled with the vulnerabilities.",https://doi.org/10.1109/ICST60714.2024.00035,27-31 May 2024
https://ieeexplore.ieee.org/document/10223873/,Object Oriented BDD and Executable Human-Language Module Specification,"['Eric Lee', 'Jiayu Gong', 'Qinghong Cao']",,"This paper presents an approach to software development which uses a generative AI Model as compiler to translate human language requirements into high-level programming language. We propose an executable human-language module specification and a tool to support it, which has been used successfully for human-language UI test automation. We anticipate further development of this approach to enable complex software to be programmed in human language, allowing for more intuitive and efficient software development.",https://doi.org/10.1109/SNPD-Winter57765.2023.10223873,05-07 July 2023
https://ieeexplore.ieee.org/document/10989022/,Addressing Data Leakage in HumanEval Using Combinatorial Test Design,"['Jeremy S. Bradbury', 'Riddhi More']",,"The use of large language models (LLMs) is widespread across many domains, including Software Engineering, where they have been used to automate tasks such as program generation and test classification. As LLM-based methods continue to evolve, it is important that we define clear and robust methods that fairly evaluate performance. Benchmarks are a common approach to assess LLMs with respect to their ability to solve problem-specific tasks as well as assess different versions of an LLM to solve tasks over time. For example, the HumanEval benchmark is composed of 164 hand-crafted tasks and has become an important tool in assessing LLM-based program generation. However, a major barrier to a fair evaluation of LLMs using benchmarks like HumanEval is data contamination resulting from data leakage of benchmark tasks and solutions into the training data set. This barrier is compounded by the black-box nature of LLM training data which makes it difficult to even know if data leakage has occurred. To address the data leakage problem, we propose a new benchmark construction method where a benchmark is composed of template tasks that can be instantiated into new concrete tasks using combinatorial test design. Concrete tasks for the same template task must be different enough that data leakage has minimal impact and similar enough that the tasks are interchangeable with respect to performance evaluation. To assess our benchmark construction method, we propose HumanEval_T, an alternative benchmark to HumanEval that was constructed using template tasks and combinatorial test design.",https://doi.org/10.1109/ICST62969.2025.10989022,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10172800/,CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models,"['Caroline Lemieux', 'Jeevana Priya Inala', 'Shuvendu K. Lahiri', 'Siddhartha Sen']",,"Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.",https://doi.org/10.1109/ICSE48619.2023.00085,14-20 May 2023
https://ieeexplore.ieee.org/document/11029729/,Clozemaster: Fuzzing Rust Compiler by Harnessing Llms for Infilling Masked Real Programs,"['Hongyan Gao', 'Yibiao Yang', 'Maolin Sun', 'Jiangchang Wu', 'Yuming Zhou', 'Baowen Xu']",,"Ensuring the reliability of the Rust compiler is of paramount importance, given increasing adoption of Rust for critical systems development, due to its emphasis on memory and thread safety. However, generating valid test programs for the Rust compiler poses significant challenges, given Rust's complex syntax and strict requirements. With the growing popularity of large language models (LLMs), much research in software testing has explored using LLMs to generate test cases. Still, directly using LLMs to generate Rust programs often results in a large number of invalid test cases. Existing studies have indicated that test cases triggering historical compiler bugs can assist in software testing. Our investigation into Rust compiler bug issues supports this observation. Inspired by existing work and our empirical research, we introduce a bracket-based masking and filling strategy called clozeMask. The clozeMask strategy involves extracting test code from historical issue reports, identifying and masking code snippets with specific structures, and using an LLM to fill in the masked portions for synthesizing new test programs. This approach harnesses the generative capabilities of LLMs while retaining the ability to trigger Rust compiler bugs. It enables comprehensive testing of the compiler's behavior, particularly exploring edge cases. We implemented our approach as a prototype ClozeMaster. ClozeMaster has identified 27 confirmed bugs for rustc and mrustc, of which 10 have been fixed by developers. Furthermore, our experimental results indicate that ClozeMaster outperforms existing fuzzers in terms of code coverage and effectiveness.",https://doi.org/10.1109/ICSE55347.2025.00175,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10429996/,Exploring the Capability of ChatGPT in Test Generation,"['Gaolei Yi', 'Zizhao Chen', 'Zhenyu Chen', 'W. Eric Wong', 'Nicholas Chau']",,"The design of test is a crucial step in the field of software testing. The quality of test significantly impacts the effectiveness of software testing, with well-designed test cases improving the efficiency of bug detection. However, manual test case design and writing are often considered time-consuming and labor-intensive. With the emergence of large language models (LLMs), especially ChatGPT, the potential of LLMs in the field of test generation has become evident. Pretrained LLMs can learn and understand code in various programming languages and design test cases using multiple testing frameworks. In this paper, we used ChatGPT to generate tests for some tested projects. Through experiments, we found that ChatGPT has some gaps compared to traditional test generation tools, but its performance is closer to manual testing. However, the tests generated by ChatGPT exhibit higher readability. We believe that ChatGPT is better suited to serve as a manual testing assistant, helping understand the tested code and providing testing ideas.",https://doi.org/10.1109/QRS-C60940.2023.00013,22-26 October 2023
https://ieeexplore.ieee.org/document/10989026/,Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs,"['Jan Corazza', 'Ivan Gavran', 'Gabriela Moreira', 'Daniel Neider']",,"When blockchain systems are said to be trustless, what this really means is that all the trust is put into software. Thus, there are strong incentives to ensure blockchain software is correct–vulnerabilities here cost millions and break businesses. One of the most powerful ways of establishing software correctness is by using formal methods. Approaches based on formal methods, however, induce a significant overhead in terms of time and expertise required to successfully employ them. Our work addresses this critical disadvantage by automating the creation of a formal model–a mathematical abstraction of the software system–which is often a core task when employing formal methods. We perform model synthesis in three phases: we first transpile the code into model stubs; then we “fill in the blanks” using a large language model (LLM); finally, we iteratively repair the generated model, on both syntactical and semantical level. In this way, we significantly reduce the amount of time necessary to create formal models and increase accessibility of valuable software verification methods that rely on them. The practical context of our work was reducing the time-to-value of using formal models for correctness audits of smart contracts.",https://doi.org/10.1109/ICST62969.2025.10989026,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10992399/,Can Large Language Models Discover Metamorphic Relations? A Large-Scale Empirical Study,"['Jiaming Zhang', 'Chang-ai Sun', 'Huai Liu', 'Sijin Dong']",,"Software testing is a mainstream approach for software quality assurance. One fundamental challenge for testing is that in many practical situations, it is very difficult to verify the correctness of test results given inputs for Software Under Test (SUT), which is known as the oracle problem. Metamorphic Testing (MT) is a software testing technique that can effectively alleviate the oracle problem. The core component of MT is a set of Metamorphic Relations (MRs), which are basically the necessary properties of SUT, represented in the form of relationship among multiple inputs and their corresponding expected outputs. Different methods have been proposed to support the systematic MR identification. However, most of them still rely heavily on test engineers' understanding of the SUT and involve massive manual work. Although a few preliminary studies have shown LLMs' viability in generating MRs, there does not exist a thorough and in-depth investigation on their capability in MR identification. We are thus motivated to conduct a comprehensive and large-scale empirical study to systematically evaluate the performance of LLMs in identifying appropriate MRs for a wide variety of software systems. This study makes use of 37 SUTs collected from previous MT studies. Prompts are constructed for two LLMs, gpt-3.5-turbo-1106 and gpt-4-1106-preview, to perform the MR identification for each SUT. The empirical results demonstrate that both LLMs can generate a large amount of MR candidates (MRCs). Among them, 29.86% and 43.79% of all MRCs are identified as the MRs valid for the corresponding SUT, respectively. In addition, 24.59% and 38.63% of all MRCs are MRs that had never been identified in previous studies. Our study not only reinforces LLM-based MR identification as a promising research direction for MT, but also provides some practical guidelines for how to further improve LLMs' performance in generating good MRs.",https://doi.org/10.1109/SANER64311.2025.00011,04-07 March 2025
https://ieeexplore.ieee.org/document/10765033/,On the Evaluation of Large Language Models in Unit Test Generation,"['Lin Yang', 'Chen Yang', 'Shutao Gao', 'Weijing Wang', 'Bo Wang', 'Qihao Zhu']",,"Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs’ capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.CCS CONCEPTS • Software and its engineering → Software testing and debugging.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10638609/,Quantizing Large-Language Models for Predicting Flaky Tests,"['Shanto Rahman', 'Abdelrahman Baz', 'Sasa Misailovic', 'August Shi']",,"A major challenge in regression testing practice is the presence of flaky tests, which non-deterministically pass or fail when run on the same code. Previous research identified multiple categories of flaky tests. Prior research has also de-veloped techniques for automatically detecting which tests are flaky or categorizing flaky tests, but these techniques generally involve repeatedly rerunning tests in various ways, making them costly to use. Although several recent approaches have utilized large-language models (LLMs) to predict which tests are flaky or predict flaky-test categories without needing to rerun tests, they are costly to use due to relying on a large neural network to perform feature extraction and prediction. We propose FlakyQ to improve the effectiveness of LLM-based flaky-test prediction by quantizing LLM's weights. The quantized LLM can extract features from test code more efficiently. To make up for loss in prediction performance due to quantization, we further train a traditional ML classifier (e.g., a random forest) to learn from the quantized LLM-extracted features and do the same prediction. The final model has similar prediction performance while running faster than the non-quantized LLM. Our evaluation finds that FlakyQ classifiers consistently improves prediction time over the non-quantized LLM classifier, saving 25.4% in prediction time over all tests, along with a 48.4 % reduction in memory usage. Furthermore, prediction performance is equal or better than the non-quantized LLM classifier.",https://doi.org/10.1109/ICST60714.2024.00018,27-31 May 2024
https://ieeexplore.ieee.org/document/11031392/,Defining a New Metric for Detecting Bias in Software Systems: Towards Ethical Software Engineering,"['Ahmed Abdelraheem', 'Malak Elbanna', 'Mohamed Elnaggar', 'Doaa Shawky']",,"Bias in software systems poses ethical concerns that may lead to unintended discrimination, particularly when sensitive variables (e.g., gender, ethnicity) influence decision-making processes. While bias detection in machine learning models has been extensively studied, traditional software systems remain largely unexplored. However, implicit bias can manifest in conditional logic, user role definitions, or static decision trees, which directly influences user experience and access equity in real-world applications (e.g., government services or healthcare platforms). This paper presents a novel static analysis methodology for detecting and quantifying bias in general-purpose software systems. The proposed approach leverages static backward slicing to isolate relevant code, builds a Control Flow Graph (CFG) to trace sensitive variables in conditional branches, and utilizes a Control Dependency Graph (CDG) to assess bias propagation in a weighted-analysis format. Two bias metrics are inferred from the process: Bias Impact Score (BIS), which quantifies how the detected bias influences code execution, and the Bias Severity Score (BSS), which measures the broader implications of the impact. A final composite metric is introduced combining static code structure and ML-based sensitivity analysis. The proposed methodology is evaluated using an LLM-generated dataset of 3920 code snippets from prior research, covering different demographic bias directions such as ethnicity, gender, religion, and occupation. Results, achieving 94.6% accuracy in bias detection, show that the proposed methodology effectively identifies and quantifies bias, allowing developers to mitigate ethical risks early in the software development lifecycle. This research paper provides a foundation for ethical software engineering by offering a systematic and scalable approach to bias detection not only limited to AI-driven models. The methodology currently focuses on Python code and may require adaptation ...",https://doi.org/10.1109/ICEENG64546.2025.11031392,12-15 May 2025
https://ieeexplore.ieee.org/document/10647083/,Neural Fault Injection: Generating Software Faults from Natural Language,"['Domenico Cotroneo', 'Pietro Liguori']",,"Traditional software fault injection methods, while foundational, face limitations in adequately representing real-world faults, offering customization, and requiring significant manual effort and expertise. This paper introduces a novel methodology that harnesses the capabilities of Large Language Models (LLMs) augmented with Reinforcement Learning from Human Feedback (RLHF) to overcome these challenges. The usage of RLHF emphasizes an iterative refinement process, allowing testers to provide feedback on generated faults, which is then used to enhance the LLM’s fault generation capabilities, ensuring the generation of fault scenarios that closely mirror actual operational risks. This innovative methodology aims to significantly reduce the manual effort involved in crafting fault scenarios as it allows testers to focus on higher-level testing strategies, hence paving the way to new possibilities for enhancing the dependability of software systems.",https://doi.org/10.1109/DSN-S60304.2024.00016,24-27 June 2024
https://ieeexplore.ieee.org/document/10962528/,Combinatorial Test Design Model Creation using Large Language Models,"['Deborah Ann Furman', 'Eitan Farchi', 'Michael Edward Gildein', 'Andrew C. M. Hicks', 'Ryan Thomas Rawlins']",,"In this paper, we report on our initial experience in using Large Language Models (LLMs), which continue to impact a growing multitude of domains, further expanding machine learning applicability. One possible use case is to apply LLMs as a tool to help drive more optimized test coverage via assisting to generate Combinatorial Test Design (CTD) models. This can lower the entry barrier for new CTD practitioners by requiring less subject matter expertise to generate a basic CTD model. In this paper we report on our initial experience in using LLMs to generate a base CTD model and analyze the usefulness of the approach. In common testing scenarios, the LLMs easily provide the necessary attributes and values that are needed to define the CTD model. Prompting the LLM for additional use cases is useful in highlighting possible interactions and determining constraints of the attributes identified in the first stage. Combining the two stages together facilitates the creation of base CTD models.",https://doi.org/10.1109/ICSTW64639.2025.10962528,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/11029962/,SpecGen: Automated Generation of Formal Program Specifications via Large Language Models,"['Lezhi Ma', 'Shangqing Liu', 'Yi Li', 'Xiaofei Xie', 'Lei Bu']",,"In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on...",https://doi.org/10.1109/ICSE55347.2025.00129,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/11029748/,TOGLL: Correct and Strong Test Oracle Generation with LLMS,"['Soneya Binta Hossain', 'Matthew B. Dwyer']",,"Test oracles play a crucial role in software testing, enabling effective bug detection. Despite initial promise, neural methods for automated test oracle generation often result in a large number of false positives and weaker test oracles. While LLMs have shown impressive effectiveness in various software engineering tasks, including code generation, test case creation, and bug fixing, there remains a notable absence of large-scale studies exploring their effectiveness in test oracle generation. The question of whether LLMs can address the challenges in effective oracle generation is both compelling and requires thorough investigation. In this research, we present the first comprehensive study to investigate the capabilities of LLMs in generating correct, diverse, and strong test oracles capable of effectively identifying a large number of unique bugs. To this end, we fine-tuned seven code LLMs using six distinct prompts on a large dataset consisting of 110 Java projects. Utilizing the most effective finetuned LLM and prompt pair, we introduce TOGLL, a novel LLM-based method for test oracle generation. To investigate the generalizability of TOGLL, we conduct studies on 25 unseen large-scale Java projects. Besides assessing the correctness, we also assess the diversity and strength of the generated oracles. We compare the results against EvoSuite and the state-of-the-art neural method, TOGA. Our findings reveal that TOGLL can produce 3.8 times more correct assertion oracles and 4.9 times more exception oracles than TOGA. Regarding bug detection effectiveness, TOGLL can detect 1,023 unique mutants that EvoSuite cannot, which is ten times more than what TOGA can detect. Additionally, TOGLL significantly outperforms TOGA in detecting real bugs from the Defects4J dataset.",https://doi.org/10.1109/ICSE55347.2025.00098,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10556390/,Using GitHub Copilot for Test Generation in Python: An Empirical Study,"['Khalid El Haji', 'Carolin Brandt', 'Andy Zaidman']",,"Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot’s test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations. Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28%
 of the tests generated by Copilot are passing tests; 54.72%
 of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45%
 of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.",,15-16 April 2024
https://ieeexplore.ieee.org/document/10647136/,What’s in a Display Name? An Empirical Study on the Use of Display Names in Open-Source JUnit Tests,"['Yining Qiao', 'José Miguel Rojas']",,"Readability is an important aspect of any production or test code artefact. During development, testing and maintenance, the readability of a unit test can be a key contributor to its usefulness for a range of tasks, including refactoring, debugging, and test augmentation. Several strategies have been proposed both in academia and in industry to build readability into unit tests, e.g., test code summarisation and test naming conventions. Display names constitute an industry-led effort to incorporate natural language descriptions into unit tests. In this paper, we investigate the use of display names in a large dataset of open-source Java projects. Our study reveals that despite being a stable feature of the JUnit framework for at least five years, display names are not widely used in open-source projects yet. We analyse existing display names in terms of length, language and grammatical structure, explore the use of a large language model to generate display names similar to those open-source developers write, and develop a taxonomy of display name smells aimed at fostering a more cohesive and coherent use of the feature by developers.",,20-20 April 2024
https://ieeexplore.ieee.org/document/11006641/,From LLMs to Randomness: Analyzing Program Input Efficacy With Resource and Language Metrics,"['Gavin Black', 'Eric Yocam', 'Varghese Mathew Vaidyan', 'Gurcan Comert', 'Yong Wang']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11006641,"Security-focused program testing typically focuses on crash detection and code coverage while overlooking additional system behaviors that can impact program confidentiality and availability. To address this gap, we propose a statistical framework that combines embedding-based anomaly detection, resource usage metrics, and resource-state distance measures to systematically profile software behaviors beyond traditional coverage-based methods. Leveraging over 5 million labeled samples from 50 Python programs, we evaluate how these independent scoring terms distinguish among different sources of input, including Large Language Model (LLM)-generated inputs, and demonstrate how standard statistical tests (e.g., Kolmogorov—Smirnov and Kendall’s τ
 ) confirm their effectiveness. Our findings show that LLM-generated samples can trigger diverse behaviors but are often less effective at exploring resource usage dynamics (CPU, memory) compared with conventional fuzzing. However, combining LLM outputs with existing techniques broadens behavior coverage and reveals commonalities between commercial LLM outputs. We provide open-source tools for this evaluation framework, demonstrating the potential to refine software testing by integrating behavior metrics into security-testing workflows.",https://doi.org/10.1109/ACCESS.2025.3571205,
https://ieeexplore.ieee.org/document/10753556/,Large Language Model-Based Optimization for System-Level Test Program Generation,"['Denis Schwachhofer', 'Peter Domanski', 'Steffen Becker', 'Stefan Wagner', 'Matthias Sauer', 'Dirk Pflüger']",,"System-Level Test (SLT) is essential for testing integrated circuits, focusing on functional and non-functional properties of the Device under Test (DUT). Traditionally, test engineers manually create tests with commercial software to simulate the DUT's end-user environment. This process is both time-consuming and offers limited control over non-functional properties. This paper proposes Large Language Models (LLMs) enhanced by Structural Chain of Thought (SCoT) prompting, a temperature schedule, and a pool of previously generated snippets to generate high-quality code snippets for SLT. We repeatedly query the LLM for a better snippet using previously generated snippets as examples, thus creating an iterative optimization loop. This approach can automatically generate snippets for SLT that target specific non-functional properties, reducing time and effort. Our findings show that this approach improves the quality of the generated snippets compared to unstructured prompts containing only a task description.",https://doi.org/10.1109/DFT63277.2024.10753556,08-10 October 2024
https://ieeexplore.ieee.org/document/11029879/,A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs,"['Myeongsoo Kim', 'Tyler Stennett', 'Saurabh Sinha', 'Alessandro Orso']",,"As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API documentation languages, such as the OpenAPI Specification, has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in fault detection. To address these limitations, we present AutoRestTest, the first black-box tool to adopt a dependency-embedded multi-agent approach for REST API testing that integrates multi-agent reinforcement learning (MARL) with a semantic property dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents-API, dependency, parameter, and value agents-collaborate to optimize API exploration. LLMs handle domain-specific value generation, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest on 12 real-world REST services shows that it outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which generates realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to trigger an internal server error in the Spotify service. Our ablation study illustrates that each component of AutoRestTest-the SPDG, the LLM, and the agent-learning mechanism-contributes to its overall effectiveness.",https://doi.org/10.1109/ICSE55347.2025.00179,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10628487/,Requirements are All You Need: From Requirements to Code with LLMs,['Bingyang Wei'],,"The pervasive use of textual formats in the documentation of software requirements presents a great opportunity for applying large language models (LLMs) to software engineering tasks. High-quality software requirements not only enhance the manual software development process but also position organizations to fully harness the potential of the emerging LLMs technology. This paper introduces a tailored LLM for automating the generation of code snippets from well-structured requirements documents. This LLM is augmented with knowledge, heuristics, and instructions that are pertinent to the software development process, requirements analysis, object-oriented design, and test-driven development, effectively emulating the expertise of a seasoned software engineer. We introduce a “Progressive Prompting” method that allows software engineers to engage with this LLM in a stepwise manner. Through this approach, the LLM incrementally tackles software development tasks by interpreting the provided requirements to extract functional requirements, using these to create object-oriented models, and subsequently generating unit tests and code based on the object-oriented designs. We demonstrate the LLM's proficiency in comprehending intricate user requirements and producing robust design and code solutions through a case study focused on the development of a web project. This study underscores the potential of integrating LLMs into the software development workflow to significantly enhance both efficiency and quality. The tailored LLM is available at https://chat.openai.com/g/g-bahoiKzkB-software-engineer-gpt.",https://doi.org/10.1109/RE59067.2024.00049,24-28 June 2024
https://ieeexplore.ieee.org/document/10923987/,AI-Powered Multi-Agent Framework for Automated Unit Test Case Generation: Enhancing Software Quality through LLM’s,"['Anusha Garlapati', 'M N V Satya Sai Muni Parmesh', 'Savitha', 'Jaisri S']",,"Recent years have witnessed an enormous rise in the design, repair and the enhancement of software automation tests. The reliability of program’s unit testing has major impact on its overall performance. The anticipated influence of Artificial Intelligence advancements on test automation methodologies are significant. Many studies on automated testing implicitly assume that the test results are deterministic, means that similar tests faults remain same. The precision of software is largely ensured by unit testing. But writing unit tests manually is a time-consuming process, which leads us to drive into ""Automation Analysis"". Recent years comprised the application of Large Language Models (LLM’s) in numerous fields related to software development, especially the automated creation of unit testing.However, these frameworks require more instructions, or few shot learnings on sample tests that already exist. This research provides a comprehensive empirical assessment of the efficiency of LLM’s for automating unit testing production, with no need for further manual analysis. The method we employ is put into practice for test cases, an adaptable Agents and LLM-based testing framework that evaluates test cases generated, by reviewing and re-writing them in different phases. Evaluation of this test cases was done by using mistral-large LLM Model. The analysis results that developed acquired an overall coverage of 100% for code given. Finally, to enhance the typical evaluation, this research suggests and concludes that LLMs, can be successfully incorporated into present practices, through adaptative instructions and improvements.",https://doi.org/10.1109/GCAT62922.2024.10923987,04-06 October 2024
https://ieeexplore.ieee.org/document/10817956/,LLM - TG: Towards Automated Test Case Generation for Processors Using Large Language Models,"['Yifei Deng', 'Renzhi Chen', 'Chao Xiao', 'Zhijie Yang', 'Yuanfeng Luo', 'Jingyue Zhao']",,"Design verification (DV) has existed for decades and is crucial for identifying potential bugs before chip tape- out. Hand-crafting test cases is time-consuming and error-prone, even for experienced verification engineers. Prior work has attempted to lighten this burden by rule-guided random test case generation. However, this approach does not eliminate the manual effort required to write rules that describe detailed hardware behavior. Motivated by advances in large language models (LLMs), we explore their potential to capture register transfer level (RTL) behavior and construct prompts for test case generation based on RTL behavior. First, we introduce a prompt framework, LLM - Driven Test Generation (LLM - TG), to generate test cases, thereby enhancing LLMs' test generation capabilities. Additionally, we provide an open-source prompt library that offers a set of standardized prompts for processor verification, aiming to improve test generation efficiency. Lastly, we use an LLM to verify a 12-stage, multi-issue, out-of-order RV64GC processor, achieving at least an 8.34 % increase in block coverage and at least a 5.8 % increase in expression coverage compared to the state-of-the-art (SOTA) methods, LLM4DV and RISCV- DV. The prompt library is available at https://github.com/LLM-TGIPrompt_Library.",https://doi.org/10.1109/ICCD63220.2024.00066,18-20 November 2024
https://ieeexplore.ieee.org/document/11024370/,Trustworthiness of Large Language Models for Code,['Dipin Khati'],,"In recent years, Large Language Models for code (LLMc) have transformed the landscape of software engineering (SE), demonstrating significant efficacy in tasks such as code completion, summarization, review, tracing, translation, test case generation, clone detection, and bug fixing. Notably, GitHub Copilot and Google's CodeBot exemplify how LLMc contributes to substantial time and effort savings in software development. However, the widespread application of these models has raised critical concerns regarding their trustworthiness. The lack of well-defined trust metrics beyond mere accuracy poses significant risks, including potential security vulnerabilities and compromised data integrity. This dissertation proposes solving this pressing need by developing a comprehensive framework to evaluate LLMc's trustworthiness. We aim to establish contextualized definitions of trust, distrust, and trustworthiness specific to LLMc, identify key influencing factors, and create a standardized evaluation framework encompassing both model-based attributes and human-centric considerations. We will validate the framework's effectiveness through rigorous empirical studies and user evaluations and provide insights for targeted improvements in LLMc development. This dissertation seeks to enhance the reliability and transparency of LLMc, fostering their responsible integration into software engineering practices and paving the way for more trustworthy AI-assisted code generation.",https://doi.org/10.1109/ICSE-Companion66252.2025.00063,27 April 2025 - 03 May 2025
https://ieeexplore.ieee.org/document/10764936/,Test-Driven Development and LLM-based Code Generation,"['Noble Saji Mathews', 'Meiyappan Nagappan']",,"Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.CCS CONCEPTS• Software and its engineering → Software development techniques; • Computing methodologies → Artificial intelligence.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11028438/,Metamon: Finding Inconsistencies between Program Documentation and Behavior using Metamorphic LLM Queries,"['Hyeonseok Lee', 'Gabin An', 'Shin Yoo']",,"Code documentation can, if written precisely, help developers better understand the code they accompany. However, unlike code, code documentation cannot be automatically verified via execution, potentially leading to inconsistencies between documentation and the actual behavior. While such inconsistencies can be harmful for the developer’s understanding of the code, checking and finding them remains a costly task due to the involvement of human engineers. This paper proposes Metamon, which uses an existing search-based test generation technique to capture the current program behavior in the form of test cases, and subsequently uses LLM-based code reasoning to identify the generated regression test oracles that are not consistent with the program specifications in the documentation. Metamon is supported in this task by metamorphic testing and self-consistency. An empirical evaluation against 9,482 pairs of code documentation and code snippets, generated using five open-source projects from Defects4J v2.0.1, shows that Metamon can classify the code-and-documentation inconsistencies with a precision of 0.72 and a recall of 0.48.",https://doi.org/10.1109/LLM4Code66737.2025.00020,03-03 May 2025
https://ieeexplore.ieee.org/document/11052822/,Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements,"['Seyed Moein Abtahi', 'Akramul Azim']",,"This study examined code issue detection and revision automation by integrating Large Language Models (LLMs) such as OpenAI’s GPT-3.5 Turbo and GPT-4o into software development workflows. A static code analysis framework detects issues such as bugs, vulnerabilities, and code smells within a large-scale software project. Detailed information on each issue was extracted and organized to facilitate automated code revision using LLMs. An iterative prompt engineering process is applied to ensure that prompts are structured to produce accurate and organized outputs aligned with the project requirements. Retrieval-augmented generation (RAG) is implemented to enhance the relevance and precision of the revisions, enabling LLM to access and integrate real-time external knowledge. The issue of LLM hallucinations—where the model generates plausible but incorrect outputs—is addressed by a custom-built ""Code Comparison App,"" which identifies and corrects erroneous changes before applying them to the codebase. Subsequent scans using the static code analysis framework revealed a significant reduction in code issues, demonstrating the effectiveness of combining LLMs, static analysis, and RAG to improve code quality, streamline the software development process, and reduce time and resource expenditure.",https://doi.org/10.1109/Forge66646.2025.00017,27-28 April 2025
https://ieeexplore.ieee.org/document/10606356/,LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation,"['Sarah Fakhoury', 'Aaditya Naik', 'Georgios Sakkas', 'Saikat Chakraborty', 'Shuvendu K. Lahiri']",,"Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow TiCoder for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.",https://doi.org/10.1109/TSE.2024.3428972,
https://ieeexplore.ieee.org/document/10764938/,Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers,"['Yang Luo', 'Richard Yu', 'Fajun Zhang', 'Ling Liang', 'Yongqiang Xiong']",,"When using large language models (LLMs) for code translation of complex software, numerous compilation and runtime errors can occur due to insufficient context awareness. To address this issue, this paper presents a code translation method based on call graphs and bridged debuggers: TransGraph. TransGraph first obtains the call graph of the entire code project using the Language Server Protocol, which provides a detailed description of the function call relationships in the program. Through this structured view of the code, LLMs can more effectively handle large-scale and complex codebases, significantly reducing compilation errors. Furthermore, TransGraph, combined with bridged debuggers and dynamic test case generation, significantly reduces runtime errors, overcoming the limitations of insufficient test case coverage in traditional methods. In experiments on six datasets including CodeNet and Avatar, TransGraph outperformed existing code translation methods and LLMs in terms of translation accuracy, with improvements of up to 10.2%.CCS CONCEPTS• Software and its engineering → Source code generation; Translator writing systems and compiler generators; Parsers.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11052811/,Vulnerability-Triggering Test Case Generation from Third-Party Libraries,"['Yi Gao', 'Xing Hu', 'Zirui Chen', 'Tongtong Xu', 'Xiaohu Yang']",,"Open-source third-party libraries are widely used in software development. These libraries offer substantial advantages in terms of time and resource savings. However, a significant concern arises due to the publicly disclosed vulnerabilities within these libraries. Existing automated vulnerability detection tools often suffer from false positives and fail to accurately assess the propagation of inputs capable of triggering vulnerabilities from client projects to vulnerable code in libraries. In this paper, we propose a novel approach called VulEUT (Vulnerability Exploit Unit Test Generation), which combines vulnerability exploitation reachability analysis and LLM-based unit test generation. VulEUT is designed to automatically verify the exploitability of vulnerabilities in third-party libraries commonly used in Java client software projects. VulEUT first analyzes the client projects to determine the reachability of vulnerability conditions. And then, it leverages the Large Language Model (LLM) to generate unit tests for vulnerability confirmation. To evaluate the effectiveness of VulEUT, we collect 32 vulnerabilities from various third-party libraries and conduct experiments on 70 real client projects. Besides, we also compare our approach with two representative tools, i.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VulEUT, with 229 out of 292 generated unit tests successfully confirming vulnerability exploitation across 70 client projects, which outperforms baselines by 24%.",https://doi.org/10.1109/Forge66646.2025.00021,27-28 April 2025
https://ieeexplore.ieee.org/document/10711016/,Automated Control Logic Test Case Generation using Large Language Models,"['Heiko Koziolek', 'Virendra Ashiwal', 'Soumyadip Bandyopadhyay', 'Chandrika K R']",,"Testing PLC and DCS control logic in industrial automation is laborious and challenging since appropriate test cases are often complex and difficult to formulate. Researchers have previously proposed several automated test case generation approaches for PLC software applying symbolic execution and search-based techniques. Often requiring formal specifications and performing a mechanical analysis of programs, these approaches may uncover specific programming errors but some-times suffer from state space explosion and cannot process rather informal specifications. We proposed a novel approach for the automatic generation of PLC test cases that queries a Large Language Model (LLM) to synthesize test cases for code provided in a prompt. Experiments with ten open-source function blocks from the OSCAT automation library showed that the approach is fast, easy to use, and can yield test cases with high statement coverage for low-to-medium complex programs. However, we also found that LLM-generated test cases suffer from erroneous assertions in many cases, which still require manual adaption.",https://doi.org/10.1109/ETFA61755.2024.10711016,10-13 September 2024
https://ieeexplore.ieee.org/document/10538871/,LLMs for Hardware Security: Boon or Bane?,"['Rahul Kande', 'Vasudev Gohil', 'Matthew DeLorenzo', 'Chen Chen', 'Jeyavijayan Rajendran']",,"Large language models (LLMs) have emerged as transformative tools within the hardware design and verification lifecycle, offering numerous capabilities in accelerating design processes. Recent research has showcased the efficacy of LLMs in translating design specifications into source code through hardware description languages. Researchers are also using LLMs to generate test cases and write assertion rules to bolster the detection of hardware vulnerabilities. Thus, the semiconductor industry is swiftly integrating LLMs into its design workflows. However, this adoption is not without its challenges.While LLMs offer remarkable benefits, they concurrently introduce security concerns that demand a thorough examination. These concerns manifest as potential vulnerabilities indirectly introduced into the designs while generating the design code, or by directly equipping the attackers with novel avenues for exploitation. In this paper, we discuss the emerging security implications due to the capabilities introduced by LLMs in the context of hardware design verification, evaluate the capabilities of existing security detection and mitigation techniques, and highlight the possible future security attacks that use LLMs.",https://doi.org/10.1109/VTS60656.2024.10538871,22-24 April 2024
https://ieeexplore.ieee.org/document/10967340/,Unraveling the Potential of Large Language Models in Code Translation: How Far are We?,"['Qingxiao Tao', 'Tingrui Yu', 'Xiaodong Gu', 'Beijun Shen']",,"While large language models (LLMs) exhibit state-of-the-art performance in various tasks, recent studies have revealed their struggle for code translation. This is because they haven't been extensively pre-trained with parallel multilingual code, which code translation heavily depends on. Moreover, existing benchmarks only cover a limited subset of common programming languages, and thus cannot reflect the full potential of LLMs in code translation. In this paper, we conduct a large-scale empirical study to exploit the capabilities and incapabilities of LLMs in code translation tasks. We first craft a novel benchmark called PolyHumanEval by extending HumanEval to a multilingual benchmark of 14 languages. With PolyHumanEval, we then perform over 110,000 translations with bleeding-edge code LLMs. The result shows LLMs' suboptimal performance on Python to other languages and the negligible impact of widely adopted LLM optimization techniques such as conventional pre-training and instruction tuning on code translation. To further uncover the potential of LLMs in code translation, we propose two methods: (1) intermediary translation which selects an intermediary language between the source and target ones; and (2) self-training which fine-tunes LLMs on self-generated parallel data. Evaluated with CodeLlama-13B, our approach yields an average improvement of 11.7% computation accuracy on Python-to-other translations. Notably, we interestingly find that Go can serve as a lingua franca for translating between any two studied languages.",https://doi.org/10.1109/APSEC65559.2024.00046,03-06 December 2024
https://ieeexplore.ieee.org/document/10554734/,CodeFuse-13B: A Pretrained Multi-Lingual Code Large Language Model,"['Peng Di', 'Jianguo Li', 'Hang Yu', 'Wei Jiang', 'Wenting Cai', 'Yang Cao']",,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectivness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CODEFuSE-13B, an open-sourced pre-trained code LLM 2. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CODEFUSE achieves its effectiveness by utilizing a high-quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HUMANEvAL-x, and the specially designed CODEFUSEEvAL for Chinese prompts. To assess the effectiveness of CODEFUSE, we actively collected valuable human feed-back from the AntGroup's software development process where CODEFUSE has been successfully deployed. The results demonstrate that CODEFUSE-13B achieves a HUMANEvAL pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CODEFUSE performs better than other models when confronted with Chinese prompts.",https://doi.org/10.1145/3639477.3639719,14-20 April 2024
https://ieeexplore.ieee.org/document/11066171/,TransferFuzz-Pro: Large Language Model Driven Code Debugging Technology for Verifying Propagated Vulnerability,"['Siyuan Li', 'Kaiyu Xie', 'Yuekang Li', 'Hong Li', 'Yimo Ren', 'Limin Sun']",,"Code reuse in software development frequently facilitates the spread of vulnerabilities, leading to imprecise scopes of affected software in CVE reports. Traditional methods focus primarily on detecting reused vulnerability code in target software but lack the ability to confirm whether these vulnerabilities can be triggered in new software contexts. In previous work, we introduced the TransferFuzz framework to address this gap by using historical trace-based fuzzing. However, its effectiveness is constrained by the need for manual intervention and reliance on source code instrumentation. To overcome these limitations, we propose TransferFuzz-Pro, a novel framework that integrates Large Language Model (LLM)-driven code debugging technology. By leveraging LLM for automated, human-like debugging and Proof-of-Concept (PoC) generation, combined with binary-level instrumentation, TransferFuzz-Pro extends verification capabilities to a wider range of targets. Our evaluation shows that TransferFuzz-Pro is significantly faster and can automatically validate vulnerabilities that were previously unverifiable using conventional methods. Notably, it expands the number of affected software instances for 15 CVE-listed vulnerabilities from 15 to 53 and successfully generates PoCs for various Linux distributions. These results demonstrate that TransferFuzz-Pro effectively verifies vulnerabilities introduced by code reuse in target software and automatically generation PoCs.",https://doi.org/10.1109/TSE.2025.3584774,
https://ieeexplore.ieee.org/document/10795597/,Transforming Software Development: A Study on the Integration of Multi-Agent Systems and Large Language Models for Automatic Code Generation,"['Rolando Ramírez-Rueda', 'Edgard Benítez-Guerrero', 'Carmen Mezura-Godoy', 'Everardo Bárcenas']",,"This paper explores the integration of Multi-Agent Systems (MAS) and Large Language Models (LLMs) for auto-matic code generation, addressing the limitations of traditional manual coding. By conducting a comprehensive review of existing literature and analyzing a practical case study, we demonstrate how MAS and LLMs can collaboratively enhance software development processes. The research focuses on the technical and theoretical challenges of this integration, highlighting the potential for improved productivity, adaptability, and quality in code generation. The findings contribute to AI-based software engineering by revealing new research directions in collective intelligence and automated programming.",https://doi.org/10.1109/CONISOFT63288.2024.00013,28 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10765071/,ℬ4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests,"['Mouxiang Chen', 'Zhongxin Liu', 'He Tao', 'Yusu Hong', 'David Lo', 'Xin Xia']",,"Selecting the best code solution from multiple generated ones is an essential task in code generation, which can be achieved by using some reliable validators (e.g., developer-written test cases) for assistance. Since reliable test cases are not always available and can be expensive to build in practice, researchers propose to automatically generate test cases to assess code solutions. However, when both code solutions and test cases are plausible and not reliable, selecting the best solution becomes challenging. Although some heuristic strategies have been proposed to tackle this problem, they lack a strong theoretical guarantee and it is still an open question whether an optimal selection strategy exists. Our work contributes in two ways. First, we show that within a Bayesian framework, the optimal selection strategy can be defined based on the posterior probability of the observed passing states between solutions and tests. The problem of identifying the best solution is then framed as an integer programming problem. Second, we propose an efficient approach for approximating this optimal (yet uncomputable) strategy, where the approximation error is bounded by the correctness of prior knowledge. We then incorporate effective prior knowledge to tailor code generation tasks. Both theoretical and empirical studies confirm that existing heuristics are limited in selecting the best solutions with plausible test cases. Our proposed approximated optimal strategy ℬ4 significantly surpasses existing heuristics in selecting code solutions generated by large language models (LLMs) with LLM-generated tests, achieving a relative performance improvement by up to 50% over the strongest heuristic and 246% over the random selection in the most challenging scenarios. Our code is publicly available at https://github.com/ZJU-CTAG/B4.CCS CONCEPTS• Computing methodologies → Artificial intelligence; • Software and its engineering → Software design engineering.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10745813/,ECG: Augmenting Embedded Operating System Fuzzing via LLM-Based Corpus Generation,"['Qiang Zhang', 'Yuheng Shen', 'Jianzhong Liu', 'Yiru Xu', 'Heyuan Shi', 'Yu Jiang']",,"Embedded operating systems (Embedded OSs) power much of our critical infrastructure but are, in general, much less tested for bugs than general-purpose operating systems. Fuzzing Embedded OSs encounter significant roadblocks due to much less documented specifications, an inherent ineffectiveness in generating high-quality payloads. In this article, we propose ECG, an Embedded OS fuzzer empowered by large language models (LLMs) to sufficiently mitigate the aforementioned issues. ECG approaches fuzzing Embedded OS by automatically generating input specifications based on readily available source code and documentation, instrumenting and intercepting execution behavior for directional guidance information, and generating inputs with payloads according to the pregenerated input specifications and directional hints provided from previous runs. These methods are empowered by using an interactive refinement method to extract the most from LLMs while using established parsing checkers to validate the outputs. Our evaluation results demonstrate that ECG uncovered 32 new vulnerabilities across three popular open-source Embedded OS (RT-Linux, RaspiOS, and OpenWrt) and detected ten bugs in a commercial Embedded OS running on an actual device. Moreover, compared to Syzkaller, Moonshine, KernelGPT, Rtkaller, and DRLF, ECG has achieved additional kernel code coverage improvements of 23.20%, 19.46%, 10.96%, 15.47%, and 11.05%, respectively, with an overall average improvement of 16.02%. These results underscore ECG’s enhanced capability in uncovering vulnerabilities, thus contributing to the overall robustness and security of the Embedded OS.",https://doi.org/10.1109/TCAD.2024.3447220,
https://ieeexplore.ieee.org/document/11028927/,FuzzCoder: Code Large Language Model-Based Fuzz Testing for Industrial IoT Programs,"['Liqun Yang', 'Chaoren Wei', 'Jian Yang', 'Wanxu Xia', 'Yuze Yang', 'Yang Luo']",,"Fuzz testing is an dynamic program analysis technique designed for discovering vulnerabilities in IoT systems. The core goal is to deliberately feed maliciously crafted inputs into an IoT device or service, triggering vulnerabilities such as system crashes, buffer overflow exploits, and memory corruption, etc. Efficiently generating malicious inputs remains challenging, with leading methods often relying on randomly mutating existing valid inputs. In this work, we propose to adopt fine-tuned large language models (FuzzCoder) to learn patterns in the input files from successful attacks to guide future fuzzing explorations. Specifically, we develop a framework that leverages code LLMs to guide the mutation process to perform meaningful input mutations. We formulate the mutation process as the sequenceto-sequence modeling, where LLM receives a sequence of bytes and outputs the mutated byte sequence. FuzzCoder is fine-tuned on our created instruction dataset (FuzzInstruct), where the successful fuzzing history is collected from the heuristic fuzzing tool. FuzzCoder can predict mutation positions and strategies for input files to trigger abnormal behaviors of the program. Most importantly, the experiment reveals results that FuzzCoder achieves better fuzzing performance compared to traditional and other AFL-based fuzzers, such as AFL, AFL++, AFLSmart, etc. On average, FuzzCoder achieves an improvement in code coverage of more than 20%, along with a significant increase in the number of crashes. 1",https://doi.org/10.1109/JIOT.2025.3577602,
https://ieeexplore.ieee.org/document/10463245/,"Generative Artificial Intelligence for Industry: Opportunities, Challenges, and Impact",['Barun Kumar Saha'],,"The recent advances in Generative Artificial In-telligence (GenAI) and Large Language Models (LLMs) have generated significant interest across the world. For a successful adoption of GenAI and LLMs by industry, it is critical to identify their potential benefits, impact, and challenges. Accordingly, in this work, we investigate a few use cases of LLMs, which are relevant across most industry segments. In order to empirically evaluate the impact of GenAI on the code generation use case, we build CodePrompt, a handcrafted dataset of sequential prompts used by a human user to generate code. We approximate efficiency by considering the ratio of the number of tokens of code generated by an LLM to the number of tokens in the user's prompt. Experimental results reveal that a sequential trial of prompts for code generation may lead to an efficiency factor of about 6.33, on average, which means that a user's effort is reduced to about one-sixth.",https://doi.org/10.1109/ICAIIC60209.2024.10463245,19-22 February 2024
https://ieeexplore.ieee.org/document/10930847/,Evaluating Spectrum-Based Fault Localization on Deep Learning Libraries,"['Ming Yan', 'Junjie Chen', 'Tianjie Jiang', 'Jiajun Jiang', 'Zan Wang']",,"Deep learning (DL) libraries have become increasingly popular and their quality assurance is also gaining significant attention. Although many fault detection techniques have been proposed, effective fault localization techniques tailored to DL libraries are scarce. Due to the unique characteristics of DL libraries (e.g., complicated code architecture supporting DL model training and inference with extensive multidimensional tensor calculations), the effectiveness of existing fault localization techniques for traditional software is also unknown on DL library faults. To bridge this gap, we conducted the first empirical study to investigate the effectiveness of fault localization on DL libraries. Specifically, we evaluated spectrum-based fault localization (SBFL) due to its high generalizability and affordable overhead on such complicated libraries. Based on the key aspects in SBFL, our study investigated the effectiveness of SBFL with different sources of passing test cases (including human-written, fuzzer-generated, and mutation-based test cases) and various suspicious value calculation methods. In particular, mutation-based test cases are produced by our designed rule-based mutation technique and LLM-based mutation technique tailored to DL library faults. To enable our extensive study, we built the first benchmark (Defects4DLL), which contains 120 real-world faults in PyTorch and TensorFlow with easy-to-use experimental environments. Our study delivered a series of useful findings. For example, the rule-based approach is effective in localizing crash faults in DL libraries, successfully localizing 44.44% of crash faults within Top-10 functions and 74.07% of crash faults within Top-10 files, while the passing test cases from DL library fuzzers perform poorly on this task. Furthermore, based on our findings on the complementarity of different sources, we designed a hybrid technique by effectively integrating human-written, LLM-mutated, rule-based mutated test cases,...",https://doi.org/10.1109/TSE.2025.3552622,
https://ieeexplore.ieee.org/document/11024336/,Students' Perception of ChatGPT in Software Engineering: Lessons Learned from Five Courses,"['Luciano Baresi', 'Andrea De Lucia', 'Antinisca Di Marco', 'Massimiliano Di Penta', 'Davide Di Ruscio', 'Leonardo Mariani']",,"A few years after their release, Large Language Models (LLMs)-based tools are becoming an essential component of software education, as calculators are used in math courses. When learning software engineering (SE), the challenge is the extent to which LLMs are suitable and easy to use for different software development tasks. In this paper, we report the findings and lessons learned from using LLM-based tools-ChatGPT in particular-in five SE courses from four universities. After instructing students on the LLM potentials in SE and about prompting strategies, we ask participants to complete a survey and be involved in semi-structured interviews. The collected results report (i) indications about the usefulness of the LLM for different tasks, (ii) challenges to prompt the LLM, i.e., interact with it, (iii) challenges to adapt the generated artifacts to their own needs, and (iv) wishes about some valuable features students would like to see in LLM-based tools. Although results vary among different courses, also because of students' seniority and course goals, the perceived usefulness is greater for lowlevel phases (e.g., coding or debugging/fault localization) than for analysis and design phases. Interaction and code adaptation challenges vary among tasks and are mostly related to the need for task-specific prompts, as well as better specification of the development context.",https://doi.org/10.1109/CSEET66350.2025.00023,27 April 2025 - 03 May 2025
https://ieeexplore.ieee.org/document/11029828/,Treefix: Enabling Execution with a Tree of Prefixes,"['Beatriz Souza', 'Michael Pradel']",,"The ability to execute code is a prerequisite for various dynamic program analyses. Learning-guided execution has been proposed as an approach to enable the execution of arbitrary code snippets by letting a neural model predict likely values for any missing variables. Although state-of-the-art learning-guided execution approaches, such as LExecutor, can enable the execution of a relative high amount of code, they are limited to predicting a restricted set of possible values and do not use any feedback from previous executions to execute even more code. This paper presents Treefix, a novel learning-guided execution approach that leverages LLMs to iteratively create code prefixes that enable the execution of a given code snippet. The approach addresses the problem in a multi-step fashion, where each step uses feedback about the code snippet and its execution to instruct an LLM to improve a previously generated prefix. This process iteratively creates a tree of prefixes, a subset of which is returned to the user as prefixes that maximize the number of executed lines in the code snippet. In our experiments with two datasets of Python code snippets, Treefix achieves 25% and 7% more coverage relative to the current state of the art in learning-guided execution, covering a total of 84% and 82% of all lines in the code snippets.",https://doi.org/10.1109/ICSE55347.2025.00215,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10548854/,Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer,"['Jingzhou Fu', 'Jie Liang', 'Zhiyong Wu', 'Yu Jiang']",,"Effective DBMS fuzzing relies on high-quality initial seeds, which serve as the starting point for mutation. These initial seeds should incorporate various DBMS features to explore the state space thoroughly. While built-in test cases are typically used as initial seeds, many DBMSs lack comprehensive test cases, making it difficult to apply state-of-the-art fuzzing techniques directly. To address this, we propose Sedar which produces initial seeds for a target DBMS by transferring test cases from other DBMSs. The underlying insight is that many DBMSs share similar functionalities, allowing seeds that cover deep execution paths in one DBMS to be adapted for other DBMSs. The challenge lies in converting these seeds to a format supported by the grammar of the target database. Sedar follows a three-step process to generate seeds. First, it executes existing SQL test cases within the DBMS they were designed for and captures the schema information during execution. Second, it utilizes large language models (LLMs) along with the captured schema information to guide the generation of new test cases based on the responses from the LLM. Lastly, to ensure that the test cases can be properly parsed and mutated by fuzzers, Sedar temporarily comments out unparsable sections for the fuzzers and uncomments them after mutation. We integrate Sedar into the DBMS fuzzers SQUIRREL and Griffin, targeting DBMSs such as Virtuoso, Mon-etDB, DuckDB, and ClickHouse. Evaluation results demonstrate significant improvements in both fuzzers. Specifically, compared to SQUIRREL and Griffin with non-transferred seeds, Sedar enhances code coverage by 72.46%-214.84% and 21.40%-194.46%; compared to SQUIRREL and Griffin with native test cases of these DBMSs as initial seeds, incorporating the transferred seeds of Sedar results in an improvement in code coverage by 4.90%-16.20% and 9.73%-28.41 %. Moreover, Sedar discovered 70 new vulnerabilities, with 60 out of them being uniquely found by Sedar with tra...",https://doi.org/10.1145/3597503.3639210,14-20 April 2024
https://ieeexplore.ieee.org/document/10589904/,Navigating Confidentiality in Test Automation: A Case Study in LLM Driven Test Data Generation,"['Hrishikesh Karmarkar', 'Supriya Agrawal', 'Avriti Chauhan', 'Pranav Shete']",,"In out sourced industrial projects for testing of web applications, often neither the application to be tested, nor its source code are provided to the testing team, due to confidentiality reasons, making systematic testing of these applications very challenging. However, textual descriptions of such systems are often available. So, one can consider leveraging a Large Language Model (LLM) to parse these descriptions and synthesize test generators (programs that produce test data). In our experience, LLM synthesized test generators suffer from two problems:- (1) unsound: the generators might produce invalid data and (2) incomplete: the generators typically fail to generate all expected valid inputs. To mitigate these problems, we introduce TestRefineGen a method for autonomously generating test data from textual descriptions. TestRe-fineGen begins by invoking an LLM to parse a given corpus of documents and produce multiple test gener-ators. It then uses a novel ranking approach to identify generators that can produce invalid test data, and then automatically repairs them using a counterexample-guided refinement process. Lastly, TestRefineGen per-forms a generalization procedure that offsets synthesis or refinements that leads to incompleteness, to obtain generators that produce more comprehensive valid in-puts. We evaluated the effectiveness of TestRefineGen on a manually curated set of 256 textual descriptions of test data. TestRefineGen synthesized generators that produce valid test data for 66.01 % of the descriptions. Using a combination of post-processing sanitisation and refinement it was able to successfully repair synthesized generators, which improved the success rate to 76.95 %. Further, our statistical analysis on a small subset of synthesized generators shows that TestRefineGen is able to generate test data that is well distributed across the input space. Thus, TestRefineGen can be an effective technique for autonomous test data generation for web testing...",https://doi.org/10.1109/SANER60148.2024.00041,12-15 March 2024
https://ieeexplore.ieee.org/document/10554852/,Domain Knowledge is All You Need: A Field Deployment of LLM-Powered Test Case Generation in FinTech Domain,"['Zhiyi Xue', 'Liangguo Li', 'Senyue Tian', 'Xiaohong Chen', 'Liangyu Chen', 'Min Zhang']",,"Despite the promise of automation, general-purpose Large Language Models (LLMs) face difficulties in generating complete and accurate test cases from informal software requirements, primarily due to challenges in interpreting unstructured text and producing diverse, relevant scenarios. This paper argues that incorporating domain knowledge significantly improves LLM performance in test case generation. We report on the successful deployment of our LLM-powered tool, LLM4Fin, in the FinTech domain, showcasing the crucial role of domain knowledge in addressing the aforementioned challenges. We demonstrate two methods for integrating domain knowledge: implicit incorporation through model fine-tuning, and explicit incorporation with algorithm design. This combined approach delivers remarkable results, achieving up to 98.18% improvement in test scenario coverage and reducing generation time from 20 minutes to 7 seconds.",https://doi.org/10.1145/3639478.3643087,14-20 April 2024
https://ieeexplore.ieee.org/document/10844968/,Quality Assurance for LLM-Generated Test Cases: A Systematic Literature Review,"['Hasali Edirisinghe', 'Dilani Wickramaarachchi']",,"The rapid advancements in artificial intelligence have transformed software testing, with Large Language Models (LLMs) emerging as powerful tools for automating test case generation. This paper explores Quality Assurance (QA) for LLM-generated test cases in black-box testing through a systematic literature review. Though LLMs are increasingly used for test case generation, challenges in ensuring their quality remain. Following PRISMA guidelines, relevant studies were selected from databases focusing on critical quality attributes, QA frameworks, metrics, and challenges. LLMs demonstrate high efficiency but face numerous issues. A recommendation for future research is given on addressing standardized metrics and improving human-AI collaboration for enhanced testing outcomes.",https://doi.org/10.1109/SLAAI-ICAI63667.2024.10844968,18-19 December 2024
https://ieeexplore.ieee.org/document/10818233/,Automated Structural Test Case Generation for Human-Computer Interaction Software Based on Large Language Model,"['Long Kang', 'Jun Ai', 'Minyan Lu']",,"As software systems expand in complexity, managing the vast and varied collection of test cases becomes increasingly difficult with traditional manual testing methods. This paper presents a new approach for automating the generation of structured test cases, named Test Element Extraction and Restructuring (TEER), which leverages the advanced natural language processing capabilities of large language models (LLMs). Specifically targeting human-computer interaction (HCI) software, TEER employs prompt tuning techniques to extract critical elements from natural language test cases and systematically reassemble them into structured formats. The study evaluates the effectiveness of TEER by applying it to common test cases from desktop HCI applications. The experimental results demonstrate that this method successfully produces structured test cases that meet predefined requirements.",https://doi.org/10.1109/DSA63982.2024.00027,02-03 November 2024
https://ieeexplore.ieee.org/document/10930297/,LLM-Based Video Analytics Test Scenario Generation in Smart Cities,"['Merve Yilmazer', 'Mehmet Karakose']",,"Rapid advances in the field of artificial intelligence have made significant contributions to the automation of software development and testing stages. Software created for use in various fields is tested with test scenarios created manually by software test experts or using test automation. Testing large-scale software with these methods complicates the testing phases because it requires increased human intervention and includes complex applications. In this study, an LLM-based scenario generation framework enhanced with prompt engineering is proposed for testing software to be used for video analysis in smart cities and smart campus areas. Thus, software test scenarios are created by strengthening large language models that are fast, flexible and have high learning ability using prompt engineering techniques. Test scenarios produced through LLM reinforced with prompt engineering techniques were evaluated with rarity and reality metrics and it was determined that more robust scenarios were produced compared to randomly generated test scenarios in the relevant field.",https://doi.org/10.1109/IT64745.2025.10930297,19-22 February 2025
https://ieeexplore.ieee.org/document/11029423/,LLM-AQuA-DiVeR: LLM-Assisted Quality Assurance Through Dialogues on Verifiable Specification with Requirement Owners,"['Shohei Mitani', 'Salonee Moona', ""Shin'ichiro Matsuo"", 'Eric Burger']",,"Quality Assurance (QA) is important for verifying software compliance with stakeholder requirements. QA faces a fundamental challenge of requirement interpretation ambiguity, which can result in insufficient software verification and failure in achieving the stakeholders' intended quality. The interpre-tation challenge intensifies in software development driven by Large Language Models (LLMs), where over-reliance can lead to missed quality-critical alternatives. However, existing works have paid limited attention to stakeholder involvement. We propose an LLM-assisted QA framework extending conventional LLM-driven development to enable stakeholder engagement in software verification. Our framework employs formal methods and rigorous testing to meet diverse quality demands, though this comprehensive verification introduces technical complexity affecting stakeholder engagement and verification costs. Our framework addresses these challenges through two key LLM roles: 1) an explanation assistant for stakeholder understanding, 2) a refinement assistant for incorporating stakeholder feedback while maintaining feasible verification costs. Our initial evaluation empirically demonstrates the framework's effectiveness through participant assessment scores, showing improved quality risk comprehension and efficient feedback incorporation in the verification process.",https://doi.org/10.1109/RAIE66699.2025.00008,29-29 April 2025
https://ieeexplore.ieee.org/document/10988960/,Poster: Unit Testing Past vs. Present: Examining LLMs' Impact on Defect Detection and Efficiency,"['Rudolf Ramler', 'Philipp Straubinger', 'Reinhold Plösch', 'Dietmar Winkler']",,"The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into software engineering workflows has shown potential to enhance productivity, particularly in software testing. This paper investigates whether LLM support improves defect detection effectiveness during unit testing. Building on prior studies comparing manual and tool-supported testing, we replicated and extended an experiment where participants wrote unit tests for a Java-based system with seeded defects within a time-boxed session, supported by LLMs. Comparing LLM supported and manual testing, results show that LLM support significantly increases the number of unit tests generated, defect detection rates, and overall testing efficiency. These findings highlight the potential of LLMs to improve testing and defect detection outcomes, providing empirical insights into their practical application in software testing.",https://doi.org/10.1109/ICST62969.2025.10988960,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10795004/,Take Loads Off Your Developers: Automated User Story Generation using Large Language Model,"['Tajmilur Rahman', 'Yuecai Zhu', 'Lamyea Maha', 'Chanchal Roy', 'Banani Roy', 'Kevin Schneider']",,"Software Maintenance and Evolution (SME) is moving fast with the assistance of artificial intelligence (AI), especially Large Language Models (LLM). Researchers have already started automating various activities of the SME workflow. Un-derstanding the requirements for maintenance and development work i.e. Requirements Engineering (RE) is a crucial phase that kicks off the SME workflow through multiple discussions on a proposed scope of work documented in different forms. The RE phase ends with a list of user stories for each unit task and usually created and tracked on a project management tool such as GitHub, Jira, AzurDev, etc. In this research, we collaborated with Bell Mobility to develop a tool “Geneus” (Generate UserSory) using GPT-4-turbo to automatically create user stories from software requirements documents. Requirements documents are usually long and contain complex information. Since LLMs typically suffer from hallucination when the input is too complex, this paper proposes a new prompting strategy, “Refine and Thought” (RaT), to mitigate that issue and improve the performance of the LLM in prompts with large and noisy contexts. Along with manual evaluation using RUST (Readability, Understandability, Specificity, Technical-aspects) survey questionnaire, automatic evaluation with BERTScore, and AlignScore evaluation metrics are used to evaluate the results of the “Geneus” tool. Results show that our method with RaT performs consistently better in most of the cases of interactions compared to the single-shot baseline method. However, the BERTScore and AlignScore test results are not consistent. In the median case, Geneus performs significantly better in all three interactions (requirements specifi-cation, user story details, and test case specifications) according to AlignScorebut it shows slightly low performance in requirements specifications according to BERTScore. Distilling RE documents requires significant time & effort from the senior members of th...",https://doi.org/10.1109/ICSME58944.2024.00082,06-11 October 2024
https://ieeexplore.ieee.org/document/11029762/,Test Intention Guided LLM-Based Unit Test Generation,"['Zifan Nan', 'Zhaoqiang Guo', 'Kui Liu', 'Xin Xia']",,"The emergence of Large Language Models (LLMs) has accelerated the progress of intelligent software engineering technologies, which brings promising possibilities for unit test generation. However, existing approaches for unit tests directly generated from Large Language Models (LLMs) often prove impractical due to their low coverage and insufficient mocking capabilities. This paper proposes IntUT, a novel approach that utilizes explicit test intentions (e.g., test inputs, mock behaviors, and expected results) to effectively guide the LLM to generate high-quality test cases. Our experimental results on three industry Java projects and live study demonstrate that prompting LLM with test intention can generate high-quality test cases for developers. Specifically, it achieves the improvements on branch coverage by 94 % and line coverage by 49 %. Finally, we obtain developers' feedback on using IntUT to generate cases for three new Java projects, achieving over 80 % line coverage and 30 % efficiency improvement on writing unit test cases.",https://doi.org/10.1109/ICSE55347.2025.00243,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10554970/,Understandable Test Generation Through Capture/Replay and LLMs,['Amirhossein Deljouyi'],https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10554970,"Automatic unit test generators, particularly search-based software testing (SBST) tools such as EvoSuite, efficiently generate unit test suites with acceptable coverage. Although this removes the burden of writing unit tests from developers, these generated tests often pose challenges in terms of comprehension for developers. In my doctoral research, I aim to investigate strategies to address the issue of comprehensibility in generated test cases and improve the test suite in terms of effectiveness. To achieve this, I introduce four projects leveraging Capture/Replay and Large Language Model (LLM) techniques. Capture/Replay carves information from End-to-End (E2E) tests, enabling the generation of unit tests containing meaningful test scenarios and actual test data. Moreover, the growing capabilities of large language models (LLMs) in language analysis and transformation play a significant role in improving readability in general. Our proposed approach involves leveraging E2E test scenario extraction alongside an LLM-guided approach to enhance test case understandability, augment coverage, and establish comprehensive mock and test oracles. In this research, we endeavor to conduct both a quantitative analysis and a user evaluation of the quality of the generated tests in terms of executability, coverage, and understandability.",https://doi.org/10.1145/3639478.3639789,14-20 April 2024
https://ieeexplore.ieee.org/document/10962454/,A System for Automated Unit Test Generation using Large Language Models and Assessment of Generated Test Suites,"['Andrea Lops', 'Fedelucio Narducci', 'Azzurra Ragone', 'Michelantonio Trizio', 'Claudio Bartolini']",,"Unit tests are fundamental for ensuring software correctness but are costly and time-intensive to design and create. Recent advances in Large Language Models (LLMs) have shown potential for automating test generation, though existing evaluations often focus on simple scenarios and lack scalability for real-world applications. To address these limitations, we present AgoneTest, an automated system for generating and assessing complex, class-level test suites for Java projects. Leveraging the Methods2Test dataset, we developed Classes2Test, a new dataset enabling the evaluation of LLM-generated tests against human-written tests. Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.",https://doi.org/10.1109/ICSTW64639.2025.10962454,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10771447/,Enhancing Black-box Compiler Option Fuzzing with LLM through Command Feedback,"['Taiyan Wang', 'Ruipeng Wang', 'Yu Chen', 'Lu Yu', 'Zulie Pan', 'Min Zhang']",,"Since the compiler acts as a core component in software building, it is essential to ensure its availability and reliability through software testing and security analysis. Most research has focused on compiler robustness when compiling various test cases, while the reliability of compiler options lacks attention, especially since each option can activate a specific compiler function. Although some researchers have made efforts in testing it, the insufficient utilization of compiler command feedback messages leads to the poor efficiency, which hinders more diverse and in-depth testing.In this paper, we propose a novel solution to enhance black-box compiler option fuzzing by utilizing command feedback, such as error messages, standard output and compiled files, to guide the error fixing and option pruning via prompting large language models for suggestions. We have implemented the prototype and evaluated it on 4 versions of LLVM. Experiments show that our method significantly improves the detection of crashes, reduces false negatives, and even increase the success rate of compilation when compared to the baseline. To date, our method has identified hundreds of unique bugs, and 9 of them are previously unknown. Among these, 8 have been assigned CVE numbers, and 1 has been fixed following our report.",https://doi.org/10.1109/ISSRE62328.2024.00039,28-31 October 2024
https://ieeexplore.ieee.org/document/10196883/,Automated Metamorphic-Relation Generation with ChatGPT: An Experience Report,"['Yifan Zhang', 'Dave Towey', 'Matthew Pike']",,"This paper reports on a pilot study of using ChatGPT, a language model based on GPT-3.5 architecture, for automatic generation of metamorphic relations (MRs), in the context of testing of autonomous driving systems (ADSs). The oracle problem is a major challenge in testing such systems, where it is difficult to determine whether or not the output of a system is correct. Metamorphic testing (MT) can alleviate this problem by checking the consistency of the system’s outputs under various transformations. However, manual generation of MRs is often a time-consuming and error-prone process. Automated MR generation can yield several benefits, including enhanced efficiency, quality, coverage, scalability, and reusability in software testing, thereby facilitating a more comprehensive and effective testing process. In this paper, we investigate the effectiveness of using ChatGPT for automatic generation of MRs for ADSs. We provide a detailed methodology for generating MRs using ChatGPT and evaluate the generated MRs using our domain knowledge and existing MRs. The results of our study indicate that our proposed approach is effective at generating high-quality MRs, and can significantly reduce the manual effort required for MR generation. Furthermore, we discuss the practical implications and limitations of using ChatGPT for MR generation and provide recommendations for future research. Our study contributes to the advancement of automated testing of ADSs, which is crucial for ensuring their safety and reliability in real-world scenarios.",https://doi.org/10.1109/COMPSAC57700.2023.00275,26-30 June 2023
https://ieeexplore.ieee.org/document/10962520/,Evaluating Large Language Model Robustness using Combinatorial Testing,"['Jaganmohan Chandrasekaran', 'Ankita Ramjibhai Patel', 'Erin Lanus', 'Laura J. Freeman']",,"Recent advancements in large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like text, leading to widespread adoption across domains. Given LLM’s versatile capabilities, current evaluation practices assess LLMs across a wide variety of tasks, including answer generation, sentiment analysis, text completion, and question and answers, to name a few. Multiple choice questions (MCQ) have emerged as a widely used evaluation task to assess LLM’s understanding and reasoning across various subject areas. However, studies from the literature have revealed that LLMs exhibit sensitivity to the ordering of options in MCQ tasks, with performance variations based on option sequence, thus underscoring the robustness concerns in LLM performance.This work presents a combinatorial testing-based framework for systematic and comprehensive robustness assessment of pre-trained LLMs. By leveraging the sequence covering array, the framework constructs test sets by systematically swapping the order of options, which are then used in ascertaining the robustness of LLMs. We performed an experimental evaluation using the Measuring Massive Multitask Language Understanding (MMLU) dataset, a widely used MCQ dataset and evaluated the robustness of GPT 3.5 Turbo, a pre-trained LLM. Results suggest the framework can effectively identify numerous robustness issues with a relatively minimal number of tests.",https://doi.org/10.1109/ICSTW64639.2025.10962520,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10942340/,"LLM-Driven, Self-Improving Framework for Security Test Automation: Leveraging Karate DSL for Augmented API Resilience","['Emil Marian Pasca', 'Daniela Delinschi', 'Rudolf Erdei', 'Oliviu Matei']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10942340,"Modern software architectures heavily rely on APIs, yet face significant security challenges, particularly with Broken Object Level Authorization (BOLA) vulnerabilities, which remain the most critical API security risk according to OWASP. This paper introduces Karate-BOLA-Guard, an innovative framework leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques to automate security-focused test case generation for APIs. Our approach integrates vector databases for context retrieval, multiple LLM models for test generation, and observability tools for process monitoring. Initial experiments were carried out on three deliberately vulnerable APIs (VAmPI, Crapi, and OWASP Juice Shop), with subsequent validation on fifteen additional production APIs spanning diverse domains including social media, version control systems, financial services, and transportation services. Our evaluation metrics show Llama 3 8B achieving consistent performance (Accuracy: 3.1-3.4, Interoperability: 3.7-4.3) with an average processing time of 143.76 seconds on GPU. Performance analysis revealed significant GPU acceleration benefits, with 20-25x improvement over CPU processing times. Smaller models demonstrated efficient processing, with Phi-3 Mini averaging 69.58 seconds and Mistral 72.14 seconds, while maintaining acceptable accuracy scores. Token utilization patterns showed Llama 3 8B using an average of 36,591 tokens per session, compared to Mistral’s 25,225 and Phi-3 Mini’s 31,007. Our framework’s effectiveness varied across APIs, with notably strong performance in complex platforms (Instagram: A = 4.3, I = 4.4) while maintaining consistent functionality in simpler implementations (VAmPI: A = 3.6, I = 4.3). The iterative refinement process, evaluated through comprehensive metrics including Accuracy (A), Complexity (C), and Interoperability (I), represents a significant advancement in automated API security testing, offering an efficient, accurate, and adapt...",https://doi.org/10.1109/ACCESS.2025.3554960,
https://ieeexplore.ieee.org/document/10764966/,Magneto: A Step-Wise Approach to Exploit Vulnerabilities in Dependent Libraries via LLM-Empowered Directed Fuzzing,"['Zhuotong Zhou', 'Yongzhuo Yang', 'Susheng Wu', 'Yiheng Huang', 'Bihuan Chen', 'Xin Peng']",,"The wide adoption of open source third-party libraries can propagate vulnerabilities that originally exist in third-party libraries through dependency chains to downstream projects. To mitigate this security risk, vulnerability exploitation analysis has been proposed to further reduce false positives of vulnerability reachability analysis. However, existing approaches work less effectively when the vulnerable function of the vulnerable library is indirectly invoked by a client project through a call chain of multiple steps.To address this problem, we propose a step-wise approach, named Magneto, to exploit vulnerabilities in dependent libraries of a client project through LLM-empowered directed fuzzing. Its core idea is to decompose the directed fuzzing for the whole call chain (from the client project to the vulnerable function) into a series of stepwise directed fuzzing for each step of the call chain. To empower directed fuzzing, it leverages LLM to facilitate the initial seed generation. Our evaluation has demonstrated the effectiveness of Magneto over the state-of-the-art; i.e., Magneto achieves an improvement of at least 75.6% in successfully exploiting the vulnerability.CCS CONCEPTS • Security and privacy → Vulnerability management.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10599579/,MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation,"['Guanyu Wang', 'Yuekang Li', 'Yi Liu', 'Gelei Deng', 'Tianlin Li', 'Guosheng Xu']",,"Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation. This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM -augmented generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissim-ilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world matching scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51% on our tests compared to the original datasets, em-phasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM -augmented applications.",https://doi.org/10.1145/3650105.3652297,14-14 April 2024
https://ieeexplore.ieee.org/document/10973230/,Action-State Testing—A Model for Test Design Automation,"['István Forgács', 'Attila Kovács']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10973230,"Model-based testing (MBT) is essential in software testing, offering automation, comprehensive coverage, and defect prevention. It uses abstract models to automatically design and generate test cases, representing the expected system behaviour, including states, transitions, inputs, and outputs. This paper explores the action-state testing modelling technique, originally introduced by the authors in Forgács and Kovács (2000). In this approach, a model step comprises an action (input), one or more responses (outputs), and an optional state. The steps can be arranged sequentially, or they may be forked and joined. Sequential steps appear within the same test case. Forked steps are distributed across different test cases. The joined steps also belong to separate test cases. In addition, the graphical model can be constructed using a text editor. This paper builds upon the concept by establishing its theoretical foundation. We demonstrate how the action-state model eliminates the need for guard conditions and coding, maintains a concise and manageable structure, and seamlessly incorporates outputs, ultimately enhancing testing efficiency. Additionally, we provide guidelines for adding new states and empirically validate the benefits of action-state testing over alternative techniques, achieving a 100% defect detection percentage (DDP). This paper marks the first instalment of the author’s Test Design Trilogy, dedicated to refining and unifying various test design techniques.",https://doi.org/10.1109/ACCESS.2025.3563337,
https://ieeexplore.ieee.org/document/11028238/,Exploring Large Language Models for Requirements on String Values,"['Aren A. Babikian', 'Boqi Chen', 'Gunter Mussbacher']",,"Behavior-driven development (BDD) enables collaboration among different stakeholders by employing a natural-language representation of system requirements and of test scenarios. These scenarios often involve constraints over string values, e.g. for the validity of email addresses, which are challenging to test comprehensively. Traditional methods like SMT solvers (e.g. Z3, Ostrich) handle constraints efficiently but produce unrealistic strings and require formal specifications that are often unavailable and expensive to compute. This paper explores the potential of large language models (LLMs) in generating realistic, constraint-satisfying strings for BDD. We propose an evaluation framework to assess LLMs’ ability to (1) generate consistent string values and (2) detect constraint inconsistencies. In our experiments, three LLMs are compared to state-of-the-art solvers using constraints from a software engineering course project. Results show that while solvers dominate in precision and recall, LLMs derive realistic strings more suitable for a requirements engineering context. With these trade-offs, we believe that, when formal constraints are available, a combined LLM-solver approach could offer a more effective solution.",https://doi.org/10.1109/MO2RE66661.2025.00009,27-27 April 2025
https://ieeexplore.ieee.org/document/10784391/,Test Case Migration from Monolith to Microservices Using Large Language Models,"['Hang-Wei Yeh', 'Shang-Pin Ma', 'Yi Chen']",,"Due to microservices' modularity, high scalability, and good fault tolerance, more and more software systems are transitioning from monolithic architectures to microservice architectures. In this migration process, the migration of test cases is a crucial step to ensure functional consistency and completeness before and after the migration, thereby maintaining the stability and reliability of the microservice system post-migration. However, despite numerous studies on architecture migration, there is still a lack of methods to efficiently convert test cases of monolithic systems into ones for the migrated microservices. Therefore, this study proposes a test migration approach based on Large Language Models (LLM), called LTM^3 (LLM-based Test Migration from Monolith to Microservices). During migration, LTM^3 identifies the correspondence between existing test cases and monolithic functions, the connections between monolithic functions and migrated microservices, and the dependencies between microservices. Subsequently, by utilizing LLM and appropriate prompting, the integration test cases of the monolithic system are transformed into contract test cases for each microservice. The experimental results showed that LTM^3 can effectively migrate most test cases, with only a tiny portion requiring manual adjustment.",https://doi.org/10.1109/ICEBE62490.2024.00014,11-13 October 2024
https://ieeexplore.ieee.org/document/11021674/,Evaluating Large Language Models Via Multi-Modal User Knowledge Graphs: A Comprehensive Assessment Framework,"['Pan Liu', 'Zizhao Chen', 'Yihao Li', 'W. Eric Wong']",,"Large language models (LLMs) have been widely adopted across various industries, but issues such as hallucinations, biases, and erroneous outputs frequently arise, compromising their reliability and safety. Objectively assessing how well an LLM interprets user queries is crucial for selecting the right model for practical problem-solving. This paper proposes an evaluation method that leverages user knowledge graphs to measure an LLM's comprehension of user input. Specifically, we construct both text-based and graphical test cases derived from a user knowledge graph, thereby enabling multi-modal assessment of the LLM's understanding. To implement our approach, a knowledge graph is first built from the user's input and then mutated to produce diverse test cases. A search-based testing method is then applied to evaluate the model's comprehension. We provide a case study demonstrating the framework. Our findings indicate that multi-modal test cases outperform purely text-based test cases in revealing the true understanding capability of LLMs. Among the eight models tested, DeepSeek and Doubao exhibit stronger comprehension than the remaining six.",https://doi.org/10.1109/ISSSR65654.2025.00047,12-13 April 2025
https://ieeexplore.ieee.org/document/11025920/,Using Large Language Models to Generate Concise and Understandable Test Case Summaries,"['Natanael Djajadi', 'Amirhossein Deljouyi', 'Andy Zaidman']",,"Software testing is essential, and automatic test case generation can be an important aid to software engineers. However, generated tests are sometimes difficult to understand. Test summarization approaches that provide an overview of what exactly is tested can provide help, but existing summarization approaches generate documentation that is lengthy and redundant. In this paper, we investigate whether large language models (LLMs) can be used to generate more concise, yet understandable summaries. In a small-scale user study with 11 participants, we obtained positive feedback on the LLM-generated summaries.",https://doi.org/10.1109/ICPC66645.2025.00040,27-28 April 2025
https://ieeexplore.ieee.org/document/11030993/,Enhancing Flutter App Development: Addressing Configuration and Compatibility Bugs Using Large Language Models,"['Supun Rajaguru', 'Lohara Chathumini', 'Samantha Kumara', 'Ashansa Wijerathna']",,"In today’s world, there are cross-platform frameworks are widely available, The frameworks like Flutter, allow the developer to develop an app from a single codebase. But there are some major issues like debugging and compatibility finding in Flutter framework. These issues lead to prolonged debugging and lesser app reliability. The proposed model will lead to overcome those mentioned challenges. The proposed model used advanced LLMs including GPT-4o, Claude Sonnet 3.5, and Gemini 2.0 Flash, combined with RAG capabilities. The data was collected from multiple sources like Stack Overflow, GitHub, and Flutter documentation. The dataset was cleaned and preprocessed by removing low-scoring answers, filtering incomplete questions, and applying text sanitization techniques to ensure structured and relevant inputs for analysis. A fixed set of prompt templates to 350 Stack Overflow questions across all the LLMs were applied to commence the evaluation process. The 25 most important questions were identified through verification via cosine similarity and precision correction between its responses. It was able to determine the two models’ efficacy in finding configuration bugs and addressing those bugs. The current work involves integrating an RAG pipeline and using a vector database for better retrieval and response generation. Early evidence hints at the enhanced accuracy/precision of an RAG-enhanced LLM when that LLM is a standalone model for better Flutter configuration. In conclusion this proposed model presents a modular framework to integrate RAG-enhanced LLMs for bug detection and fixing. The framework requires much lesser debugging efforts along with more reliability of the app. Additionally, it can also adopt the fast-evolving Flutter framework. By filling a significant gap in cross-platform development literature, it helps advance AI-assisted debugging and improve the development workflows of Flutter apps.",https://doi.org/10.1109/SCSE65633.2025.11030993,03-03 April 2025
https://ieeexplore.ieee.org/document/11028452/,Do Code LLMs Understand Design Patterns?,"['Zhenyu Pan', 'Xuefeng Song', 'Yunkun Wang', 'Rongyu Cao', 'Binhua Li', 'Yongbin Li']",,"Code Large Language Models (LLMs) demonstrate great versatility in adapting to various downstream tasks, including code generation and completion, as well as bug detection and fixing. However, Code LLMs often fail to capture existing coding standards, leading to the generation of code that conflicts with the required design patterns for a given project. As a result, developers must post-process to adapt the generated code to the project’s design norms. In this work, we empirically investigate the biases of Code LLMs in software development. Through carefully designed experiments, we assess the models’ understanding of design patterns across recognition, comprehension, and generation. Our findings reveal that biases in Code LLMs significantly affect the reliability of downstream tasks.",https://doi.org/10.1109/LLM4Code66737.2025.00031,03-03 May 2025
https://ieeexplore.ieee.org/document/10825785/,RAGFix: Enhancing LLM Code Repair Using RAG and Stack Overflow Posts,"['Elijah Mansur', 'Johnson Chen', 'Muhammad Anas Raza', 'Mohammad Wardat']",,"Identifying, localizing, and resolving bugs in software engineering is challenging and costly. Approaches to resolve software bugs range from Large Language Model (LLM) code analysis and repair, and automated code repair technology that aims to alleviate the technical burden of difficult to solve bugs. We propose RAGFix, which enhances LLM’s capabilities for bug localization and code repair using Retrieval Augmented Generation (RAG) based on dynamically collected Stack Overflow posts. These posts are searchable via a Question and Answer Knowledge Graph (KGQA). We evaluate our method on the HumanEvalFix benchmark for Python using relevant closed and open-source models. Our approach facilitates error resolution in Python coding problems by creating a searchable, embedded knowledge graph representation of bug and solution information from Stack Overflow, interlinking bugs, and solutions through semi-supervised graph construction methods. We use cosine similarity on embeddings based on LLM-synthesized summaries and algorithmic features describing the coding problem and potential solution to find relevant results that improve LLM in-context performance. Our results indicate that our system enhances small open-source models’ ability to effectively repair code, particularly where these models have less parametric knowledge about relevant coding problems and can leverage nonparametric knowledge to provide accurate, actionable fixes.",https://doi.org/10.1109/BigData62323.2024.10825785,15-18 December 2024
https://ieeexplore.ieee.org/document/11024256/,CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge Graph,"['Hanxiang Xu', 'Wei Ma', 'Ting Zhou', 'Yanjie Zhao', 'Kai Chen', 'Qiang Hu']",,"In recent years, the programming capabilities of large language models (LLMs) have garnered significant attention. Fuzz testing, a highly effective technique, plays a key role in enhancing software reliability and detecting vulnerabilities. However, traditional fuzz testing tools rely on manually crafted fuzz drivers, which can limit both testing efficiency and effectiveness. To address this challenge, we propose an automated fuzz testing method driven by a code knowledge graph and powered by an LLM-based intelligent agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a code generation task, leveraging the knowledge graph of the code repository to automate the generation process within the fuzzing loop, while continuously refining both the fuzz driver and input seeds. The code knowledge graph is constructed through interprocedural program analysis, where each node in the graph represents a code entity, such as a function or a file. The knowledge graph-enhanced CKGFuzzer not only effectively resolves compilation errors in fuzz drivers and generates input seeds tailored to specific API usage scenarios, but also analyzes fuzz driver crash reports, assisting developers in improving code quality. By querying the knowledge graph of the code repository and learning from API usage scenarios, we can better identify testing targets and understand the specific purpose of each fuzz driver. We evaluated our approach using eight open-source software projects. The experimental results indicate that CKGFuzzer achieved an average improvement of 8.73% in code coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer reduced the manual review workload in crash case analysis by 84.4% and successfully detected 11 real bugs (including nine previously unreported bugs) across the tested libraries. Our research enhances the overall performance of fuzz testing by refining fuzz driver generation strategies and input seed analysis, offering a more effect...",https://doi.org/10.1109/ICSE-Companion66252.2025.00079,27 April 2025 - 03 May 2025
https://ieeexplore.ieee.org/document/10765042/,Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency,"['Yichi Zhang', 'Zixi Liu', 'Yang Feng', 'Baowen Xu']",,"Rust is renowned for its robust memory safety capabilities, yet its distinctive memory management model poses substantial challenges in both writing and understanding programs. Within Rust source code, comments are employed to clearly delineate conditions that might cause panic behavior, thereby warning developers about potential hazards associated with specific operations. Therefore, comments are particularly crucial for documenting Rust’s program logic and design. Nevertheless, as modern software frequently undergoes updates and modifications, maintaining the accuracy and relevance of these comments becomes a labor-intensive endeavor.In this paper, inspired by the remarkable capabilities of Large Language Models (LLMs) in understanding software programs, we propose a code-comment inconsistency detection tool, namely RustC4, that combines program analysis and LLM-driven techniques to identify inconsistencies in code comments. RustC4 leverages LLMs’ ability to interpret natural language descriptions within code comments, facilitating the extraction of design constraints. Program analysis techniques are then employed to accurately verify the implementation of these constraints. To evaluate the effectiveness of RustC4, we construct a dataset from 12 large-scale real-world Rust projects. The experiment results demonstrate that RustC4 is effective in detecting 176 real inconsistent cases and 23 of them have been confirmed and fixed by developers by the time this paper was submitted.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11029833/,Boosting Static Resource Leak Detection via LLM-based Resource-Oriented Intention Inference,"['Chong Wang', 'Jianan Liu', 'Xin Peng', 'Yang Liu', 'Yiling Lou']",,"Resource leaks, caused by resources not being released after acquisition, often lead to performance issues and system crashes. Existing static detection techniques rely on mechanical matching of predefined resource acquisition/release APIs and null-checking conditions to find unreleased resources, suffering from both (1) false negatives caused by the incompleteness of predefined resource acquisition/release APIs and (2) false positives caused by the incompleteness of resource reachability validation identification. To overcome these challenges, we propose InferROI, a novel approach that leverages the exceptional code comprehension capability of large language models (LLMs) to directly infer resource-oriented intentions (acquisition, release, and reachability validation) in code. InferROI first prompts the LLM to infer involved intentions for a given code snippet, and then incorporates a two-stage static analysis approach to check control-flow paths for resource leak detection based on the inferred intentions. We evaluate the effectiveness of InferROI in both resource-oriented intention inference and resource leak detection. Experimental results on the DroidLeaks and JLeaks datasets demonstrate InferROI achieves promising bug detection rate (59.3% and 62.5%) and false alarm rate (18.6% and 19.5%). Compared to three industrial static detectors, InferROI detects 14~45 and 149~485 more bugs in DroidLeaks and JLeaks, respectively. When applied to real-world open-source projects, InferROI identifies 29 unknown resource leak bugs (verified by authors), with 7 of them being confirmed by developers. In addition, the results of an ablation study underscores the importance of combining LLM-based inference with static analysis. Finally, manual annotation indicated that InferROI achieved a precision of 74.6% and a recall of 81.8% in intention inference, covering more than 60% resource types involved in the datasets.",https://doi.org/10.1109/ICSE55347.2025.00131,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/11052804/,Testing Refactoring Engine via Historical Bug Report driven LLM,"['Haibo Wang', 'Zhuolin Xu', 'Shin Hwei Tan']",,"Refactoring is the process of restructuring existing code without changing its external behavior while improving its internal structure. Refactoring engines are integral components of modern Integrated Development Environments (IDEs) and can automate or semi-automate this process to enhance code readability, reduce complexity, and improve the maintainability of software products. Similar to traditional software systems such as compilers, refactoring engines may also contain bugs that can lead to unexpected behaviors. In this paper, we propose a novel approach called RETester, a LLM-based framework for automated refactoring engine testing. Specifically, by using input program structure templates extracted from historical bug reports and input program characteristics that are error-prone, we design chain-of-thought (CoT) prompts to perform refactoring-preserving transformations. The generated variants are then tested on the latest version of refactoring engines using differential testing. We evaluate RETester on two most popular modern refactoring engines (i.e., Eclipse, and IntelliJ IDEA). It successfully revealed 18 previously unknown bugs in the latest version of those refactoring engines, seven of them have been confirmed by their developers, and three have been fixed.",https://doi.org/10.1109/Forge66646.2025.00020,27-28 April 2025
https://ieeexplore.ieee.org/document/11029791/,The Seeds of the Future Sprout from History: Fuzzing for Unveiling Vulnerabilities in Prospective Deep-Learning Libraries,"['Zhiyuan Li', 'Jingzheng Wu', 'Xiang Ling', 'Tianyue Luo', 'Zhiqing Rui', 'Yanjun Wu']",,"The widespread application of large language models (LLMs) underscores the importance of deep learning (DL) technologies that rely on foundational DL libraries such as PyTorch and TensorFlow. Despite their robust features, these libraries face challenges with scalability and adaptation to rapid advancements in the LLM community. In response, tech giants like Apple and Huawei are developing their own DL libraries to enhance performance, increase scalability, and safeguard intellectual property. Ensuring the security of these libraries is crucial, with fuzzing being a vital solution. However, existing fuzzing frameworks struggle with target flexibility, effectively testing bug-prone API sequences, and leveraging the limited available information in new libraries. To address these limitations, we propose FUTURE, the first universal fuzzing framework tailored for newly introduced and prospective DL libraries. FUTURE leverages historical bug information from existing libraries and fine-tunes LLMs for specialized code generation. This strategy helps identify bugs in new libraries and uses insights from these libraries to enhance security in existing ones, creating a cycle from history to future and back. To evaluate FUTURE's effectiveness, we conduct comprehensive evaluations on three newly introduced DL libraries. Evaluation results demonstrate that FUTURE significantly outperforms existing fuzzers in bug detection, success rate of bug reproduction, validity rate of code generation, and API coverage. Notably, FUTURE has detected 148 bugs across 452 targeted APIs, including 142 previously unknown bugs. Among these, 10 have been assigned CVE IDs. Additionally, FUTURE detects 7 bugs in PyTorch, demonstrating its ability to enhance security in existing libraries in reverse.",https://doi.org/10.1109/ICSE55347.2025.00132,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/11028362/,Are Large Language Models Memorizing Bug Benchmarks?,"['Daniel Ramos', 'Claudia Mamede', 'Kush Jain', 'Paulo Canelas', 'Catarina Gamboa', 'Claire Le Goues']",,"Large Language Models (LLMs) have become integral to various software engineering tasks, including code generation, bug detection, and repair. To evaluate model performance in these domains, numerous bug benchmarks containing real-world bugs from software projects have been developed. However, a growing concern within the software engineering community is that these benchmarks may not reliably reflect true LLM performance due to the risk of data leakage. Despite this concern, limited research has been conducted to quantify the impact of potential leakage.In this paper, we systematically evaluate popular LLMs to assess their susceptibility to data leakage from widely used bug benchmarks. To identify potential leakage, we use multiple metrics, including a study of benchmark membership within commonly used training datasets, as well as analyses of negative log-likelihood and 5-gram accuracy. Our findings show that certain models, in particular codegen-multi, exhibit significant evidence of memorization in widely used benchmarks like Defects4J, while newer models trained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage. These results highlight the need for careful benchmark selection and the adoption of robust metrics to adequately assess models capabilities.",https://doi.org/10.1109/LLM4Code66737.2025.00005,03-03 May 2025
https://ieeexplore.ieee.org/document/10764990/,LLMs and Prompting for Unit Test Generation: A Large-Scale Evaluation,"['Wendkûuni C. Ouédraogo', 'Kader Kaboré', 'Yewei Song', 'Jacques Klein', 'Haoye Tian', 'Anil Koyuncu']",,"Unit testing, essential for identifying bugs, is often neglected due to time constraints. Automated test generation tools exist but typically lack readability and require developer intervention. Large Language Models (LLMs) like GPT and Mistral show potential in test generation, but their effectiveness remains unclear.This study evaluates four LLMs and five prompt engineering techniques, analyzing 216 300 tests for 690 Java classes from diverse datasets. We assess correctness, readability, coverage, and bug detection, comparing LLM-generated tests to EvoSuite. While LLMs show promise, improvements in correctness are needed. The study highlights both the strengths and limitations of LLMs, offering insights for future research.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11022958/,BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection,"['Shams Tarek', 'Dipayan Saha', 'Sujan Kumar Saha', 'Farimah Farahmandi']",,"The current landscape of system-on-chips (SoCs) security verification faces challenges due to manual, labor-intensive, and inflexible methodologies. These issues limit the scalability and effectiveness of security protocols, making bug detection at the Register-Transfer Level (RTL) difficult. This paper proposes a new framework named BugWhisperer that utilizes a specialized, fine-tuned Large Language Model (LLM) to address these challenges. By enhancing the LLM’s hardware security knowledge and leveraging its capabilities for text inference and knowledge transfer, this approach automates and improves the adaptability and reusability of the verification process. We introduce an open-source, fine-tuned LLM specifically designed for detecting security vulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM effectively enhances the efficiency and flexibility of the security verification process. Additionally, we introduce a comprehensive hardware vulnerability database that supports this work and will further assist the research community in enhancing the security verification process.",https://doi.org/10.1109/VTS65138.2025.11022958,28-30 April 2025
https://ieeexplore.ieee.org/document/11042761/,Intent-driven Web UI Tests Repair with LLM,"['Yingjie Tao', 'Weiwei Wang', 'Junxia Guo']",,"As web applications are frequently updated, changes may introduce to web elements in new versions, causing test cases to fail. Consequently, automatic web test repair techniques are proposed to reduce the cost of regression testing. Most existing methods focus on finding the correct candidate elements or related attributes to fix the broken test case. However, when test case failures are caused by test flow changes or propagated breakages, those methods that focus solely on matching the failing element in the new version cannot work well. Through empirical analysis, we found that the test intent and the reasons that caused the test failure are useful in test case reparation. This paper proposes a novel intent-driven web test repair approach named LetTe, which first parses the test intent and failure reasons of failed web UI tests, and then guides the Large Language Model (LLM) to fix them via prompt design and fine-tuning. LetTe’s repair logic is to simulate that of a human expert. According to the test intent and reason for failure, “think about” the possible repair plan and then generate repair candidates through the corresponding chain-of-thought. We evaluate LetTe on 7 web applications collected from open-source websites as well as publicly available datasets. The experimental results show that our approach has a 75% correct repair rate, which is higher than all baseline methods.",https://doi.org/10.1109/AEMCSE65292.2025.11042761,09-11 May 2025
https://ieeexplore.ieee.org/document/10556117/,(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs,"['Wanqin Ma', 'Chenyang Yang', 'Christian Kästner']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10556117,"Large Language Models (LLMs) are increasingly integrated into software applications. Downstream application developers often access LLMs through APIs provided as a service. However, LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices, as evidenced by our case study on toxicity detection. Based on our case study, we emphasize the need for and re-examine the concept of regression testing for evolving LLM APIs. We argue that regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.CCS CONCEPTS•Software and its engineering → Software testing and debugging.",,14-15 April 2024
https://ieeexplore.ieee.org/document/10845786/,Addressing Technical Challenges in Large Language Model-Driven Educational Software System,"['Nacha Chondamrongkul', 'Georgi Hristov', 'Punnarumol Temdee']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10845786,"The integration of large language models (LLMs) into educational systems poses significant challenges across several key attributes, including integration, explainability, testability, and scalability. These challenges arise from the complexity of coordinating system components, difficulty interpreting LLM decision-making processes, and the need for reliable, consistent model outputs in varied educational scenarios. Additionally, ensuring scalability requires robust autoscaling mechanisms and suitable architecture design to handle fluctuating workloads. This paper tackles these challenges by proposing tactics to improve system integration, enhance explainability through metadata and an algorithm process, ensure response consistency via regression testing, and facilitate efficient autoscaling through an event-driven microservice architecture. The evaluation results highlight the effectiveness of these tactics, confirming both functional consistency and robust system performance under varying loads.",https://doi.org/10.1109/ACCESS.2025.3531380,
https://ieeexplore.ieee.org/document/10771408/,On Enhancing Root Cause Analysis with SQL Summaries for Failures in Database Workload Replays at SAP HANA,"['Neetha Jambigi', 'Joshua Hammesfahr', 'Moritz Mueller', 'Thomas Bach', 'Michael Felderer']",,"Capturing the workload of a database and replaying this workload for a new version of the database can be an effective approach for regression testing. However, false positive errors caused by many factors such as data privacy limitations, time dependency or non-determinism in multi-threaded environment can negatively impact the effectiveness. Therefore, we employ a machine learning based framework to automate the root cause analysis of failures found during replays. However, handling unseen novel issues not found in the training data is one general challenge of machine learning approaches with respect to generalizability of the learned model. We describe how we continue to address this challenge for more robust long-term solutions. From our experience, retraining with new failures is inadequate due to features overlapping across distinct root causes. Hence, we leverage a large language model (LLM) to analyze failed SQL statements and extract concise failure summaries as an additional feature to enhance the classification process. Our experiments show the F1-Macro score improved by 4.77% for our data. We consider our approach beneficial for providing end users with additional information to gain more insights into the found issues and to improve the assessment of the replay results.",https://doi.org/10.1109/ISSREW63542.2024.00052,28-31 October 2024
https://ieeexplore.ieee.org/document/10366620/,The Causal Reasoning Ability of Open Large Language Model: A Comprehensive and Exemplary Functional Testing,"['Shun-Hang Li', 'Gang Zhou', 'Zhi-Bo Li', 'Ji-Cang Lu', 'Ning-Bo Huang']",,"As the intelligent software, the development and application of large language models are extremely hot topics recently, bringing tremendous changes to general AI and software industry. Nonetheless, large language models, especially open source ones, incontrollably suffer from some potential software quality issues such as instability, inaccuracy, and insecurity, making software testing necessary. In this paper, we propose the first solution for functional testing of open large language models to check full-scene availability and conclude empirical principles for better steering large language models, particularly considering their black box and intelligence properties. Specifically, we focus on the model’s causal reasoning ability, which is the core of artificial intelligence but almost ignored by most previous work. First, for comprehensive evaluation, we deconstruct the causal reasoning capability into five dimensions and summary the forms of causal reasoning task as causality identification and causality matching. Then, rich datasets are introduced and further modified to generate test cases along with different ability dimensions and task forms to improve the testing integrity. Moreover, we explore the ability boundary of open large language models in two usage modes: prompting and lightweight fine-tuning. Our work conducts comprehensive functional testing on the causal reasoning ability of open large language models, establishes benchmarks, and derives empirical insights for practical usage. The proposed testing solution can be transferred to other similar evaluation tasks as a general framework for large language models or their derivations.",https://doi.org/10.1109/QRS60937.2023.00032,22-26 October 2023
https://ieeexplore.ieee.org/document/10638557/,Intent-Driven Mobile GUI Testing with Autonomous Large Language Model Agents,"['Juyeon Yoon', 'Robert Feldt', 'Shin Yoo']",,"GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51 % for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 547 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.",https://doi.org/10.1109/ICST60714.2024.00020,27-31 May 2024
https://ieeexplore.ieee.org/document/11029843/,Automating a Complete Software Test Process Using LLMs: An Automotive Case Study,"['Shuai Wang', 'Yinan Yu', 'Robert Feldt', 'Dhasarathy Parthasarathy']",,"Vehicle API testing verifies whether the interactions between a vehicle's internal systems and external applications meet expectations, ensuring that users can access and control various vehicle functions and data. However, this task is inherently complex, requiring the alignment and coordination of API systems, communication protocols, and even vehicle simulation systems to develop valid test cases. In practical industrial scenarios, inconsistencies, ambiguities, and interde-pendencies across various documents and system specifications pose significant challenges. This paper presents a system designed for the automated testing of in-vehicle APIs. By clearly defining and segmenting the testing process, we enable Large Language Models (LLMs) to focus on specific tasks, ensuring a stable and controlled testing workflow. Experiments conducted on over 100 APIs demonstrate that our system effectively automates vehicle API testing. The results also confirm that LLMs can efficiently handle mundane tasks requiring human judgment, making them suitable for complete automation in similar industrial contexts.",https://doi.org/10.1109/ICSE55347.2025.00211,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10548840/,CrashTranslator: Automatically Reproducing Mobile Application Crashes Directly from Stack Trace,"['Yuchao Huang', 'Junjie Wang', 'Zhe Liu', 'Yawen Wang', 'Song Wang', 'Chunyang Chen']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10548840,"Crash reports are vital for software maintenance since they allow the developers to be informed of the problems encountered in the mobile application. Before fixing, developers need to reproduce the crash, which is an extremely time-consuming and tedious task. Existing studies conducted the automatic crash reproduction with the natural language described reproducing steps. Yet we find a non-neglectable portion of crash reports only contain the stack trace when the crash occurs. Such stack-trace-only crashes merely reveal the last GUI page when the crash occurs, and lack step-by-step guidance. Developers tend to spend more effort in understanding the problem and reproducing the crash, and existing techniques cannot work on this, thus calling for a greater need for automatic support. This paper proposes an approach named CrashTranslator to automatically reproduce mobile application crashes directly from the stack trace. It accomplishes this by leveraging a pre-trained Large Language Model to predict the exploration steps for triggering the crash, and designing a reinforcement learning based technique to mitigate the inaccurate prediction and guide the search holistically. We evaluate CrashTranslator on 75 crash reports involving 58 popular Android apps, and it successfully reproduces 61.3% of the crashes, outperforming the state-of-the-art baselines by 109% to 206%. Besides, the average reproducing time is 68.7 seconds, out-performing the baselines by 302% to 1611%. We also evaluate the usefulness of CrashTranslator with promising results.",https://doi.org/10.1145/3597503.3623298,14-20 April 2024
https://ieeexplore.ieee.org/document/10740182/,RESTLess: Enhancing State-of-the-Art REST API Fuzzing With LLMs in Cloud Service Computing,"['Tao Zheng', 'Jiang Shao', 'Jinqiao Dai', 'Shuyu Jiang', 'Xingshu Chen', 'Changxiang Shen']",,"REST API Fuzzing is an emerging approach for automated vulnerability detection in cloud services. However, existing SOTA fuzzers face challenges in generating lengthy sequences comprising high-semantic requests, so that they may hardly trigger hard-to-reach states within a cloud service. To overcome this problem, we propose RESTLess, a flexible and efficient approach with hybrid optimization strategies for REST API fuzzing enhancement. Specifically, to pass the cloud gateway syntax semantic checking, we construct a dataset of valid parameters of REST API with Large Language Model named RTSet, then utilize it to develop an efficient REST API specification semantic enhancement approach. To detect vulnerability hidden under complex API operations, we design a flexible parameter rendering order optimization algorithm to increase the length and type of request sequences. Evaluation results highlight that RESTLess manifests noteworthy enhancements in the semantic quality of generated sequences in comparison to existing tools, thereby augmenting their capabilities in detecting vulnerabilities effectively. We also apply RESTLess to nine real-world cloud service such as Microsoft Azure, Amazon Web Services, Google Cloud, etc., and detecte 38 vulnerabilities, of which 16 have been confirmed and fixed by the relevant vendors.",https://doi.org/10.1109/TSC.2024.3489441,
https://ieeexplore.ieee.org/document/10391027/,AI-Powered Software Testing: The Impact of Large Language Models on Testing Methodologies,"['Vahit Bayrı', 'Ece Demirel']",,"Software testing is a crucial aspect of the software development lifecycle, ensuring the delivery of high-quality, reliable, and secure software systems. With the advancements in Artificial Intelligence (AI) and Natural Language Processing (NLP), Large Language Models (LLMs) have emerged as powerful tools capable of understanding and processing natural language texts easly. This article investigates the application of AI-based software testing, with a specific focus on the impact of LLMs in traditional testing methodologies. Through a comprehensive review of relevant literature and SeturDigital’s 25 year testing experience, this article explores the potential benefits, challenges, and prospects of integrating LLMs into software testing.",https://doi.org/10.1109/IISEC59749.2023.10391027,21-22 December 2023
https://ieeexplore.ieee.org/document/10734644/,Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs,"['Shengbei Jiang', 'Jiabao Zhang', 'Wei Chen', 'Bo Wang', 'Jianyi Zhou', 'Jie M. Zhang']",,"Automated debugging is an emerging research field that aims to automatically find and repair bugs. In this field, Fault Localization (FL) and Automated Program Repair (APR) gain the most research efforts. Most recently, researchers have adopted pre-trained Large Language Models (LLMs) to facilitate FL and APR and their results are promising. However, the LLMs they used either vanished (such as Codex) or outdated (such as early versions of GPT). In this paper, we evaluate the performance of recent commercial closed-source general-purpose LLMs on FL and APR, i.e., ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0. We select three popular LLMs and evaluate them on 120 real-world Java bugs from the benchmark Defects4J. For FL and APR, we designed three kinds of prompts for each, considering different kinds of information. The results show that these LLMs could successfully locate 53.3% and correctly fix 12.5% of these bugs.CCS CONCEPTS• Software and its engineering → Search-based software engineering; Software testing and debugging.",,20-20 April 2024
https://ieeexplore.ieee.org/document/10298349/,From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining,"['Xiaoxue Ren', 'Xinyuan Ye', 'Dehai Zhao', 'Zhenchang Xing', 'Xiaohu Yang']",,"Large Language Models (LLMs) have shown promising results in automatic code generation by improving coding efficiency to a certain extent. However, generating high-quality and reliable code remains a formidable task because of LLMs' lack of good programming practice, especially in exception handling. In this paper, we first conduct an empirical study and summarize three crucial challenges of LLMs in exception handling, i.e., incomplete exception handling, incorrect exception handling and abuse of try-catch. We then try prompts with different granularities to address such challenges, finding fine-grained knowledge-driven prompts works best. Based on our empirical study, we propose a novel Knowledge-driven Prompt Chaining-based code generation approach, name KPC, which decomposes code generation into an AI chain with iterative check-rewrite steps and chains fine-grained knowledge-driven prompts to assist LLMs in considering exception-handling specifications. We evaluate our KPC-based approach with 3,079 code generation tasks extracted from the Java official API documentation. Extensive experimental results demonstrate that the KPC-based approach has considerable potential to ameliorate the quality of code generated by LLMs. It achieves this through proficiently managing exceptions and obtaining remarkable enhancements of 109.86% and 578.57% with static evaluation methods, as well as a reduction of 18 runtime bugs in the sampled dataset with dynamic validation.",https://doi.org/10.1109/ASE56229.2023.00143,11-15 September 2023
https://ieeexplore.ieee.org/document/10615269/,Test Code Generation for Telecom Software Systems Using Two-Stage Generative Model,"['Mohamad Nabeel', 'Doumitrou Daniil Nimara', 'Tahar Zanouda']",,"In recent years, the evolution of Telecom towards achieving intelligent, autonomous, and open networks has led to an increasingly complex Telecom Software system, supporting various heterogeneous deployment scenarios, with multi-standard and multi-vendor support. As a result, it becomes a challenge for large-scale Telecom software companies to develop and test software for all deployment scenarios. To address these challenges, we propose a framework for Automated Test Generation for large-scale Telecom Software systems. We begin by generating Test Case Input data for test scenarios observed using a time-series Generative model trained on historical Telecom Network data during field trials. Additionally, the time-series Generative model helps in preserving the privacy of Telecom data. The generated time-series software performance data are then utilized with test descriptions written in natural language to generate Test Script using the Generative Large Language Model. Our comprehensive experiments on public datasets and Telecom datasets obtained from operational Telecom Networks demonstrate that the framework can effectively generate comprehensive test case data input and useful test code.",https://doi.org/10.1109/ICCWorkshops59551.2024.10615269,09-13 June 2024
https://ieeexplore.ieee.org/document/10903339/,Exploring Testing Methods for Large Language Models,"['Timothy Elvira', 'Tyler Thomas Procko', 'Lynn Vonderhaar', 'Omar Ochoa']",,"Large Language Models (LLMs) are extensive aggregations of human language, designed to understand and generate sophisticated text. LLMs are becoming ubiquitous in a range of applications, from social media to code generation. With their immense size, LLMs face scalability challenges, making testing methods particularly difficult to implement effectively. Traditional machine learning and software testing methods, derived and adapted for LLMs, test these models to a point; however, they still struggle to accurately capture the full complexity of model behavior. This paper aims to capture the current efforts and techniques in testing LLMs, specifically focusing on stress testing, mutation testing, regression testing, metamorphic testing, and adversarial testing. This survey focuses on how traditional testing methods must be adapted to fit the needs of LLMs. Furthermore, while this area is fairly novel, there are still gaps in the literature that have been identified for future research.",https://doi.org/10.1109/ICMLA61862.2024.00177,18-20 December 2024
https://ieeexplore.ieee.org/document/10684633/,Symbolic Execution with Test Cases Generated by Large Language Models,"['Jiahe Xu', 'Jingwei Xu', 'Taolue Chen', 'Xiaoxing Ma']",,"Symbolic execution is a powerful program analysis technique. External environment construction and internal path explosion are two long-standing problems which may affect the effectiveness and performance of symbolic execution on complex programs. The intrinsic challenge is to achieve a sufficient understanding of the program context to construct a set of execution environments which can guide the selection of symbolic states. In this paper, we propose a novel program-context-guided symbolic execution framework LangSym based on program’s instruction/user manual. Leveraging the capabilities of natural language understanding and code generation in large language models (LLMs), LangSym can automatically extract the knowledge related to the functionality of the program, and generate adequate test cases and the corresponding environments as the prior knowledge for symbolic execution. We instantiate LangSym in KLEE, a widely adopted symbolic execution engine, to build a pipeline that could automatically leverage LLMs to boost the symbolic execution. We evaluate LangSym on almost all GNU Coreutils programs and considerable large-scale programs, showing that LangSym outperforms the existing strategies in KLEE with at least a 10% increase for line coverage.",https://doi.org/10.1109/QRS62785.2024.00031,01-05 July 2024
https://ieeexplore.ieee.org/document/10638618/,KAT: Dependency-Aware Automated API Testing with Large Language Models,"['Tri Le', 'Thien Tran', 'Duy Cao', 'Vy Le', 'Tien N. Nguyen', 'Vu Nguyen']",,"API testing has increasing demands for software companies. Prior API testing tools were aware of certain types of dependencies that needed to be concise between operations and parameters. However, their approaches, which are mostly done manually or using heuristic-based algorithms, have limitations due to the complexity of these dependencies. In this paper, we present KAT (Katalon API Testing), a novel AI -driven approach that leverages the large language model GPT in conjunction with advanced prompting techniques to autonomously generate test cases to validate RESTful APIs. Our comprehensive strategy encompasses various processes to construct an operation depen-dency graph from an OpenAPI specification and to generate test scripts, constraint validation scripts, test cases, and test data. Our evaluation of KAT using 12 real-world RESTful services shows that it can improve test coverage, detect more undocumented status codes, and reduce false positives in these services in comparison with a state-of-the-art automated test generation tool. These results indicate the effectiveness of using the large language model for generating test scripts and data for API testing.",https://doi.org/10.1109/ICST60714.2024.00017,27-31 May 2024
https://ieeexplore.ieee.org/document/10609742/,Assessing Evaluation Metrics for Neural Test Oracle Generation,"['Jiho Shin', 'Hadi Hemmati', 'Moshi Wei', 'Song Wang']",,"Recently, deep learning models have shown promising results in test oracle generation. Neural Oracle Generation (NOG) models are commonly evaluated using static (automatic) metrics which are mainly based on textual similarity of the output, e.g. BLEU, ROUGE-L, METEOR, and Accuracy. However, these textual similarity metrics may not reflect the testing effectiveness of the generated oracle within a test suite, which is often measured by dynamic (execution-based) test adequacy metrics such as code coverage and mutation score. In this work, we revisit existing oracle generation studies plus gpt-3.5 to empirically investigate the current standing of their performance in textual similarity and test adequacy metrics. Specifically, we train and run four state-of-the-art test oracle generation models on seven textual similarity and two test adequacy metrics for our analysis. We apply two different correlation analyses between these two different sets of metrics. Surprisingly, we found no significant correlation between the textual similarity metrics and test adequacy metrics. For instance, gpt-3.5 on the jackrabbit-oak project had the highest performance on all seven textual similarity metrics among the studied NOGs. However, it had the lowest test adequacy metrics compared to all the studied NOGs. We further conducted a qualitative analysis to explore the reasons behind our observations. We found that oracles with high textual similarity metrics but low test adequacy metrics tend to have complex or multiple chained method invocations within the oracle's parameters, making them hard for the model to generate completely, affecting the test adequacy metrics. On the other hand, oracles with low textual similarity metrics but high test adequacy metrics tend to have to call different assertion types or a different method that functions similarly to the ones in the ground truth. Overall, this work complements prior studies on test oracle generation with an extensive performance ev...",https://doi.org/10.1109/TSE.2024.3433463,
https://ieeexplore.ieee.org/document/10764870/,AgoneTest: Automated creation and assessment of Unit tests leveraging Large Language Models,"['Andrea Lops', 'Fedelucio Narducci', 'Azzurra Ragone', 'Michelantonio Trizio']",,"Software correctness is crucial, with unit testing playing an indispensable role in the software development lifecycle. However, creating unit tests is time-consuming and costly, underlining the need for automation. Leveraging Large Language Models (LLMs) for unit test generation is a promising solution, but existing studies focus on simple, small-scale scenarios, leaving a gap in understanding LLMs’ performance in real-world applications, particularly regarding integration and assessment efficacy at scale. Here, we present AgoneTest, a system focused on automatically generating and evaluating complex class-level test suites. Our contributions include a scalable automated system, a newly developed dataset for rigorous evaluation, and a detailed methodology for test quality assessment.CCSCONCEPTS• Software and its engineering → Automatic programming; Software testing and debugging.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11025195/,"Exploration of Course Practice on the Integration of AI Language Model and Ideological and Political Education : The Course of ""Software Testing"" as an Example","['Chang Liu', 'Yan Jiang', 'Dongxia Zheng']",,"With the rapid development of information technology, artificial intelligence technology, and digital technology, cutting-edge technologies such as big data, intelligent algorithms, cloud computing, the Internet of Things, and blockchain are gradually penetrating into the field of education, promoting the modernization and digitization of education. In this context, software testing courses, as important courses in computer science and technology and software engineering majors, are facing new challenges and opportunities. In response to the mismatch between the curriculum teaching system and the new demands of the artificial intelligence era, it is proposed to focus on artificial intelligence technology, optimize the curriculum system from multiple dimensions, introduce AI language models to drive teaching, and pay attention to the deep integration of ideological and political education in the curriculum. This article demonstrates how to integrate ideological and political elements into software testing courses through specific cases. Practice has proven that AI driven courses can better stimulate students' interest in learning, help improve their ideological level, technical ability, and industry competitiveness.",https://doi.org/10.1109/ICISE-IE64355.2024.11025195,20-22 December 2024
https://ieeexplore.ieee.org/document/10298360/,Towards Autonomous Testing Agents via Conversational Large Language Models,"['Robert Feldt', 'Sungmin Kang', 'Juyeon Yoon', 'Shin Yoo']",,"Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized “hallucination” of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.",https://doi.org/10.1109/ASE56229.2023.00148,11-15 September 2023
https://ieeexplore.ieee.org/document/10638604/,Poster: gptCombFuzz: Combinatorial Oriented LLM Seed Generation for effective Fuzzing,"['Darshan Lohiya', 'Monika Rani Golla', 'Sangharatna Godboley', 'P. Radha Krishna']",,"The important contribution that large language models (LLMs) have made to the development of a new software testing era is the main objective of this proposed approach. It emphasizes the role that LLMs play in producing complex and diverse input seeds, which opens the way for efficient bug discovery. In the study we also introduce a systematic approach for combining various input values, employing the principles of Combinatorial testing using the PICT (Pairwise independent Combinatorial testing). By promoting a more varied set of inputs for thorough testing, PICT enhances the seed production process. Then we show how these different seeds may be easily included in the American Fuzzy Lop (AFL) tool, demonstrating how AFL can effectively use them to find and detect software flaws. This integrated technique offers a powerful yet straightforward approach to software Quality.",https://doi.org/10.1109/ICST60714.2024.00048,27-31 May 2024
https://ieeexplore.ieee.org/document/10638611/,Improving Patch Correctness Analysis via Random Testing and Large Language Models,"['Facundo Molina', 'Juan Manuel Copia', 'Alessandra Gorla']",,"Patch correctness assessment represents a crucial step in the patch validation process, with the potential to enhance the practical adoption of automated program repair (APR) techniques and substantially reduce validation costs. While some automated techniques have been proposed for assessing patch correctness, they primarily focus on either ranking patches based on their likelihood of being correct or classifying them as correct or incorrect without offering any further explanatory information. In this paper, we introduce FIXCHECK, a novel approach that combines random testing and large language models to automatically generate fault-revealing tests for potentially incorrect patches. To achieve this, FIXCHECK employs a two-fold process: Firstly, a random testing procedure generates a comprehensive set of test cases. Secondly, a large language model is utilized to derive meaningful assertions for each test case. Additionally, FIXCHECK incorporates a selection and prioritization mechanism, which evaluates the generated tests executed on the patched program and discards or ranks them based on their likelihood of revealing faults in the patch. To assess the effectiveness of our approach, we conducted evaluations on a benchmark comprising 160 patches, encompassing both patches created by developers and patches generated by APR tools. The results demonstrate that FIXCHECK effectively generates fault-revealing tests for 62 % of incorrect patches written by developers, with a high level of confidence. Furthermore, it complements existing patch correctness assessment techniques by providing fault-revealing tests for up to 50% of the incorrect patches identified by state-of-the-art techniques.",https://doi.org/10.1109/ICST60714.2024.00036,27-31 May 2024
https://ieeexplore.ieee.org/document/10123585/,Large Language Models: The Next Frontier for Variable Discovery within Metamorphic Testing?,"['Christos Tsigkanos', 'Pooja Rani', 'Sebastian Müller', 'Timo Kehrer']",,"Metamorphic testing involves reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables. A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation, a strategy particularly fitting to testing scientific software. However, such software typically has large input-output spaces, and the fundamental prerequisite – extracting variables of interest – is an arduous and non-scalable process when performed manually. To this end, we devise a workflow around an autoregressive transformer-based Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software. Our end-to-end approach, besides a prompt specification consisting of few-shot examples by a human user, is fully automated, in contrast to current practice requiring human intervention. We showcase our LLM workflow over a real case, and compare variables extracted to ground truth manually labelled by experts. Our preliminary results show that our LLM-based workflow achieves an accuracy of 0.87, while successfully deriving 61.8% of variables as partial matches and 34.7% as exact matches.",https://doi.org/10.1109/SANER56733.2023.00070,21-24 March 2023
https://ieeexplore.ieee.org/document/10556182/,Seven Failure Points When Engineering a Retrieval Augmented Generation System,"['Scott Barnett', 'Stefanus Kurniawan', 'Srikanth Thudumu', 'Zach Brannelly', 'Mohamed Abdelrazek']",,"Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.CCS CONCEPTS• Software and its engineering → Empirical software validation.",,14-15 April 2024
https://ieeexplore.ieee.org/document/10962487/,Quality Assurance for LLM-RAG Systems: Empirical Insights from Tourism Application Testing,"['Bestoun S. Ahmed', 'Ludwig Otto Baader', 'Firas Bayram', 'Siri Jagstedt', 'Peter Magnusson']",,"This paper presents a comprehensive framework for testing and evaluating quality characteristics of Large Language Model (LLM) systems enhanced with Retrieval-Augmented Generation (RAG) in tourism applications. Through systematic empirical evaluation of three different LLM variants across multiple parameter configurations, we demonstrate the effectiveness of our testing methodology in assessing both functional correctness and extra-functional properties. Our framework implements 17 distinct metrics that encompass syntactic analysis, semantic evaluation, and behavioral evaluation through LLM judges. The study reveals significant information about how different architectural choices and parameter configurations affect system performance, particularly highlighting the impact of temperature and top-p parameters on response quality. The tests were carried out on a tourism recommendation system for the Varmland region in Sweden, utilizing standard and RAG-enhanced configurations. The results indicate that the newer LLM versions show modest improvements in performance metrics, though the differences are more pronounced in response length and complexity rather than in semantic quality. The research contributes practical insights for implementing robust testing practices in LLM-RAG systems, providing valuable guidance to organizations deploying these architectures in production environments.",https://doi.org/10.1109/ICSTW64639.2025.10962487,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10796866/,Automated Test Case Generation for Satellite FRD Using NLP and Large Language Model,"['Shakthi S', 'Pratibha Srivastava', 'Ravi Kumar L', 'SG Prasad']",,"In recent times, the research on the use of Large Language Models (LLMs) for developing software applications has grown exponentially. Generating test cases for satellite Functional Requirement Documents (FRDs) pose a significant challenge due to their complex nature, requiring intricate analysis. Manual methods are time-consuming and error-prone, prompting the need for automated solutions or semi-automated solutions. This work proposes a novel approach to automate test case generation from FRDs using LLMs and Natural Language Processing (NLP). By harnessing the capabilities of LLMs, our system extracts and interprets complex variables and equations, facilitating the automated creation of comprehensive test cases. This approach aims to streamline the satellite testing process, improving efficiency and accuracy while reducing the burden on human analysts. We generate a custom dataset of 10 samples and then benchmark 4 LLMs on the dataset. We open-source the complete codebase for implementation and for further research.",https://doi.org/10.1109/ICECCME62383.2024.10796866,04-06 November 2024
https://ieeexplore.ieee.org/document/10500073/,Requirements Verification Through the Analysis of Source Code by Large Language Models,"['Juan Ortiz Couder', 'Dawson Gomez', 'Omar Ochoa']",,"In the most recent years, Large Language Models (LLMs) have gained popularity and have been accepted and used in different domains due to their ability to understand and generate written language. LLMs allow us to analyze large amounts of data in a few moments, yet they are also extremely simple to use, making them a very powerful assistive tool that can aid in a wide range of tasks; from planning a family trip, to aid during the development process of a huge system. For software developers, LLMs have been mostly used for code generation, explanation, or optimization. Software verification is a crucial part of software development as it is the process of ensuring that a system meets specific requirements. Requirements specifications play a pivotal role in software verification as they define what a system should do. In this paper we propose the use of LLMs for code verification through the analysis of requirements specifications. We prove that LLMs, such as GPT-3.5, can verify a list of requirements through a given code and evaluate why the requirements have or have not been met.",https://doi.org/10.1109/SoutheastCon52093.2024.10500073,15-24 March 2024
https://ieeexplore.ieee.org/document/10298442/,SMT Solver Validation Empowered by Large Pre-Trained Language Models,"['Maolin Sun', 'Yibiao Yang', 'Yang Wang', 'Ming Wen', 'Haoxiang Jia', 'Yuming Zhou']",,"SMT solvers are utilized to check the satisfiability of logic formulas and have been applied in various crucial domains, including software verification, test case generation, and program synthesis. However, bugs hidden in SMT solvers can lead to severe consequences, causing erroneous results in these domains. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Despite several testing approaches proposed for SMT solvers, generating effective test formulas to comprehensively test SMT solvers remains a challenge. To address this challenge, in this study, we propose to port large language models (LLMs) to generate SMT formulas for fuzzing solvers. Specifically, the study presents a novel retrain-finetune pipeline to unleash the potential of language models to generate effective SMT formulas and improve their generation performance through data augmentation. We implemented our approach as a practical fuzzing tool, named LasT,and then extensively tested the state-of-the-art SMT solvers, namely Z3, cvc5, and Bitwuzla. To date, Last has successfully uncovered 65 genuine bugs for the solvers, of which 45 have been fixed by the developers.",https://doi.org/10.1109/ASE56229.2023.00180,11-15 September 2023
https://ieeexplore.ieee.org/document/10605166/,AutoTest: Evolutionary Code Solution Selection with Test Cases,"['Zhihua Duan', 'Jialin Wang']",,"With the development of code generation techniques, selecting the correct code solution from multiple candidate solutions has become a crucial task. This study proposes AutoTest, a novel technique that combines automated test case generation with code solution execution to optimize the selection process using an evolutionary genetic algorithm. Firstly, AutoTest utilizes large pre-trained language models such as codegen-16B, code-davinci-002, and incoder-6B to provide code solutions and their corresponding test cases. Then, by executing the code solutions and evaluating their performance on the test cases, a consensus set is formed. Fine-grained ranking is achieved through the selection, mutation, and crossover mechanisms based on the evolutionary genetic algorithm, with the adjustment of alpha and beta parameters. Finally, the best code solution is chosen. AutoTest demonstrates significant performance improvements on the HumanEval benchmark test. The HumanEval dataset consists of 164 programming problems, and AutoTest achieves approximately a 10% improvement over the baseline method in terms of pass@1 score.",https://doi.org/10.1109/CSCloud62866.2024.00030,28-30 June 2024
https://ieeexplore.ieee.org/document/10659742/,Leveraging Large Language Model for Automatic Patch Correctness Assessment,"['Xin Zhou', 'Bowen Xu', 'Kisub Kim', 'DongGyun Han', 'Hung Huu Nguyen', 'Thanh Le-Cong']",,"Automated Program Repair (APR) techniques have shown more and more promising results in fixing real-world bugs. Despite the effectiveness, APR techniques still face an overfitting problem: a generated patch can be incorrect although it passes all tests. It is time-consuming to manually evaluate the correctness of generated patches that can pass all available test cases. To address this problem, many approaches have been proposed to automatically assess the correctness of patches generated by APR techniques. These approaches are mainly evaluated within the cross-validation setting. However, for patches generated by a new or unseen APR tool, users are implicitly required to manually label a significant portion of these patches (e.g., 90% in 10-fold cross-validation) in the cross-validation setting before inferring the remaining patches (e.g., 10% in 10-fold cross-validation). To mitigate the issue, in this study, we propose LLM4PatchCorrect, the patch correctness assessment by adopting a large language model for code. Specifically, for patches generated by a new or unseen APR tool, LLM4PatchCorrect does not need labeled patches of this new or unseen APR tool for training but directly queries the large language model for code to get predictions on the correctness labels without training. In this way, LLM4PatchCorrect can reduce the manual labeling effort when building a model to automatically assess the correctness of generated patches of new APR tools. To provide knowledge regarding the automatic patch correctness assessment (APCA) task to the large language model for code, LLM4PatchCorrect leverages bug descriptions, execution traces, failing test cases, test coverage, and labeled patches generated by existing APR tools, before deciding the correctness of the unlabeled patches of a new or unseen APR tool. Additionally, LLM4PatchCorrect prioritizes labeled patches from existing APR tools that exhibit semantic similarity to those generated by new APR tools, enhancing t...",https://doi.org/10.1109/TSE.2024.3452252,
https://ieeexplore.ieee.org/document/10765035/,JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models,"['Jialun Cao', 'Zhiyong Chen', 'Jiarong Wu', 'Shing-Chi Cheung', 'Chang Xu']",,"Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs’ capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8% of benchmarks involve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs’ capability to generate Java code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced features in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM’s capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 48.24% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an idea...",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10967319/,A Study of Using Multimodal LLMs for Non-Crash Functional Bug Detection in Android Apps,"['Bangyan Ju', 'Jin Yang', 'Tingting Yu', 'Tamerlan Abdullayev', 'Yuanyuan Wu', 'Dingbang Wang']",,"Numerous approaches employing various strategies have been developed to test the graphical user interfaces (GUIs) of mobile apps. However, traditional GUI testing techniques, such as random and model-based testing, primarily focus on generating test sequences that excel in achieving high code coverage but often fail to act as effective test oracles for noncrash functional (NCF) bug detection. To tackle these limitations, this study empirically investigates the capability of leveraging large language models (LLMs) to be test oracles to detect NCF bugs in Android apps. Our intuition is that the training corpora of LLMs, encompassing extensive mobile app usage and bug report descriptions, enable them with the domain knowledge relevant to NCF bug detection. We conducted a comprehensive empirical study to explore the effectiveness of LLMs as test oracles for detecting NCF bugs in Android apps on 71 welldocumented NCF bugs. The results demonstrated that LLMs achieve a 49% bug detection rate, outperforming existing tools for detecting NCF bugs in Android apps. Additionally, by leveraging LLMs to be test oracles, we successfully detected 24 previously unknown NCF bugs in 64 Android apps, with four of these bugs being confirmed or fixed. However, we also identified limitations of LLMs, primarily related to performance degradation, inherent randomness, and false positives. Our study highlights the potential of leveraging LLMs as test oracles for Android NCF bug detection and suggests directions for future research.",https://doi.org/10.1109/APSEC65559.2024.00017,03-06 December 2024
https://ieeexplore.ieee.org/document/10772249/,MoCo: Fuzzing Deep Learning Libraries via Assembling Code,"['Pin Ji', 'Yang Feng', 'Duo Wu', 'Lingyue Yan', 'Penglin Chen', 'Jia Liu']",,"The rapidly developing Deep Learning (DL) techniques have been applied in software systems of various types. However, they can also pose new safety threats with potentially serious consequences, especially in safety-critical domains. DL libraries serve as the underlying foundation for DL systems, and bugs in them can have unpredictable impacts that directly affect the behaviors of DL systems. Previous research on fuzzing DL libraries still has limitations in generating tests corresponding to crucial testing scenarios and constructing test oracles. In this paper, we propose MoCo, a novel fuzzing testing method for DL libraries via assembling code. The seed tests used by MoCo are code files that implement DL models, covering both model construction and training in the most common real-world application scenarios for DL libraries. MoCo first disassembles the seed code files to extract templates and code blocks, then applies code block mutation operators (e.g., API replacement, random generation, and boundary checking) to generate new code blocks that fit the template. To ensure the correctness of the code block mutation, we employ the Large Language Model to parse the official documents of DL libraries for information about the parameters and the constraints between them. By inserting context-appropriate code blocks into the template, MoCo can generate a tree of code files with intergenerational relations. According to the derivation relations in this tree, we construct the test oracle based on the execution state consistency and the calculation result consistency. Since the granularity of code assembly is controlled rather than randomly divergent, we can quickly pinpoint the lines of code where the bugs are located and the corresponding triggering conditions. We conduct a comprehensive experiment to evaluate the efficiency and effectiveness of MoCo using three widely-used DL libraries (i.e., TensorFlow, PyTorch, and Jittor). During the experiments, MoCo detects 77 new...",https://doi.org/10.1109/TSE.2024.3509975,
https://ieeexplore.ieee.org/document/10644000/,"PyBugHive: A Comprehensive Database of Manually Validated, Reproducible Python Bugs","['Gábor Antal', 'Norbert Vándor', 'István Kolláth', 'Balázs Mosolygó', 'Péter Hegedűs', 'Rudolf Ferenc']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10644000,"Python is currently the number one language in the TIOBE index and has been the second most popular language on GitHub for years. But so far, there are only a few bug databases that contain bugs for Python projects and even fewer in which bugs can be reproduced. In this paper, we present a manually curated database of reproducible Python bugs called PyBugHive. The initial version of PyBugHive is a benchmark of 149 real, manually validated bugs from 11 Python projects. Each entry in our database contains the summary of the bug report, the corresponding patch, and the test cases that expose the given bug. PyBugHive features a rich command line interface for accessing both the buggy and fixed versions of the programs and provides the abstraction for executing the corresponding test cases. The interface facilitates highly reproducible empirical research and tool comparisons in fields such as testing, automated program repair, or bug prediction. The usage of our database is demonstrated through a use case involving a large language model, GPT-3.5. First, we evaluated the bug detection capabilities of the model with the help of the bug repository. Using multiple prompts, we found out that GPT-3.5 was able to detect 67 out of 149 bugs (45%). Furthermore, we leveraged the constructed bug dataset in assessing the automatic program repair capabilities of GPT-3.5 by comparing the generated fixes with the real patches contained in the dataset. However, its performance was far worse in this task compared to bug detection, as it was able to fix only one of the detected issues.",https://doi.org/10.1109/ACCESS.2024.3449106,
https://ieeexplore.ieee.org/document/11029933/,Ranking Relevant Tests for Order-Dependent Flaky Tests,"['Shanto Rahman', 'Bala Naren Chanumolu', 'Suzzana Rafi', 'August Shi', 'Wing Lam']",,"One major challenge of regression testing are flaky tests, i.e., tests that may pass in one run but fail in another run for the same version of code. One prominent category of flaky tests is order-dependent (OD) flaky tests, which can pass or fail depending on the order in which the tests are run. To help developers debug and fix OD tests, prior work attempts to automatically find OD-relevant tests, which are tests that determine whether an OD test passes or fails, depending on whether the OD-relevant tests run before or after the OD test. Prior work found OD-relevant tests by running different tests before the OD test, without considering each test's likelihood of being OD-relevant tests. We propose RankF to rank tests in order of likelihood of being OD-relevant tests, finding the first OD-relevant test for a given OD test more quickly. We propose two ranking approaches, each requiring different information. Our first approach, RankFL
, relies on training a large-language model to analyze test code. Our second approach, RankFO
, relies on analyzing prior test-order execution information. We evaluate our approaches on 155 OD tests across 24 open-source projects. We compare RankF against baselines from prior work, where we find that RankF finds the first OD-relevant test for an OD test faster than the best baseline; depending on the type of OD-relevant test, RankF takes 9.4 to 14.1 seconds on median, compared to the baseline's 34.2 to 118.5 seconds on median.",https://doi.org/10.1109/ICSE55347.2025.00178,26 April 2025 - 06 May 2025
https://arxiv.org/abs/2503.20576,Optimizing Case-Based Reasoning System for FunctionalTest Script Generation with Large Language Models,"Siyuan Guo, Huiwu Liu, Xiaolong Chen, Yuming Xie, Liang Zhang, Tao Han, Hechang Chen, Yi Chang, Jun Wang",https://arxiv.org/pdf/2503.20576,"In this work, we explore the potential of large language models (LLMs) for generating functionaltest scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs.",,"v1 submitted 26 March, 2025"
https://arxiv.org/abs/2306.12643,FLAG: Finding Line Anomalies (in code) with Generative AI,"Baleegh Ahmad, Benjamin Tan, Ramesh Karri, Hammond Pearce",https://arxiv.org/pdf/2306.12643,"Code contains security and functional bugs. The process of identifying and localizing them is difficult and relies on human labor. In this work, we present a novel approach (FLAG) to assist human debuggers. FLAG is based on the lexical capabilities of generative AI, specifically, Large Language Models (LLMs). Here, we input a code file then extract and regenerate each line within that file for self-comparison. By comparing the original code with an LLM-generated alternative, we can flag notable differences as anomalies for further inspection, with features such as distance from comments and LLM confidence also aiding this classification. This reduces the inspection search space for the designer. Unlike other automated approaches in this area, FLAG is language-agnostic, can work on incomplete (and even non-compiling) code and requires no creation of security properties, functionaltests or definition of rules. In this work, we explore the features that help LLMs in this classification and evaluate the performance of FLAG on known bugs. We use 121 benchmarks across C, Python and Verilog; with each benchmark containing a known security or functional weakness. We conduct the experiments using two state of the art LLMs in OpenAI's code-davinci-002 and gpt-3.5-turbo, but our approach may be used by other models. FLAG can identify 101 of the defects and helps reduce the search space to 12-17% of source code.",,originally announced June 2023.
https://arxiv.org/abs/2504.01495,Are Autonomous Web Agents Good Testers?,"Antoine Chevrot, Alexandre Vernotte, Jean-Rémy Falleri, Xavier Blanc, Bruno Legeard",https://arxiv.org/pdf/2504.01495,"Despite advances in automated testing, manual testing remains prevalent due to the high maintenance demands associated with test script fragility-scripts often break with minor changes in application structure. Recent developments in Large Language Models (LLMs) offer a potential alternative by powering Autonomous Web Agents (AWAs) that can autonomously interact with applications. These agents may serve as Autonomous Test Agents (ATAs), potentially reducing the need for maintenance-heavy automated scripts by utilising natural language instructions similar to those used by human testers. This paper investigates the feasibility of adapting AWAs for natural language test case execution and how to evaluate them. We contribute with (1) a benchmark of three offline web applications, and a suite of 113 manual test cases, split between passing and failing cases, to evaluate and compare ATAs performance, (2) SeeAct-ATA and pinATA, two open-source ATA implementations capable of executing test steps, verifying assertions and giving verdicts, and (3) comparative experiments using our benchmark that quantifies our ATAs effectiveness. Finally we also proceed to a qualitative evaluation to identify the limitations of PinATA, our best performing implementation. Our findings reveal that our simple implementation, SeeAct-ATA, does not perform well compared to our more advanced PinATA implementation when executing test cases (50% performance improvement). However, while PinATA obtains around 60% of correct verdict and up to a promising 94% specificity, we identify several limitations that need to be addressed to develop more resilient and reliable ATAs, paving the way for robust, low maintenance test automation. CCS Concepts: \bullet
 Software and its engineering \rightarrow
Softwaretesting and debugging.",,originally announced April 2025.
https://arxiv.org/abs/2503.04479,ToolFuzz -- Automated Agent Tool Testing,"Ivan Milev, Mislav Balunović, Maximilian Baader, Martin Vechev",https://arxiv.org/pdf/2503.04479,"Large Language Model (LLM) Agents leverage the advanced reasoning capabilities of LLMs in real-world applications. To interface with an environment, these agents often rely on tools, such as web search or database APIs. As the agent provides the LLM with tool documentation along the user query, the completeness and correctness of this documentation is critical. However, tool documentation is often over-, under-, or ill-specified, impeding the agent's accuracy. Standard softwaretesting approaches struggle to identify these errors as they are expressed in natural language. Thus, despite its importance, there currently exists no automated method to test the tool documentation for agents. To address this issue, we present ToolFuzz, the first method for automated testing of tool documentations. ToolFuzz is designed to discover two types of errors: (1) user queries leading to tool runtime errors and (2) user queries that lead to incorrect agent responses. ToolFuzz can generate a large and diverse set of natural inputs, effectively finding tool description errors at a low false positive rate. Further, we present two straightforward prompt-engineering approaches. We evaluate all three tool testing approaches on 32 common LangChain tools and 35 newly created custom tools and 2 novel benchmarks to further strengthen the assessment. We find that many publicly available tools suffer from underspecification. Specifically, we show that ToolFuzz identifies 20x more erroneous inputs compared to the prompt-engineering approaches, making it a key component for building reliable AI agents.",,"v1 submitted 6 March, 2025"
https://arxiv.org/abs/2501.06837,An efficient approach to represent enterprise web application structure using Large Language Model in the service of Intelligent Quality Engineering,"Zaber Al Hassan Ayon, Gulam Husain, Roshankumar Bisoi, Waliur Rahman, Dr Tom Osborn",https://arxiv.org/pdf/2501.06837,"This paper presents a novel approach to represent enterprise web application structures using Large Language Models (LLMs) to enable intelligent quality engineering at scale. We introduce a hierarchical representation methodology that optimizes the few-shot learning capabilities of LLMs while preserving the complex relationships and interactions within web applications. The approach encompasses five key phases: comprehensive DOM analysis, multi-page synthesis, test suite generation, execution, and result analysis. Our methodology addresses existing challenges around usage of Generative AI techniques in automated softwaretesting by developing a structured format that enables LLMs to understand web application architecture through in-context learning. We evaluated our approach using two distinct web applications: an e-commerce platform (Swag Labs) and a healthcare application (MediBox) which is deployed within Atalgo engineering environment. The results demonstrate success rates of 90\% and 70\%, respectively, in achieving automated testing, with high relevance scores for test cases across multiple evaluation criteria. The findings suggest that our representation approach significantly enhances LLMs' ability to generate contextually relevant test cases and provide better quality assurance overall, while reducing the time and effort required for testing.",,originally announced January 2025.
https://arxiv.org/abs/2411.17045,Redefining Crowdsourced Test Report Prioritization: An Innovative Approach with Large Language Model,"Yuchen Ling, Shengcheng Yu, Chunrong Fang, Guobin Pan, Jun Wang, Jia Liu",https://arxiv.org/pdf/2411.17045,"Context: Crowdsourced testing has gained popularity in softwaretesting, especially for mobile app testing, due to its ability to bring diversity and tackle fragmentation issues. However, the openness of crowdsourced testing presents challenges, particularly in the manual review of numerous test reports, which is time-consuming and labor-intensive. Objective: The primary goal of this research is to improve the efficiency of review processes in crowdsourced testing. Traditional approaches to test report prioritization lack a deep understanding of semantic information in textual descriptions of these reports. This paper introduces LLMPrior, a novel approach for prioritizing crowdsourced test reports using large language models (LLMs). Method: LLMPrior leverages LLMs for the analysis and clustering of crowdsourced test reports based on the types of bugs revealed in their textual descriptions. This involves using prompt engineering techniques to enhance the performance of LLMs. Following the clustering, a recurrent selection algorithm is applied to prioritize the reports. Results: Empirical experiments are conducted to evaluate the effectiveness of LLMPrior. The findings indicate that LLMPrior not only surpasses current state-of-the-art approaches in terms of performance but also proves to be more feasible, efficient, and reliable. This success is attributed to the use of prompt engineering techniques and the cluster-based prioritization strategy. Conclusion: LLMPrior represents a significant advancement in crowdsourced test report prioritization. By effectively utilizing large language models and a cluster-based strategy, it addresses the challenges in traditional prioritization approaches, offering a more efficient and reliable solution for app developers dealing with crowdsourced test reports.",,originally announced November 2024.
https://arxiv.org/abs/2506.14606,Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees,"Ahmed Heakl, Sarim Hashmi, Chaimaa Abi, Celine Lee, Abdulrahman Mahmoud",https://arxiv.org/pdf/2506.14606,"The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established softwaretesting constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.",,originally announced June 2025.
https://arxiv.org/abs/2506.10998,Towards Automated Formal Verification of Backend Systems with LLMs,"Kangping Xu, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao",https://arxiv.org/pdf/2506.10998,"Softwaretesting plays a critical role in ensuring that systems behave as intended. However, existing automated testing approaches struggle to match the capabilities of human engineers due to key limitations such as test locality, lack of general reliability, and business logic blindness. In this work, we propose a novel framework that leverages functional programming and type systems to translate Scala backend code into formal Lean representations. Our pipeline automatically generates theorems that specify the intended behavior of APIs and database operations, and uses LLM-based provers to verify them. When a theorem is proved, the corresponding logic is guaranteed to be correct and no further testing is needed. If the negation of a theorem is proved instead, it confirms a bug. In cases where neither can be proved, human intervention is required. We evaluate our method on realistic backend systems and find that it can formally verify over 50% of the test requirements, which suggests that half of a testing engineer's workload can be automated. Additionally, with an average cost of only $2.19 per API, LLM-based verification is significantly more cost-effective than manual testing and can be scaled easily through parallel execution. Our results indicate a promising direction for scalable, AI-powered softwaretesting, with the potential to greatly improve engineering productivity as models continue to advance.",,originally announced June 2025.
https://arxiv.org/abs/2506.09002,Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models,"Bei Chu, Yang Feng, Kui Liu, Hange Shi, Zifan Nan, Zhaoqiang Guo, Baowen Xu",https://arxiv.org/pdf/2506.09002,"Unit testing is essential for ensuring software reliability and correctness. Classic Search-Based SoftwareTesting (SBST) methods and concolic execution-based approaches for generating unit tests often fail to achieve high coverage due to difficulties in handling complex program units, such as branching conditions and external dependencies. Recent work has increasingly utilized large language models (LLMs) to generate test cases, improving the quality of test generation by providing better context and correcting errors in the model's output. However, these methods rely on fixed prompts, resulting in relatively low compilation success rates and coverage. This paper presents PALM, an approach that leverages large language models (LLMs) to enhance the generation of high-coverage unit tests. PALM performs program analysis to identify branching conditions within functions, which are then combined into path constraints. These constraints and relevant contextual information are used to construct prompts that guide the LLMs in generating unit tests. We implement the approach and evaluate it in 10 open-source Rust crates. Experimental results show that within just two or three hours, PALM can significantly improves test coverage compared to classic methods, with increases in overall project coverage exceeding 50% in some instances and its generated tests achieving an average coverage of 75.77%, comparable to human effort (71.30%), highlighting the potential of LLMs in automated test generation. We submitted 91 PALM-generated unit tests targeting new code. Of these submissions, 80 were accepted, 5 were rejected, and 6 remain pending review. The results demonstrate the effectiveness of integrating program analysis with AI and open new avenues for future research in automated softwaretesting.",,"v1 submitted 10 June, 2025"
https://arxiv.org/abs/2505.12424,EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and Genetic Optimization,"Lior Broide, Roni Stern",https://arxiv.org/pdf/2505.12424,"Large Language Models (LLMs) have recently emerged as promising tools for automated unit test generation. We introduce a hybrid framework called EvoGPT that integrates LLM-based test generation with evolutionary search techniques to create diverse, fault-revealing unit tests. Unit tests are initially generated with diverse temperature sampling to maximize behavioral and test suite diversity, followed by a generation-repair loop and coverage-guided assertion enhancement. The resulting test suites are evolved using genetic algorithms, guided by a fitness function prioritizing mutation score over traditional coverage metrics. This design emphasizes the primary objective of unit testing-fault detection. Evaluated on multiple open-source Java projects, EvoGPT achieves an average improvement of 10% in both code coverage and mutation score compared to LLMs and traditional search-based softwaretesting baselines. These results demonstrate that combining LLM-driven diversity, targeted repair, and evolutionary optimization produces more effective and resilient test suites.",,originally announced May 2025.
https://arxiv.org/abs/2505.08903,Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks,"Xing Hu, Feifei Niu, Junkai Chen, Xin Zhou, Junwei Zhang, Junda He, Xin Xia, David Lo",https://arxiv.org/pdf/2505.08903,"Large language models (LLMs) are gaining increasing popularity in software engineering (SE) due to their unprecedented performance across various applications. These models are increasingly being utilized for a range of SE tasks, including requirements engineering and design, code analysis and generation, software maintenance, and quality assurance. As LLMs become more integral to SE, evaluating their effectiveness is crucial for understanding their potential in this field. In recent years, substantial efforts have been made to assess LLM performance in various SE tasks, resulting in the creation of several benchmarks tailored to this purpose. This paper offers a thorough review of 291 benchmarks, addressing three main aspects: what benchmarks are available, how benchmarks are constructed, and the future outlook for these benchmarks. We begin by examining SE tasks such as requirements engineering and design, coding assistant, softwaretesting, AIOPs, software maintenance, and quality management. We then analyze the benchmarks and their development processes, highlighting the limitations of existing benchmarks. Additionally, we discuss the successes and failures of LLMs in different software tasks and explore future opportunities and challenges for SE-related benchmarks. We aim to provide a comprehensive overview of benchmark research in SE and offer insights to support the creation of more effective evaluation tools.",,"v1 submitted 13 May, 2025"
https://arxiv.org/abs/2505.05584,PRIMG : Efficient LLM-driven Test Generation Using Mutant Prioritization,"Mohamed Salah Bouafif, Mohammad Hamdaqa, Edward Zulkoski",https://arxiv.org/pdf/2505.05584,"Mutation testing is a widely recognized technique for assessing and enhancing the effectiveness of softwaretest suites by introducing deliberate code mutations. However, its application often results in overly large test suites, as developers generate numerous tests to kill specific mutants, increasing computational overhead. This paper introduces PRIMG (Prioritization and Refinement Integrated Mutation-driven Generation), a novel framework for incremental and adaptive test case generation for Solidity smart contracts. PRIMG integrates two core components: a mutation prioritization module, which employs a machine learning model trained on mutant subsumption graphs to predict the usefulness of surviving mutants, and a test case generation module, which utilizes Large Language Models (LLMs) to generate and iteratively refine test cases to achieve syntactic and behavioral correctness. We evaluated PRIMG on real-world Solidity projects from Code4Arena to assess its effectiveness in improving mutation scores and generating high-quality test cases. The experimental results demonstrate that PRIMG significantly reduces test suite size while maintaining high mutation coverage. The prioritization module consistently outperformed random mutant selection, enabling the generation of high-impact tests with reduced computational effort. Furthermore, the refining process enhanced the correctness and utility of LLM-generated tests, addressing their inherent limitations in handling edge cases and complex program logic.",,originally announced May 2025.
https://arxiv.org/abs/2504.16472,Harden and Catch for Just-in-Time Assured LLM-Based SoftwareTesting: Open Research Challenges,"Mark Harman, Peter O'Hearn, Shubho Sengupta",https://arxiv.org/pdf/2504.16472,"Despite decades of research and practice in automated softwaretesting, several fundamental concepts remain ill-defined and under-explored, yet offer enormous potential real-world impact. We show that these concepts raise exciting new challenges in the context of Large Language Models for softwaretest generation. More specifically, we formally define and investigate the properties of hardening and catching tests. A hardening test is one that seeks to protect against future regressions, while a catching test is one that catches such a regression or a fault in new functionality introduced by a code change. Hardening tests can be generated at any time and may become catching tests when a future regression is caught. We also define and motivate the Catching 'Just-in-Time' (JiTTest) Challenge, in which tests are generated 'just-in-time' to catch new faults before they land into production. We show that any solution to Catching JiTTest generation can also be repurposed to catch latent faults in legacy code. We enumerate possible outcomes for hardening and catching tests and JiTTests, and discuss open research problems, deployment options, and initial results from our work on automated LLM-based hardening at Meta. This paper was written to accompany the keynote by the authors at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025. Author order is alphabetical. The corresponding author is Mark Harman.",,"v1 submitted 23 April, 2025"
https://arxiv.org/abs/2504.01866,From Code Generation to SoftwareTesting: AI Copilot with Context-Based RAG,"Yuchen Wang, Shangxin Guo, Chee Wei Tan",https://arxiv.org/pdf/2504.01866,"The rapid pace of large-scale software development places increasing demands on traditional testing methodologies, often leading to bottlenecks in efficiency, accuracy, and coverage. We propose a novel perspective on softwaretesting by positing bug detection and coding with fewer bugs as two interconnected problems that share a common goal, which is reducing bugs with limited resources. We extend our previous work on AI-assisted programming, which supports code auto-completion and chatbot-powered Q&A, to the realm of softwaretesting. We introduce Copilot for Testing, an automated testing system that synchronizes bug detection with codebase updates, leveraging context-based Retrieval Augmented Generation (RAG) to enhance the capabilities of large language models (LLMs). Our evaluation demonstrates a 31.2% improvement in bug detection accuracy, a 12.6% increase in critical test coverage, and a 10.5% higher user acceptance rate, highlighting the transformative potential of AI-driven technologies in modern software development practices.",,"v1 submitted 2 April, 2025"
https://arxiv.org/abs/2503.10784,Vulnerability Detection: From Formal Verification to Large Language Models and Hybrid Approaches: A Comprehensive Overview,"Norbert Tihanyi, Tamas Bisztray, Mohamed Amine Ferrag, Bilel Cherif, Richard A. Dubniczky, Ridhi Jain, Lucas C. Cordeiro",https://arxiv.org/pdf/2503.10784,"Softwaretesting and verification are critical for ensuring the reliability and security of modern software systems. Traditionally, formal verification techniques, such as model checking and theorem proving, have provided rigorous frameworks for detecting bugs and vulnerabilities. However, these methods often face scalability challenges when applied to complex, real-world programs. Recently, the advent of Large Language Models (LLMs) has introduced a new paradigm for software analysis, leveraging their ability to understand insecure coding practices. Although LLMs demonstrate promising capabilities in tasks such as bug prediction and invariant generation, they lack the formal guarantees of classical methods. This paper presents a comprehensive study of state-of-the-art softwaretesting and verification, focusing on three key approaches: classical formal methods, LLM-based analysis, and emerging hybrid techniques, which combine their strengths. We explore each approach's strengths, limitations, and practical applications, highlighting the potential of hybrid systems to address the weaknesses of standalone methods. We analyze whether integrating formal rigor with LLM-driven insights can enhance the effectiveness and scalability of software verification, exploring their viability as a pathway toward more robust and adaptive testing frameworks.",,originally announced March 2025.
https://arxiv.org/abs/2502.11167,SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors,"Bohan Lyu, Siqiao Huang, Zichen Liang, Qi-An Sun, Jiaming Zhang",https://arxiv.org/pdf/2502.11167,"Neural surrogate models have emerged as powerful and efficient tools in data mining. Meanwhile, large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks. We investigate a novel application: using LLMs as surrogate models for code execution prediction. Given LLMs' unique ability to understand and process diverse programs, they present a promising direction for building general-purpose surrogate models. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark with 1160
 problems covering 8
 key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. Through extensive empirical analysis of 21
 open-source and proprietary LLMs, we examine scaling laws, data efficiency, and predictive accuracy. Our findings reveal important insights about the feasibility of LLMs as efficient surrogates for computational processes, with implications for automated softwaretesting, program analysis, and computational resource optimization in data mining applications. Code and dataset are released at https://github.com/Imbernoulli/SURGE.",,"v1 submitted 16 February, 2025"
https://arxiv.org/abs/2502.01806,Toward Neurosymbolic Program Comprehension,"Alejandro Velasco, Aya Garryyeva, David N. Palacio, Antonio Mastropaolo, Denys Poshyvanyk",https://arxiv.org/pdf/2502.01806,"Recent advancements in Large Language Models (LLMs) have paved the way for Large Code Models (LCMs), enabling automation in complex software engineering tasks, such as code generation, softwaretesting, and program comprehension, among others. Tools like GitHub Copilot and ChatGPT have shown substantial benefits in supporting developers across various practices. However, the ambition to scale these models to trillion-parameter sizes, exemplified by GPT-4, poses significant challenges that limit the usage of Artificial Intelligence (AI)-based systems powered by large Deep Learning (DL) models. These include rising computational demands for training and deployment and issues related to trustworthiness, bias, and interpretability. Such factors can make managing these models impractical for many organizations, while their ""black-box'' nature undermines key aspects, including transparency and accountability. In this paper, we question the prevailing assumption that increasing model parameters is always the optimal path forward, provided there is sufficient new data to learn additional patterns. In particular, we advocate for a Neurosymbolic research direction that combines the strengths of existing DL techniques (e.g., LLMs) with traditional symbolic methods--renowned for their reliability, speed, and determinism. To this end, we outline the core features and present preliminary results for our envisioned approach, aimed at establishing the first Neurosymbolic Program Comprehension (NsPC) framework to aid in identifying defective code components.",,originally announced February 2025.
https://arxiv.org/abs/2412.14137,Design choices made by LLM-based test generators prevent them from finding bugs,"Noble Saji Mathews, Meiyappan Nagappan",https://arxiv.org/pdf/2412.14137,"There is an increasing amount of research and commercial tools for automated test case generation using Large Language Models (LLMs). This paper critically examines whether recent LLM-based test generation tools, such as Codium CoverAgent and CoverUp, can effectively find bugs or unintentionally validate faulty code. Considering bugs are only exposed by failing test cases, we explore the question: can these tools truly achieve the intended objectives of softwaretesting when their test oracles are designed to pass? Using real human-written buggy code as input, we evaluate these tools, showing how LLM-generated tests can fail to detect bugs and, more alarmingly, how their design can worsen the situation by validating bugs in the generated test suite and rejecting bug-revealing tests. These findings raise important questions about the validity of the design behind LLM-based test generation tools and their impact on software quality and test suite reliability.",,originally announced December 2024.
https://arxiv.org/abs/2412.12340,A Large Language Model Approach to Identify Flakiness in C++ Projects,"Xin Sun, Daniel Ståhl, Kristian Sandahl",https://arxiv.org/pdf/2412.12340,"The role of regression testing in softwaretesting is crucial as it ensures that any new modifications do not disrupt the existing functionality and behaviour of the software system. The desired outcome is for regression tests to yield identical results without any modifications made to the system being tested. In practice, however, the presence of Flaky Tests introduces non-deterministic behaviour and undermines the reliability of regression testing results. In this paper, we propose an LLM-based approach for identifying the root cause of flaky tests in C++ projects at the code level, with the intention of assisting developers in debugging and resolving them more efficiently. We compile a comprehensive collection of C++ project flaky tests sourced from GitHub repositories. We fine-tune Mistral-7b, Llama2-7b and CodeLlama-7b models on the C++ dataset and an existing Java dataset and evaluate the performance in terms of precision, recall, accuracy, and F1 score. We assess the performance of the models across various datasets and offer recommendations for both research and industry applications. The results indicate that our models exhibit varying performance on the C++ dataset, while their performance is comparable to that of the Java dataset. The Mistral-7b surpasses the other two models regarding all metrics, achieving a score of 1. Our results demonstrate the exceptional capability of LLMs to accurately classify flakiness in C++ and Java projects, providing a promising approach to enhance the efficiency of debugging flaky tests in practice.",,"v1 submitted 16 December, 2024"
https://arxiv.org/abs/2412.02735,CPP-UT-Bench: Can LLMs Write Complex Unit Tests in C++?,"Vaishnavi Bhargava, Rajat Ghosh, Debojyoti Dutta",https://arxiv.org/pdf/2412.02735,"We introduce CPP-UT-Bench, a benchmark dataset to measure C++ unit test generation capability of a large language model (LLM). CPP-UT-Bench aims to reflect a broad and diverse set of C++ codebases found in the real world. The dataset includes 2,653 {code, unit test} pairs drawn from 14 different opensource C++ codebases spanned across nine diverse domains including machine learning, softwaretesting, parsing, standard input-output, data engineering, logging, complete expression evaluation, key value storage, and server protocols. We demonstrated the effectiveness of CPP-UT-Bench as a benchmark dataset through extensive experiments in in-context learning, parameter-efficient fine-tuning (PEFT), and full-parameter fine-tuning. We also discussed the challenges of the dataset compilation and insights we learned from in-context learning and fine-tuning experiments. Besides the CPP-UT-Bench dataset and data compilation code, we are also offering the fine-tuned model weights for further research. For nine out of ten experiments, our fine-tuned LLMs outperformed the corresponding base models by an average of more than 70%.",,originally announced December 2024.
https://arxiv.org/abs/2411.01789,Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs,"Shan Jiang, Chenguang Zhu, Sarfraz Khurshid",https://arxiv.org/pdf/2411.01789,"Softwaretesting remains the most widely used methodology for validating quality of code. However, effectiveness of testing critically depends on the quality of test suites used. Test cases in a test suite consist of two fundamental parts: (1) input values for the code under test, and (2) correct checks for the outputs it produces. These checks are commonly written as assertions, and termed test oracles. The last couple of decades have seen much progress in automated test input generation, e.g., using fuzzing and symbolic execution. However, automating test oracles remains a relatively less explored problem area. Indeed, a test oracle by its nature requires knowledge of expected behavior, which may only be known to the developer and may not not exist in a formal language that supports automated reasoning. Our focus in this paper is automation of test oracles for clients of widely used Java libraries, e.g., java.lang and java.util packages. Our key insight is that Javadocs that provide a rich source of information can enable automated generation of test oracles. Javadocs of the core Java libraries are fairly detailed documents that contain natural language descriptions of not only how the libraries behave but also how the clients must (not) use them. We use large language models as an enabling technology to embody our insight into a framework for test oracle automation, and evaluate it experimentally. Our experiments demonstrate that LLMs can generate oracles for checking normal and exceptional behaviors from Javadocs, with 98.8% of these oracles being compilable and 96.4% accurately reflecting intended properties. Even for the few incorrect oracles, errors are minor and can be easily corrected with the help of additional comment information generated by the LLMs.",,"v1 submitted 3 November, 2024"
https://arxiv.org/abs/2410.21136,Do LLMs generate test oracles that capture the actual or the expected program behaviour?,"Michael Konstantinou, Renzo Degiovanni, Mike Papadakis",https://arxiv.org/pdf/2410.21136,"Softwaretesting is an essential part of the software development cycle to improve the code quality. Typically, a unit test consists of a test prefix and a test oracle which captures the developer's intended behaviour. A known limitation of traditional test generation techniques (e.g. Randoop and Evosuite) is that they produce test oracles that capture the actual program behaviour rather than the expected one. Recent approaches leverage Large Language Models (LLMs), trained on an enormous amount of data, to generate developer-like code and test cases. We investigate whether the LLM-generated test oracles capture the actual or expected software behaviour. We thus, conduct a controlled experiment to answer this question, by studying LLMs performance on two tasks, namely, test oracle classification and generation. The study includes developer-written and automatically generated test cases and oracles for 24 open-source Java repositories, and different well tested prompts. Our findings show that LLM-based test generation approaches are also prone on generating oracles that capture the actual program behaviour rather than the expected one. Moreover, LLMs are better at generating test oracles rather than classifying the correct ones, and can generate better test oracles when the code contains meaningful test or variable names. Finally, LLM-generated test oracles have higher fault detection potential than the Evosuite ones.",,originally announced October 2024.
https://arxiv.org/abs/2410.10628,Test smells in LLM-Generated Unit Tests,"Wendkûuni C. Ouédraogo, Yinghua Li, Kader Kaboré, Xunzhu Tang, Anil Koyuncu, Jacques Klein, David Lo, Tegawendé F. Bissyandé",https://arxiv.org/pdf/2410.10628,"The use of Large Language Models (LLMs) in automated test generation is gaining popularity, with much of the research focusing on metrics like compilability rate, code coverage and bug detection. However, an equally important quality metric is the presence of test smells design flaws or anti patterns in test code that hinder maintainability and readability. In this study, we explore the diffusion of test smells in LLM generated unit test suites and compare them to those found in human written ones. We analyze a benchmark of 20,500 LLM-generated test suites produced by four models (GPT-3.5, GPT-4, Mistral 7B, and Mixtral 8x7B) across five prompt engineering techniques, alongside a dataset of 780,144 human written test suites from 34,637 projects. Leveraging TsDetect, a state of the art tool capable of detecting 21 different types of test smells, we identify and analyze the prevalence and co-occurrence of various test smells in both human written and LLM-generated test suites. Our findings reveal new insights into the strengths and limitations of LLMs in test generation. First, regarding prevalence, we observe that LLMs frequently generate tests with common test smells, such as Magic Number Test and Assertion Roulette. Second, in terms of co occurrence, certain smells, like Long Test and Useless Test, tend to co occur in LLM-generated suites, influenced by specific prompt techniques. Third, we find that project complexity and LLM specific factors, including model size and context length, significantly affect the prevalence of test smells. Finally, the patterns of test smells in LLM-generated tests often mirror those in human-written tests, suggesting potential data leakage from training datasets. These insights underscore the need to refine LLM-based test generation for cleaner code and suggest improvements in both LLM capabilities and softwaretesting practices.",,originally announced October 2024.
https://arxiv.org/abs/2410.00752,TestGenEval: A Real World Unit Test Generation and Test Completion Benchmark,"Kush Jain, Gabriel Synnaeve, Baptiste Rozière",https://arxiv.org/pdf/2410.00752,"Code generation models can help improve many common software tasks ranging from code completion to defect prediction. Most of the existing benchmarks for code generation LLMs focus on code authoring or code completion. Surprisingly, there has been far less effort dedicated to benchmarking softwaretesting, despite the strong correlation between well-tested software and effective bug detection. To address this gap, we create and release TestGenEval, a large-scale benchmark to measure test generation performance. Based on SWEBench, TestGenEval comprises 68,647 tests from 1,210 code and test file pairs across 11 well-maintained Python repositories. It covers initial tests authoring, test suite completion, and code coverage improvements. Test authoring simulates the process of a developer writing a test suite from scratch, while test completion mimics the scenario where a developer aims to improve the coverage of an existing test suite. We evaluate several popular models, with sizes ranging from 7B to 405B parameters. Our detailed analysis highlights TestGenEval's contribution to a comprehensive evaluation of test generation performance. In particular, models struggle to generate high-coverage test suites, with the best model, GPT-4o, achieving an average coverage of only 35.2%. This is primarily due to models struggling to reason about execution, and their frequent assertion errors when addressing complex code paths.",,"v1 submitted 1 October, 2024"
https://arxiv.org/abs/2409.17561,TestBench: Evaluating Class-Level Test Case Generation Capability of Large Language Models,"Quanjun Zhang, Ye Shang, Chunrong Fang, Siqi Gu, Jianyi Zhou, Zhenyu Chen",https://arxiv.org/pdf/2409.17561,"Softwaretesting is a crucial phase in the software life cycle, helping identify potential risks and reduce maintenance costs. With the advancement of Large Language Models (LLMs), researchers have proposed an increasing number of LLM-based softwaretesting techniques, particularly in the area of test case generation. Despite the growing interest, limited efforts have been made to thoroughly evaluate the actual capabilities of LLMs in this task. In this paper, we introduce TestBench, a benchmark for class-level LLM-based test case generation. We construct a dataset of 108 Java programs from 9 real-world, large-scale projects on GitHub, each representing a different thematic domain. We then design three distinct types of prompts based on context descriptions, including self-contained context, full context, and simple context. Besides, we propose a fine-grained evaluation framework that considers five aspects of test cases: syntactic correctness, compilation correctness, test correctness, code coverage rate, and defect detection rate. Furthermore, we propose a heuristic algorithm to repair erroneous test cases generated by LLMs. We evaluate CodeLlama-13b, GPT-3.5, and GPT-4 on the TestBench, and our experimental results indicate that larger models demonstrate a greater ability to effectively utilize contextual information, thus generating higher-quality test cases. Smaller models may struggle with the noise introduced by the extensive information contained within the full context. However, when using the simplified version, namely the simple context, which is derived from the full context via abstract syntax tree analysis, the performance of these models improves significantly. Our analysis highlights the current progress and pinpoints future directions to further enhance the effectiveness of models by handling contextual information for test case generation.",,originally announced September 2024.
https://arxiv.org/abs/2409.09271,Python Symbolic Execution with LLM-powered Code Generation,"Wenhan Wang, Kaibo Liu, An Ran Chen, Ge Li, Zhi Jin, Gang Huang, Lei Ma",https://arxiv.org/pdf/2409.09271,"Symbolic execution is a key technology in softwaretesting, which generates test cases by collecting symbolic path constraints and then solving constraints with SMT solvers. Symbolic execution has been proven helpful in generating high-coverage test cases, but its limitations, e.g., the difficulties in solving path constraints, prevent it from broader usage in softwaretesting. Moreover, symbolic execution has encountered many difficulties when applied to dynamically typed languages like Python, because it is extremely challenging to translate the flexible Python grammar into rigid solvers. To overcome the main challenges of applying symbolic execution in Python, we proposed an LLM-empowered agent, LLM-Sym, that automatically calls an SMT solver, Z3, to solve execution path constraints. Based on an introductory-level symbolic execution engine, our LLM agent can extend it to supporting programs with complex data type `list'. The core contribution of LLM-Sym is translating complex Python path constraints into Z3 code. To enable accurate path-to-Z3 translation, we design a multiple-step code generation pipeline including type inference, retrieval and self-refine. Our experiments demonstrate that LLM-Sym is capable of solving path constraints on Leetcode problems with complicated control flows and list data structures, which is impossible for the backbone symbolic execution engine. Our approach paves the way for the combination of the generation ability of LLMs with the reasoning ability of symbolic solvers, and opens up new opportunities in LLM-augmented test case generation.",,originally announced September 2024.
https://arxiv.org/abs/2409.06416,Exploring the Integration of Large Language Models in Industrial Test Maintenance Processes,"Jingxiong Liu, Ludvig Lemner, Linnea Wahlgren, Gregory Gay, Nasser Mohammadiha, Joakim Wennerberg",https://arxiv.org/pdf/2409.06416,"Much of the cost and effort required during the softwaretesting process is invested in performing test maintenance - the addition, removal, or modification of test cases to keep the test suite in sync with the system-under-test or to otherwise improve its quality. Tool support could reduce the cost - and improve the quality - of test maintenance by automating aspects of the process or by providing guidance and support to developers. In this study, we explore the capabilities and applications of large language models (LLMs) - complex machine learning models adapted to textual analysis - to support test maintenance. We conducted a case study at Ericsson AB where we explore the triggers that indicate the need for test maintenance, the actions that LLMs can take, and the considerations that must be made when deploying LLMs in an industrial setting. We also propose and demonstrate a multi-agent architecture that can predict which tests require maintenance following a change to the source code. Collectively, these contributions advance our theoretical and practical understanding of how LLMs can be deployed to benefit industrial test maintenance processes.",,"v1 submitted 10 September, 2024"
https://arxiv.org/abs/2406.12952,SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents,"Niels Mündler, Mark Niklas Müller, Jingxuan He, Martin Vechev",https://arxiv.org/pdf/2406.12952,"Rigorous softwaretesting is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests. We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent. We release all data and code at https://github.com/logic-star-ai/SWT-Bench",,"v1 submitted 18 June, 2024"
https://arxiv.org/abs/2406.11339,Unveiling Assumptions: Exploring the Decisions of AI Chatbots and Human Testers,Francisco Gomes de Oliveira Neto,https://arxiv.org/pdf/2406.11339,"The integration of Large Language Models (LLMs) and chatbots introduces new challenges and opportunities for decision-making in softwaretesting. Decision-making relies on a variety of information, including code, requirements specifications, and other software artifacts that are often unclear or exist solely in the developer's mind. To fill in the gaps left by unclear information, we often rely on assumptions, intuition, or previous experiences to make decisions. This paper explores the potential of LLM-based chatbots like Bard, Copilot, and ChatGPT, to support software testers in test decisions such as prioritizing test cases effectively. We investigate whether LLM-based chatbots and human testers share similar ""assumptions"" or intuition in prohibitive testing scenarios where exhaustive execution of test cases is often impractical. Preliminary results from a survey of 127 testers indicate a preference for diverse test scenarios, with a significant majority (96%) favoring dissimilar test sets. Interestingly, two out of four chatbots mirrored this preference, aligning with human intuition, while the others opted for similar test scenarios, chosen by only 3.9% of testers. Our initial insights suggest a promising avenue within the context of enhancing the collaborative dynamics between testers and chatbots.",,originally announced June 2024.
https://arxiv.org/abs/2406.04531,TESTEVAL: Benchmarking Large Language Models for Test Case Generation,"Wenhan Wang, Chenyuan Yang, Zhijie Wang, Yuheng Huang, Zhaoyang Chu, Da Song, Lingming Zhang, An Ran Chen, Lei Ma",https://arxiv.org/pdf/2406.04531,"Testing plays a crucial role in the software development cycle, enabling the detection of bugs, vulnerabilities, and other undesirable behaviors. To perform softwaretesting, testers need to write code snippets that execute the program under test. Recently, researchers have recognized the potential of large language models (LLMs) in softwaretesting. However, there remains a lack of fair comparisons between different LLMs in terms of test case generation capabilities. In this paper, we propose TESTEVAL, a novel benchmark for test case generation with LLMs. We collect 210 Python programs from an online programming platform, LeetCode, and design three different tasks: overall coverage, targeted line/branch coverage, and targeted path coverage. We further evaluate sixteen popular LLMs, including both commercial and open-source ones, on TESTEVAL. We find that generating test cases to cover specific program lines/branches/paths is still challenging for current LLMs, indicating a lack of ability to comprehend program logic and execution paths. We have open-sourced our dataset and benchmark pipelines at https://github.com/LLM4SoftwareTesting/TestEval.",,"v1 submitted 6 June, 2024"
https://arxiv.org/abs/2404.10304,LLM-Powered Test Case Generation for Detecting Bugs in Plausible Programs,"Kaibo Liu, Zhenpeng Chen, Yiyang Liu, Jie M. Zhang, Mark Harman, Yudong Han, Yun Ma, Yihong Dong, Ge Li, Gang Huang",https://arxiv.org/pdf/2404.10304,"Detecting tricky bugs in plausible programs, those that pass existing test suites yet still contain bugs, remains a significant challenge in softwaretesting. To address this problem, we propose TrickCatcher, an LLM-powered approach to generating test cases for uncovering bugs in plausible programs. TrickCatcher operates in three stages: First, it uses an LLM to generate program variants based on the program under test (PUT) and its specification. Second, it employs an LLM to construct an input generator from the specification for producing test inputs. Finally, these inputs are executed on both the PUT and its program variants to detect inconsistencies in their outputs. We evaluate TrickCatcher on two datasets, TrickyBugs and EvalPlus, which include 366 human-written and 151 AI-generated plausible programs with tricky bugs. TrickCatcher achieves recall, precision, and F1 scores that are 1.80x, 2.65x, and 1.66x those of the state-of-the-art baselines, respectively. Code and data used are available at https://github.com/RinCloud/TrickCatcher.",,"v1 submitted 16 April, 2024"
https://arxiv.org/abs/2402.03396,UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing,"Yifeng He, Jiabo Huang, Yuyang Rong, Yiwen Guo, Ethan Wang, Hao Chen",https://arxiv.org/pdf/2402.03396,"The remarkable capability of large language models (LLMs) in generating high-quality code has drawn increasing attention in the softwaretesting community. However, existing codeLLMs often demonstrate unsatisfactory capabilities in generating accurate and complete tests since they were trained on code snippets collected without differentiating between code for testing purposes and other code. In this paper, we present a large-scale dataset UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test Synthesis. Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified. By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics that tend to be fragile and difficult to scale. It contains 2.7 million focal-test pairs across five mainstream programming languages, making it possible to be utilized for enhancing the test generation ability of LLMs. The details of UniTSyn can be found in Table 1. Our experiments demonstrate that, by building an autoregressive model based on UniTSyn, we can achieve significant benefits in learning and understanding unit test representations, resulting in improved generation accuracy and code coverage across all evaluated programming languages. Code and data will be publicly available.",,originally announced February 2024.
https://arxiv.org/abs/2402.00097,Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM,"Gabriel Ryan, Siddhartha Jain, Mingyue Shang, Shiqi Wang, Xiaofei Ma, Murali Krishna Ramanathan, Baishakhi Ray",https://arxiv.org/pdf/2402.00097,"Testing plays a pivotal role in ensuring software quality, yet conventional Search Based SoftwareTesting (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent works using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt's approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.",,"v1 submitted 31 January, 2024"
https://arxiv.org/abs/2312.04860,Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in SoftwareTesting,"Robson Santos, Italo Santos, Cleyton Magalhaes, Ronnie de Souza Santos",https://arxiv.org/pdf/2312.04860,"A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates coherent content, including grammatically precise sentences, human-like paragraphs, and syntactically accurate code snippets. LLMs can play a pivotal role in software development, including softwaretesting. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in softwaretesting within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts, specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, softwaretesting professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.",,originally announced December 2023.
https://arxiv.org/abs/2310.06320,Automatic Generation of Test Cases based on Bug Reports: a Feasibility Study with Large Language Models,"Laura Plein, Wendkûuni C. Ouédraogo, Jacques Klein, Tegawendé F. Bissyandé",https://arxiv.org/pdf/2310.06320,"Softwaretesting is a core discipline in software engineering where a large array of research results has been produced, notably in the area of automatic test generation. Because existing approaches produce test cases that either can be qualified as simple (e.g. unit tests) or that require precise specifications, most testing procedures still rely on test cases written by humans to form test suites. Such test suites, however, are incomplete: they only cover parts of the project or they are produced after the bug is fixed. Yet, several research challenges, such as automatic program repair, and practitioner processes, build on the assumption that available test suites are sufficient. There is thus a need to break existing barriers in automatic test case generation. While prior work largely focused on random unit testing inputs, we propose to consider generating test cases that realistically represent complex user execution scenarios, which reveal buggy behaviour. Such scenarios are informally described in bug reports, which should therefore be considered as natural inputs for specifying bug-triggering test cases. In this work, we investigate the feasibility of performing this generation by leveraging large language models (LLMs) and using bug reports as inputs. Our experiments include the use of ChatGPT, as an online service, as well as CodeGPT, a code-related pre-trained LLM that was fine-tuned for our task. Overall, we experimentally show that bug reports associated to up to 50% of Defects4J bugs can prompt ChatGPT to generate an executable test case. We show that even new bug reports can indeed be used as input for generating executable test cases. Finally, we report experimental results which confirm that LLM-generated test cases are immediately useful in software engineering tasks such as fault localization as well as patch validation in automated program repair.",,originally announced October 2023.
https://arxiv.org/abs/2308.16557,Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing,"Arghavan Moradi Dakhel, Amin Nikanjam, Vahid Majdinasab, Foutse Khomh, Michel C. Desmarais",https://arxiv.org/pdf/2308.16557,"One of the critical phases in software development is softwaretesting. Testing helps with identifying potential bugs and reducing maintenance costs. The goal of automated test generation tools is to ease the development of tests by suggesting efficient bug-revealing tests. Recently, researchers have leveraged Large Language Models (LLMs) of code to generate unit tests. While the code coverage of generated tests was usually assessed, the literature has acknowledged that the coverage is weakly correlated with the efficiency of tests in bug detection. To improve over this limitation, in this paper, we introduce MuTAP for improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing. Our goal is achieved by augmenting prompts with surviving mutants, as those mutants highlight the limitations of test cases in detecting bugs. MuTAP is capable of generating effective test cases in the absence of natural language descriptions of the Program Under Test (PUTs). We employ different LLMs within MuTAP and evaluate their performance on different benchmarks. Our results show that our proposed method is able to detect up to 28% more faulty human-written code snippets. Among these, 17% remained undetected by both the current state-of-the-art fully automated test generation tool (i.e., Pynguin) and zero-shot/few-shot learning approaches on LLMs. Furthermore, MuTAP achieves a Mutation Score (MS) of 93.57% on synthetic buggy code, outperforming all other approaches in our evaluation. Our findings suggest that although LLMs can serve as a useful tool to generate test cases, they require specific post-processing steps to enhance the effectiveness of the generated test cases which may suffer from syntactic or functional errors and may be ineffective in detecting certain types of bugs and testing corner cases PUTs.",,originally announced August 2023.
https://arxiv.org/abs/2307.04346,Can Large Language Models Write Good Property-Based Tests?,"Vasudev Vikram, Caroline Lemieux, Joshua Sunshine, Rohan Padhye",https://arxiv.org/pdf/2307.04346,"Property-based testing (PBT), while an established technique in the softwaretesting research community, is still relatively underused in real-world software. Pain points in writing property-based tests include implementing diverse random input generators and thinking of meaningful properties to test. Developers, however, are more amenable to writing documentation; plenty of library API documentation is available and can be used as natural language specifications for PBTs. As large language models (LLMs) have recently shown promise in a variety of coding tasks, we investigate using modern LLMs to automatically synthesize PBTs using two prompting techniques. A key challenge is to rigorously evaluate the LLM-synthesized PBTs. We propose a methodology to do so considering several properties of the generated tests: (1) validity, (2) soundness, and (3) property coverage, a novel metric that measures the ability of the PBT to detect property violations through generation of property mutants. In our evaluation on 40 Python library API methods across three models (GPT-4, Gemini-1.5-Pro, Claude-3-Opus), we find that with the best model and prompting approach, a valid and sound PBT can be synthesized in 2.4 samples on average. We additionally find that our metric for determining soundness of a PBT is aligned with human judgment of property assertions, achieving a precision of 100% and recall of 97%. Finally, we evaluate the property coverage of LLMs across all API methods and find that the best model (GPT-4) is able to automatically synthesize correct PBTs for 21% of properties extractable from API documentation.",,"v1 submitted 10 July, 2023"
https://arxiv.org/abs/2304.01397,LTM: Scalable and Black-box Similarity-based Test Suite Minimization based on Language Models,"Rongqi Pan, Taher A. Ghaleb, Lionel Briand",https://arxiv.org/pdf/2304.01397,"Test suites tend to grow when software evolves, making it often infeasible to execute all test cases with the allocated testing budgets, especially for large software systems. Test suite minimization (TSM) is employed to improve the efficiency of softwaretesting by removing redundant test cases, thus reducing testing time and resources, while maintaining the fault detection capability of the test suite. Most existing TSM approaches rely on code coverage (white-box) or model-based features, which are not always available to test engineers. Recent TSM approaches that rely only on test code (black-box) have been proposed, such as ATM and FAST-R. To address the scalability, we propose LTM (Language model-based Test suite Minimization), a novel, scalable, and black-box similarity-based TSM approach based on large language models (LLMs), which is the first application of LLMs in the context of TSM. To support similarity measurement for test code embeddings, we investigate five pre-trained language models: CodeBERT, GraphCodeBERT, UniXcoder, StarEncoder, and CodeLlama, on which we compute two similarity measures: Cosine Similarity and Euclidean Distance. Our goal is to find similarity measures that are not only computationally more efficient but can also better guide a Genetic Algorithm (GA) to search for optimal minimized test suites, thus reducing the overall search time. Experimental results show that the best configuration of LTM (UniXcoder/Cosine) outperforms ATM in three aspects: (a) achieving a slightly greater saving rate of testing time (41.72% versus 41.02%, on average); (b) attaining a significantly higher fault detection rate (0.84 versus 0.81, on average); and, most importantly, (c) minimizing test suites nearly five times faster on average, with higher gains for larger test suites and systems, thus achieving much higher scalability.",,"v1 submitted 3 April, 2023"
https://arxiv.org/abs/2504.07244,Acceptance Test Generation with Large Language Models: An Industrial Case Study,"Margarida Ferreira, Luis Viegas, Joao Pascoal Faria, Bruno Lima",https://arxiv.org/pdf/2504.07244,"Large language model (LLM)-powered assistants are increasingly used for generating program code and unit tests, but their application in acceptance testing remains underexplored. To help address this gap, this paper explores the use of LLMs for generating executable acceptance tests for web applications through a two-step process: (i) generating acceptance test scenarios in natural language (in Gherkin) from user stories, and (ii) converting these scenarios into executable test scripts (in Cypress), knowing the HTML code of the pages under test. This two-step approach supports acceptance test-driven development, enhances tester control, and improves test quality. The two steps were implemented in the AutoUAT and Test Flow tools, respectively, powered by GPT-4 Turbo, and integrated into a partner company's workflow and evaluated on real-world projects. The users found the acceptance test scenarios generated by AutoUAT helpful 95% of the time, even revealing previously overlooked cases. Regarding Test Flow, 92% of the acceptance testcasesgenerated by Test Flow were considered helpful: 60% were usable as generated, 8% required minor fixes, and 24% needed to be regenerated with additional inputs; the remaining 8% were discarded due to major issues. These results suggest that LLMs can,in fact, help improve the acceptance test process with appropriate tooling and supervision.",,originally announced April 2025.
https://arxiv.org/abs/2405.09965,Large Language Models for Automated Web-Form-Test Generation: An Empirical Study,"Tao Li, Chenhui Cui, Rubing Huang, Dave Towey, Lei Ma",https://arxiv.org/pdf/2405.09965,"Testing web forms is an essential activity for ensuring the quality of web applications. It typically involves evaluating the interactions between users and forms. Automated test-casegeneration remains a challenge for web-form testing: Due to the complex, multi-level structure of web pages, it can be difficult to automatically capture their inherent contextual information for inclusion in the tests. Large Language Models (LLMs) have shown great potential for contextual text generation. This motivated us to explore how they could generate automated tests for web forms, making use of the contextual information within form elements. To the best of our knowledge, no comparative study examining different LLMs has yet been reported for web-form-test generation. To address this gap in the literature, we conducted a comprehensive empirical study investigating the effectiveness of 11 LLMs on 146 web forms from 30 open-source Java web applications. In addition, we propose three HTML-structure-pruning methods to extract key contextual information. The experimental results show that different LLMs can achieve different testing effectiveness. Compared with GPT-4, the other LLMs had difficulty generating appropriate tests for the web forms: Their successfully-submitted rates (SSRs) decreased by 9.10% to 74.15%. Our findings also show that, for all LLMs, when the designed prompts include complete and clear contextual information about the web forms, more effective web-form tests were generated. Specifically, when using Parser-Processed HTML for Task Prompt (PH-P), the SSR averaged 70.63%, higher than the 60.21% for Raw HTML for Task Prompt (RH-P) and 50.27% for LLM-Processed HTML for Task Prompt (LH-P). Finally, this paper also highlights strategies for selecting LLMs based on performance metrics, and for optimizing the prompt design to improve the quality of the web-form tests.",,"v1 submitted 16 May, 2024"
https://arxiv.org/abs/2506.07524,IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents,"Shiwei Feng, Xiangzhe Xu, Xuan Chen, Kaiyuan Zhang, Syed Yusuf Ahmed, Zian Su, Mingwei Zheng, Xiangyu Zhang",https://arxiv.org/pdf/2506.07524,"LLM agents are increasingly deployed to automate real-world tasks by invoking APIs through natural language instructions. While powerful, they often suffer from misinterpretation of user intent, leading to the agent's actions that diverge from the user's intended goal, especially as external toolkits evolve. Traditional softwaretesting assumes structured inputs and thus falls short in handling the ambiguity of natural language. We introduce IntenTest, an API-centric stress testing framework that systematically uncovers intent integrity violations in LLM agents. Unlike prior work focused on fixed benchmarks or adversarial inputs, IntenTest generates realistic tasks based on toolkits' documentation and applies targeted mutations to expose subtle agent errors while preserving user intent. To guide testing, we propose semantic partitioning, which organizes natural language tasks into meaningful categories based on toolkit API parameters and their equivalence classes. Within each partition, seed tasks are mutated and ranked by a lightweight predictor that estimates the likelihood of triggering agent errors. To enhance efficiency, IntenTest maintains a datatype-aware strategy memory that retrieves and adapts effective mutation patterns from past cases. Experiments on 80 toolkit APIs demonstrate that IntenTest effectively uncovers intent integrity violations, significantly outperforming baselines in both error-exposing rate and query efficiency. Moreover, IntenTest generalizes well to stronger target models using smaller LLMs for test generation, and adapts to evolving APIs across domains.",,originally announced June 2025.
https://arxiv.org/abs/2504.18985,Tracking the Moving Target: A Framework for Continuous Evaluation of LLM Test Generation in Industry,"Maider Azanza, Beatriz Pérez Lamancha, Eneko Pizarro",https://arxiv.org/pdf/2504.18985,"Large Language Models (LLMs) have shown great potential in automating softwaretesting tasks, including test generation. However, their rapid evolution poses a critical challenge for companies implementing DevSecOps - evaluations of their effectiveness quickly become outdated, making it difficult to assess their reliability for production use. While academic research has extensively studied LLM-based test generation, evaluations typically provide point-in-time analyses using academic benchmarks. Such evaluations do not address the practical needs of companies who must continuously assess tool reliability and integration with existing development practices. This work presents a measurement framework for the continuous evaluation of commercial LLM test generators in industrial environments. We demonstrate its effectiveness through a longitudinal study at LKS Next. The framework integrates with industry-standard tools like SonarQube and provides metrics that evaluate both technical adequacy (e.g., test coverage) and practical considerations (e.g., maintainability or expert assessment). Our methodology incorporates strategies for test case selection, prompt engineering, and measurement infrastructure, addressing challenges such as data leakage and reproducibility. Results highlight both the rapid evolution of LLM capabilities and critical factors for successful industrial adoption, offering practical guidance for companies seeking to integrate these technologies into their development pipelines.",,originally announced April 2025.
https://arxiv.org/abs/2504.18827,Test It Before You Trust It: Applying SoftwareTesting for Trustworthy In-context Learning,"Teeradaj Racharak, Chaiyong Ragkhitwetsagul, Chommakorn Sontesadisai, Thanwadee Sunetnanta",https://arxiv.org/pdf/2504.18827,"In-context learning (ICL) has emerged as a powerful capability of large language models (LLMs), enabling them to perform new tasks based on a few provided examples without explicit fine-tuning. Despite their impressive adaptability, these models remain vulnerable to subtle adversarial perturbations and exhibit unpredictable behavior when faced with linguistic variations. Inspired by softwaretesting principles, we introduce a softwaretesting-inspired framework, called MMT4NL, for evaluating the trustworthiness of in-context learning by utilizing adversarial perturbations and softwaretesting techniques. It includes diverse evaluation aspects of linguistic capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around the idea of crafting metamorphic adversarial examples from a test set in order to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is to treat any LLM as software and validate its functionalities just like testing the software. Finally, we demonstrate applications of MMT4NL on the sentiment analysis and question-answering tasks. Our experiments could reveal various linguistic bugs in state-of-the-art LLMs.",,"v1 submitted 26 April, 2025"
https://arxiv.org/abs/2503.14000,LLM-based Unit Test Generation for Dynamically-Typed Programs,"Runlin Liu, Zhe Zhang, Yunge Hu, Yuhang Lin, Xiang Gao, Hailong Sun",https://arxiv.org/pdf/2503.14000,"Automated unit test generation has been widely studied, but generating effective tests for dynamically typed programs remains a significant challenge. Existing approaches, including search-based softwaretesting (SBST) and recent LLM-based methods, often suffer from type errors, leading to invalid inputs and assertion failures, ultimately reducing testing effectiveness. To address this, we propose TypeTest, a novel framework that enhances type correctness in test generation through a vector-based Retrieval-Augmented Generation (RAG) system. TypeTest employs call instance retrieval and feature-based retrieval to infer parameter types accurately and construct valid test inputs. Furthermore, it utilizes the call graph to extract richer contextual information, enabling more accurate assertion generation. In addition, TypeTest incorporates a repair mechanism and iterative test generation, progressively refining test cases to improve coverage. In an evaluation on 125 real-world Python modules, TypeTest achieved an average statement coverage of 86.6% and branch coverage of 76.8%, outperforming state-of-theart tools by 5.4% and 9.3%, respectively.",,originally announced March 2025.
https://arxiv.org/abs/2503.04730,WinClick: GUI Grounding with Multimodal Large Language Models,"Zheng Hui, Yinheng Li, Dan zhao, Tianyi Chen, Colby Banbury, Kazuhito Koishida",https://arxiv.org/pdf/2503.04730,"Graphical User Interface (GUI) tasks are vital for automating workflows such as softwaretesting, user interface navigation. For users, the GUI is the most intuitive platform for interacting with a computer. Previous work identified a key challenge in developing visual GUI agents: GUI grounding - the ability to accurately locate screen elements based on instructions. However, most existing GUI agents rely on structured data formats like DOM or HTML files in training or inferencing, which are inaccessible across all applications, particular in a general desktop environments such as Windows OS. To address this, we introduce WinClick, a novel visual GUI agent developed in Windows platform. WinClick leverages screenshots to detect actionable regions. To overcome the challenge of GUI grounding, we enhance WinClick with GUI grounding pre-training and propose an LLM-based method for aligning GUI grounding data. Additionally, we introduce WinSpot, the first comprehensive benchmark for GUI grounding on Windows. Our experiments demonstrate that WinClick, combined with GUI grounding pre-training, significantly outperforms existing baselines, offering a scalable solution for GUI automation in desktop environments. WinSpot is publicly available at https://github.com/zackhuiiiii/WinSpot.",,originally announced March 2025.
https://arxiv.org/abs/2503.00795,Towards Reliable LLM-Driven Fuzz Testing: Vision and Road Ahead,"Yiran Cheng, Hong Jin Kang, Lwin Khin Shar, Chaopeng Dong, Zhiqiang Shi, Shichao Lv, Limin Sun",https://arxiv.org/pdf/2503.00795,"Fuzz testing is a crucial component of software security assessment, yet its effectiveness heavily relies on valid fuzz drivers and diverse seed inputs. Recent advancements in Large Language Models (LLMs) offer transformative potential for automating fuzz testing (LLM4Fuzz), particularly in generating drivers and seeds. However, current LLM4Fuzz solutions face critical reliability challenges, including low driver validity rates and seed quality trade-offs, hindering their practical adoption. This paper aims to examine the reliability bottlenecks of LLM-driven fuzzing and explores potential research directions to address these limitations. It begins with an overview of the current development of LLM4SE and emphasizes the necessity for developing reliable LLM4Fuzz solutions. Following this, the paper envisions a vision where reliable LLM4Fuzz transforms the landscape of softwaretesting and security for industry, software development practitioners, and economic accessibility. It then outlines a road ahead for future research, identifying key challenges and offering specific suggestions for the researchers to consider. This work strives to spark innovation in the field, positioning reliable LLM4Fuzz as a fundamental component of modern softwaretesting.",,originally announced March 2025.
https://arxiv.org/abs/2502.20812,Towards Reliable Vector Database Management Systems: A SoftwareTesting Roadmap for 2030,"Shenao Wang, Yanjie Zhao, Yinglin Xie, Zhao Liu, Xinyi Hou, Quanchen Zou, Haoyu Wang",https://arxiv.org/pdf/2502.20812,"The rapid growth of Large Language Models (LLMs) and AI-driven applications has propelled Vector Database Management Systems (VDBMSs) into the spotlight as a critical infrastructure component. VDBMS specializes in storing, indexing, and querying dense vector embeddings, enabling advanced LLM capabilities such as retrieval-augmented generation, long-term memory, and caching mechanisms. However, the explosive adoption of VDBMS has outpaced the development of rigorous softwaretesting methodologies tailored for these emerging systems. Unlike traditional databases optimized for structured data, VDBMS face unique testing challenges stemming from the high-dimensional nature of vector data, the fuzzy semantics in vector search, and the need to support dynamic data scaling and hybrid query processing. In this paper, we begin by conducting an empirical study of VDBMS defects and identify key challenges in test input generation, oracle definition, and test evaluation. Drawing from these insights, we propose the first comprehensive research roadmap for developing effective testing methodologies tailored to VDBMS. By addressing these challenges, the softwaretesting community can contribute to the development of more reliable and trustworthy VDBMS, enabling the full potential of LLMs and data-intensive AI applications.",,originally announced February 2025.
https://arxiv.org/abs/2502.09801,Unit Testing Past vs. Present: Examining LLMs' Impact on Defect Detection and Efficiency,"Rudolf Ramler, Philipp Straubinger, Reinhold Plösch, Dietmar Winkler",https://arxiv.org/pdf/2502.09801,"The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into software engineering workflows has shown potential to enhance productivity, particularly in softwaretesting. This paper investigates whether LLM support improves defect detection effectiveness during unit testing. Building on prior studies comparing manual and tool-supported testing, we replicated and extended an experiment where participants wrote unit tests for a Java-based system with seeded defects within a time-boxed session, supported by LLMs. Comparing LLM supported and manual testing, results show that LLM support significantly increases the number of unit tests generated, defect detection rates, and overall testing efficiency. These findings highlight the potential of LLMs to improve testing and defect detection outcomes, providing empirical insights into their practical application in softwaretesting.",,originally announced February 2025.
https://arxiv.org/abs/2502.04008,Automating a Complete SoftwareTest Process Using LLMs: An Automotive Case Study,"Shuai Wang, Yinan Yu, Robert Feldt, Dhasarathy Parthasarathy",https://arxiv.org/pdf/2502.04008,"Vehicle API testing verifies whether the interactions between a vehicle's internal systems and external applications meet expectations, ensuring that users can access and control various vehicle functions and data. However, this task is inherently complex, requiring the alignment and coordination of API systems, communication protocols, and even vehicle simulation systems to develop valid test cases. In practical industrial scenarios, inconsistencies, ambiguities, and interdependencies across various documents and system specifications pose significant challenges. This paper presents a system designed for the automated testing of in-vehicle APIs. By clearly defining and segmenting the testing process, we enable Large Language Models (LLMs) to focus on specific tasks, ensuring a stable and controlled testing workflow. Experiments conducted on over 100 APIs demonstrate that our system effectively automates vehicle API testing. The results also confirm that LLMs can efficiently handle mundane tasks requiring human judgment, making them suitable for complete automation in similar industrial contexts.",,originally announced February 2025.
https://arxiv.org/abs/2502.02866,A Systematic Approach for Assessing Large Language Models' Test Case Generation Capability,"Hung-Fu Chang, Mohammad Shokrolah Shirazi",https://arxiv.org/pdf/2502.02866,"Softwaretesting ensures the quality and reliability of software products, but manual test case creation is labor-intensive. With the rise of large language models (LLMs), there is growing interest in unit test creation with LLMs. However, effective assessment of LLM-generated test cases is limited by the lack of standardized benchmarks that comprehensively cover diverse programming scenarios. To address the assessment of LLM's test case generation ability and lacking dataset for evaluation, we propose the Generated Benchmark from Control-Flow Structure and Variable Usage Composition (GBCV) approach, which systematically generates programs used for evaluating LLMs' test generation capabilities. By leveraging basic control-flow structures and variable usage, GBCV provides a flexible framework to create a spectrum of programs ranging from simple to complex. Because GPT-4o and GPT-3-Turbo are publicly accessible models, to present real-world regular user's use case, we use GBCV to assess LLM performance on them. Our findings indicate that GPT-4o performs better on complex program structures, while all models effectively detect boundary values in simple conditions but face challenges with arithmetic computations. This study highlights the strengths and limitations of LLMs in test generation, provides a benchmark framework, and suggests directions for future improvement.",,originally announced February 2025.
https://arxiv.org/abs/2502.02025,From Accidents to Insights: Leveraging Multimodal Data for Scenario-Driven ADS Testing,"Siwei Luo, Yang Zhang, Yao Deng, Xi Zheng",https://arxiv.org/pdf/2502.02025,"The rapid advancements in Autonomous Driving Systems (ADS) have necessitated robust softwaretesting to ensure safety and reliability. However, automating the generation of scalable and concrete test scenarios remains a significant challenge. Current scenario-based test case generation methods often face limitations, such as unrealistic scenes and inaccurate vehicle trajectories. These challenges largely result from the loss of map information during data extraction and the lack of an effective verification mechanism to mitigate hallucinations in large language models (LLMs). This paper introduces TRACE, a scenario-based ADS Test case Generation framework for Critical Scenarios. By leveraging multimodal data to extract challenging scenarios from real-world car crash reports, TRACE constructs numerous critical test cases with less data, significantly enhancing ADS bug detection efficiency. Using in-context learning, chain-of-thought prompting, and self-validation approaches, we use LLMs to extract environmental and road network information from crash reports. For vehicle trajectory planning, data containing map information and vehicle coordinates serves as a knowledge base to build a ChatGPT-based LLM with path-planning capabilities, which we named TrackMate. Based on 50 existing crash reports, our approach successfully tested three ADS models across two simulation platforms, MetaDrive and BeamNG. Of the 290 constructed test scenarios, 127 are identified as critical, as they resulted in vehicle collisions. Additionally, user feedback reveals that TRACE demonstrates superior scenario reconstruction accuracy, with 77.5% of the scenarios being rated as 'mostly or 'totally' consistent, compared to only 27% for the most related SOTA, LCTGen.",,originally announced February 2025.
https://arxiv.org/abs/2501.14465,Boundary Value Test Input Generation Using Prompt Engineering with LLMs: Fault Detection and Coverage Analysis,"Xiujing Guo, Chen Li, Tatsuhiro Tsuchiya",https://arxiv.org/pdf/2501.14465,"As software systems grow more complex, automated testing has become essential to ensuring reliability and performance. Traditional methods for boundary value test input generation can be time-consuming and may struggle to address all potential error cases effectively, especially in systems with intricate or highly variable boundaries. This paper presents a framework for assessing the effectiveness of large language models (LLMs) in generating boundary value test inputs for white-box softwaretesting by examining their potential through prompt engineering. Specifically, we evaluate the effectiveness of LLM-based test input generation by analyzing fault detection rates and test coverage, comparing these LLM-generated test sets with those produced using traditional boundary value analysis methods. Our analysis shows the strengths and limitations of LLMs in boundary value generation, particularly in detecting common boundary-related issues. However, they still face challenges in certain areas, especially when handling complex or less common test inputs. This research provides insights into the role of LLMs in boundary value testing, underscoring both their potential and areas for improvement in automated testing methods.",,originally announced January 2025.
https://arxiv.org/abs/2501.00217,The Potential of LLMs in Automating SoftwareTesting: From Generation to Reporting,"Betim Sherifi, Khaled Slhoub, Fitzroy Nembhard",https://arxiv.org/pdf/2501.00217,"Having a high quality software is essential in software engineering, which requires robust validation and verification processes during testing activities. Manual testing, while effective, can be time consuming and costly, leading to an increased demand for automated methods. Recent advancements in Large Language Models (LLMs) have significantly influenced software engineering, particularly in areas like requirements analysis, test automation, and debugging. This paper explores an agent-oriented approach to automated softwaretesting, using LLMs to reduce human intervention and enhance testing efficiency. The proposed framework integrates LLMs to generate unit tests, visualize call graphs, and automate test execution and reporting. Evaluations across multiple applications in Python and Java demonstrate the system's high test coverage and efficient operation. This research underscores the potential of LLM-powered agents to streamline softwaretesting workflows while addressing challenges in scalability and accuracy.",,originally announced January 2025.
https://arxiv.org/abs/2412.15254,"RIRO: Reshaping Inputs, Refining Outputs Unlocking the Potential of Large Language Models in Data-Scarce Contexts","Ali Hamdi, Hozaifa Kassab, Mohamed Bahaa, Marwa Mohamed",https://arxiv.org/pdf/2412.15254,"Large language models (LLMs) have significantly advanced natural language processing, excelling in areas like text generation, summarization, and question-answering. Despite their capabilities, these models face challenges when fine-tuned on small, domain-specific datasets, often struggling to generalize and deliver accurate results with unfamiliar inputs. To tackle this issue, we introduce RIRO, a novel two-layer architecture designed to improve performance in data-scarce environments. The first layer leverages advanced prompt engineering to reformulate inputs, ensuring better alignment with training data, while the second layer focuses on refining outputs to minimize inconsistencies. Through fine-tuning models like Phi-2, Falcon 7B, and Falcon 1B, with Phi-2 outperforming the others. Additionally, we introduce a benchmark using evaluation metrics such as cosine similarity, Levenshtein distance, BLEU score, ROUGE-1, ROUGE-2, and ROUGE-L. While these advancements improve performance, challenges like computational demands and overfitting persist, limiting the potential of LLMs in data-scarce, high-stakes environments such as healthcare, legal documentation, and softwaretesting.",,originally announced December 2024.
https://arxiv.org/abs/2411.08254,VALTEST: Automated Validation of Language Model Generated Test Cases,"Hamed Taherkhani, Hadi Hemmati",https://arxiv.org/pdf/2411.08254,"Large Language Models (LLMs) have demonstrated significant potential in automating softwaretesting, specifically in generating unit test cases. However, the validation of LLM-generated test cases remains a challenge, particularly when the ground truth is unavailable. This paper introduces VALTEST, a novel framework designed to automatically validate test cases generated by LLMs by leveraging token probabilities. We evaluate VALTEST using nine test suites generated from three datasets (HumanEval, MBPP, and LeetCode) across three LLMs (GPT-4o, GPT-3.5-turbo, and LLama3.1 8b). By extracting statistical features from token probabilities, we train a machine learning model to predict test case validity. VALTEST increases the validity rate of test cases by 6.2% to 24%, depending on the dataset and LLM. Our results suggest that token probabilities are reliable indicators for distinguishing between valid and invalid test cases, which provides a robust solution for improving the correctness of LLM-generated test cases in softwaretesting. In addition, we found that replacing the identified invalid test cases by VALTEST, using a Chain-of-Thought prompting results in a more effective test suite while keeping the high validity rates.",,originally announced November 2024.
https://arxiv.org/abs/2410.01933,TAEGAN: Generating Synthetic Tabular Data For Data Augmentation,"Jiayu Li, Zilong Zhao, Kevin Yee, Uzair Javaid, Biplab Sikdar",https://arxiv.org/pdf/2410.01933,"Synthetic tabular data generation has gained significant attention for its potential in data augmentation, softwaretesting and privacy-preserving data sharing. However, most research has primarily focused on larger datasets and evaluating their quality in terms of metrics like column-wise statistical distributions and inter-feature correlations, while often overlooking its utility for data augmentation, particularly for datasets whose data is scarce. In this paper, we propose Tabular Auto-Encoder Generative Adversarial Network (TAEGAN), an improved GAN-based framework for generating high-quality tabular data. Although large language models (LLMs)-based methods represent the state-of-the-art in synthetic tabular data generation, they are often overkill for small datasets due to their extensive size and complexity. TAEGAN employs a masked auto-encoder as the generator, which for the first time introduces the power of self-supervised pre-training in tabular data generation so that essentially exposes the networks to more information. We extensively evaluate TAEGAN against five state-of-the-art synthetic tabular data generation algorithms. Results from 10 datasets show that TAEGAN outperforms existing deep-learning-based tabular data generation models on 9 out of 10 datasets on the machine learning efficacy and achieves superior data augmentation performance on 7 out of 8 smaller datasets.",,originally announced October 2024.
https://arxiv.org/abs/2409.12405,On the Effectiveness of LLMs for Manual Test Verifications,"Myron David Lucena Campos Peixoto, Davy de Medeiros Baia, Nathalia Nascimento, Paulo Alencar, Baldoino Fonseca, Márcio Ribeiro",https://arxiv.org/pdf/2409.12405,"Background: Manual testing is vital for detecting issues missed by automated tests, but specifying accurate verifications is challenging. Aims: This study aims to explore the use of Large Language Models (LLMs) to produce verifications for manual tests. Method: We conducted two independent and complementary exploratory studies. The first study involved using 2 closed-source and 6 open-source LLMs to generate verifications for manual test steps and evaluate their similarity to original verifications. The second study involved recruiting softwaretesting professionals to assess their perception and agreement with the generated verifications compared to the original ones. Results: The open-source models Mistral-7B and Phi-3-mini-4k demonstrated effectiveness and consistency comparable to closed-source models like Gemini-1.5-flash and GPT-3.5-turbo in generating manual test verifications. However, the agreement level among professional testers was slightly above 40%, indicating both promise and room for improvement. While some LLM-generated verifications were considered better than the originals, there were also concerns about AI hallucinations, where verifications significantly deviated from expectations. Conclusion: We contributed by generating a dataset of 37,040 test verifications using 8 different LLMs. Although the models show potential, the relatively modest 40% agreement level highlights the need for further refinement. Enhancing the accuracy, relevance, and clarity of the generated verifications is crucial to ensure greater reliability in real-world testing scenarios.",,originally announced September 2024.
https://arxiv.org/abs/2409.00551,"Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness",Wenxuan Wang,https://arxiv.org/pdf/2409.00551,"Large language models (LLMs), such as ChatGPT, have rapidly penetrated into people's work and daily lives over the past few years, due to their extraordinary conversational skills and intelligence. ChatGPT has become the fastest-growing software in terms of user numbers in human history and become an important foundational model for the next generation of artificial intelligence applications. However, the generations of LLMs are not entirely reliable, often producing content with factual errors, biases, and toxicity. Given their vast number of users and wide range of application scenarios, these unreliable responses can lead to many serious negative impacts. This thesis introduces the exploratory works in the field of language model reliability during the PhD study, focusing on the correctness, non-toxicity, and fairness of LLMs from both softwaretesting and natural language processing perspectives. First, to measure the correctness of LLMs, we introduce two testing frameworks, FactChecker and LogicAsker, to evaluate factual knowledge and logical reasoning accuracy, respectively. Second, for the non-toxicity of LLMs, we introduce two works for red-teaming LLMs. Third, to evaluate the fairness of LLMs, we introduce two evaluation frameworks, BiasAsker and XCulturalBench, to measure the social bias and cultural bias of LLMs, respectively.",,originally announced September 2024.
https://arxiv.org/abs/2408.09785,GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive Software Release Decision-Making,"Arsham Gholamzadeh Khoee, Yinan Yu, Robert Feldt, Andris Freimanis, Patrick Andersson Rhodin, Dhasarathy Parthasarathy",https://arxiv.org/pdf/2408.09785,"Traditional methods for making software deployment decisions in the automotive industry typically rely on manual analysis of tabular softwaretest data. These methods often lead to higher costs and delays in the software release cycle due to their labor-intensive nature. Large Language Models (LLMs) present a promising solution to these challenges. However, their application generally demands multiple rounds of human-driven prompt engineering, which limits their practical deployment, particularly for industrial end-users who need reliable and efficient results. In this paper, we propose GoNoGo, an LLM agent system designed to streamline automotive software deployment while meeting both functional requirements and practical industrial constraints. Unlike previous systems, GoNoGo is specifically tailored to address domain-specific and risk-sensitive systems. We evaluate GoNoGo's performance across different task difficulties using zero-shot and few-shot examples taken from industrial practice. Our results show that GoNoGo achieves a 100% success rate for tasks up to Level 2 difficulty with 3-shot examples, and maintains high performance even for more complex tasks. We find that GoNoGo effectively automates decision-making for simpler tasks, significantly reducing the need for manual intervention. In summary, GoNoGo represents an efficient and user-friendly LLM-based solution currently employed in our industrial partner's company to assist with software release decision-making, supporting more informed and timely decisions in the release process for risk-sensitive vehicle systems.",,"v1 submitted 19 August, 2024"
https://arxiv.org/abs/2407.21429,Chat-like Asserts Prediction with the Support of Large Language Model,"Han Wang, Han Hu, Chunyang Chen, Burak Turhan",https://arxiv.org/pdf/2407.21429,"Unit testing is an essential component of softwaretesting, with the assert statements playing an important role in determining whether the tested function operates as expected. Although research has explored automated test case generation, generating meaningful assert statements remains an ongoing challenge. While several studies have investigated assert statement generation in Java, limited work addresses this task in popular dynamically-typed programming languages like Python. In this paper, we introduce Chat-like execution-based Asserts Prediction (\tool), a novel Large Language Model-based approach for generating meaningful assert statements for Python projects. \tool utilizes the persona, Chain-of-Thought, and one-shot learning techniques in the prompt design, and conducts rounds of communication with LLM and Python interpreter to generate meaningful assert statements. We also present a Python assert statement dataset mined from GitHub. Our evaluation demonstrates that \tool achieves 64.7\% accuracy for single assert statement generation and 62\% for overall assert statement generation, outperforming the existing approaches. We also analyze the mismatched assert statements, which may still share the same functionality and discuss the potential help \tool could offer to the automated Python unit test generation. The findings indicate that \tool has the potential to benefit the SE community through more practical usage scenarios.",,originally announced July 2024.
https://arxiv.org/abs/2405.12766,Test Oracle Automation in the era of LLMs,"Facundo Molina, Alessandra Gorla",https://arxiv.org/pdf/2405.12766,"The effectiveness of a test suite in detecting faults highly depends on the correctness and completeness of its test oracles. Large Language Models (LLMs) have already demonstrated remarkable proficiency in tackling diverse softwaretesting tasks, such as automated test generation and program repair. This paper aims to enable discussions on the potential of using LLMs for test oracle automation, along with the challenges that may emerge during the generation of various types of oracles. Additionally, our aim is to initiate discussions on the primary threats that SE researchers must consider when employing LLMs for oracle automation, encompassing concerns regarding oracle deficiencies and data leakages.",,originally announced May 2024.
https://arxiv.org/abs/2404.13945,How Multi-Modal LLMs Reshape Visual Deep Learning Testing? A Comprehensive Study Through the Lens of Image Mutation,"Liwen Wang, Yuanyuan Yuan, Ao Sun, Zongjie Li, Pingchuan Ma, Daoyuan Wu, Shuai Wang",https://arxiv.org/pdf/2404.13945,"Visual deep learning (VDL) systems have shown significant success in real-world applications like image recognition, object detection, and autonomous driving. To evaluate the reliability of VDL, a mainstream approach is softwaretesting, which requires diverse mutations over image semantics. The rapid development of multi-modal large language models (MLLMs) has introduced revolutionary image mutation potentials through instruction-driven methods. Users can now freely describe desired mutations and let MLLMs generate the mutated images. Hence, parallel to large language models' (LLMs) recent success in traditional software fuzzing, one may also expect MLLMs to be promising for VDL testing in terms of offering unified, diverse, and complex image mutations. However, the quality and applicability of MLLM-based mutations in VDL testing remain largely unexplored. We present the first study, aiming to assess MLLMs' adequacy from 1) the semantic validity of MLLM mutated images, 2) the alignment of MLLM mutated images with their text instructions (prompts), and 3) the faithfulness of how different mutations preserve semantics that are ought to remain unchanged. With large-scale human studies and quantitative evaluations, we identify MLLM's promising potentials in expanding the covered semantics of image mutations. Notably, while SoTA MLLMs (e.g., GPT-4V) fail to support or perform worse in editing existing semantics in images (as in traditional mutations like rotation), they generate high-quality test inputs using ""semantic-replacement"" mutations (e.g., ""dress a dog with clothes""), which bring extra semantics to images; these were infeasible for past approaches. Hence, we view MLLM-based mutations as a vital complement to traditional mutations, and advocate future VDL testing tasks to combine MLLM-based methods and traditional image mutations for comprehensive and reliable testing.",,"v1 submitted 22 April, 2024"
https://arxiv.org/abs/2404.09384,Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches,"Víctor A. Braberman, Flavia Bonomo-Braberman, Yiannis Charalambous, Juan G. Colonna, Lucas C. Cordeiro, Rosiane de Freitas",https://arxiv.org/pdf/2404.09384,"Prompting has become one of the main approaches to leverage emergent capabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al. TMLR 2022, Wei et al. NeurIPS 2022]. Recently, researchers and practitioners have been ""playing"" with prompts (e.g., In-Context Learning) to see how to make the most of pre-trained Language Models. By homogeneously dissecting more than a hundred articles, we investigate how softwaretesting and verification research communities have leveraged LLMs capabilities. First, we validate that downstream tasks are adequate to convey a nontrivial modular blueprint of prompt-based proposals in scope. Moreover, we name and classify the concrete downstream tasks we recover in both validation research papers and solution proposals. In order to perform classification, mapping, and analysis, we also develop a novel downstream-task taxonomy. The main taxonomy requirement is to highlight commonalities while exhibiting variation points of task types that enable pinpointing emerging patterns in a varied spectrum of Software Engineering problems that encompasses testing, fuzzing, fault localization, vulnerability detection, static analysis, and program verification approaches. Avenues for future research are also discussed based on conceptual clusters induced by the taxonomy.",,"v1 submitted 14 April, 2024"
https://arxiv.org/abs/2404.04966,Enhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis,"Chen Yang, Junjie Chen, Bin Lin, Jianyi Zhou, Ziqi Wang",https://arxiv.org/pdf/2404.04966,"Automatic test generation plays a critical role in software quality assurance. While the recent advances in Search-Based SoftwareTesting (SBST) and Large Language Models (LLMs) have shown promise in generating useful tests, these techniques still struggle to cover certain branches. Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing test generation techniques. In this work, we propose TELPA, a novel technique aimed at addressing these challenges. Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-to-cover branches to learn the semantics of branch constraints. To enhance efficiency and effectiveness, TELPA identifies a set of ineffective tests as counter-examples for LLMs and employs a feedback-based process to iteratively refine these counter-examples. Then, TELPA integrates program analysis results and counter-examples into the prompt, guiding LLMs to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches. Our experimental results on 27 open-source Python projects demonstrate that TELPA significantly outperforms the state-of-the-art SBST and LLM-based techniques, achieving an average improvement of 31.39% and 22.22% in terms of branch coverage.",,originally announced April 2024.
https://arxiv.org/abs/2403.03897,Fuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing,"Asmita, Yaroslav Oliinyk, Michael Scott, Ryan Tsang, Chongzhou Fang, Houman Homayoun",https://arxiv.org/pdf/2403.03897,"BusyBox, an open-source software bundling over 300 essential Linux commands into a single executable, is ubiquitous in Linux-based embedded devices. Vulnerabilities in BusyBox can have far-reaching consequences, affecting a wide array of devices. This research, driven by the extensive use of BusyBox, delved into its analysis. The study revealed the prevalence of older BusyBox versions in real-world embedded products, prompting us to conduct fuzz testing on BusyBox. Fuzzing, a pivotal softwaretesting method, aims to induce crashes that are subsequently scrutinized to uncover vulnerabilities. Within this study, we introduce two techniques to fortify softwaretesting. The first technique enhances fuzzing by leveraging Large Language Models (LLM) to generate target-specific initial seeds. Our study showed a substantial increase in crashes when using LLM-generated initial seeds, highlighting the potential of LLM to efficiently tackle the typically labor-intensive task of generating target-specific initial seeds. The second technique involves repurposing previously acquired crash data from similar fuzzed targets before initiating fuzzing on a new target. This approach streamlines the time-consuming fuzz testing process by providing crash data directly to the new target before commencing fuzzing. We successfully identified crashes in the latest BusyBox target without conducting traditional fuzzing, emphasizing the effectiveness of LLM and crash reuse techniques in enhancing softwaretesting and improving vulnerability detection in embedded systems. Additionally, manual triaging was performed to identify the nature of crashes in the latest BusyBox.",,originally announced March 2024.
https://arxiv.org/abs/2402.13518,RITFIS: Robust input testing framework for LLMs-based intelligent software,"Mingxuan Xiao, Yan Xiao, Hai Dong, Shunhui Ji, Pengcheng Zhang",https://arxiv.org/pdf/2402.13518,"The dependence of Natural Language Processing (NLP) intelligent software on Large Language Models (LLMs) is increasingly prominent, underscoring the necessity for robustness testing. Current testing methods focus solely on the robustness of LLM-based software to prompts. Given the complexity and diversity of real-world inputs, studying the robustness of LLMbased software in handling comprehensive inputs (including prompts and examples) is crucial for a thorough understanding of its performance. To this end, this paper introduces RITFIS, a Robust Input Testing Framework for LLM-based Intelligent Software. To our knowledge, RITFIS is the first framework designed to assess the robustness of LLM-based intelligent software against natural language inputs. This framework, based on given threat models and prompts, primarily defines the testing process as a combinatorial optimization problem. Successful test cases are determined by a goal function, creating a transformation space for the original examples through perturbation means, and employing a series of search methods to filter cases that meet both the testing objectives and language constraints. RITFIS, with its modular design, offers a comprehensive method for evaluating the robustness of LLMbased intelligent software. RITFIS adapts 17 automated testing methods, originally designed for Deep Neural Network (DNN)-based intelligent software, to the LLM-based softwaretesting scenario. It demonstrates the effectiveness of RITFIS in evaluating LLM-based intelligent software through empirical validation. However, existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models. Therefore, we conducted a comprehensive analysis based on five metrics and provided insightful testing method optimization strategies, benefiting both researchers and everyday users.",,originally announced February 2024.
https://arxiv.org/abs/2402.00350,On the Challenges of Fuzzing Techniques via Large Language Models,"Linghan Huang, Peizhou Zhao, Huaming Chen, Lei Ma",https://arxiv.org/pdf/2402.00350,"In the modern era where software plays a pivotal role, software security and vulnerability analysis are essential for secure software development. Fuzzing test, as an efficient and traditional softwaretesting method, has been widely adopted across various domains. Meanwhile, the rapid development in Large Language Models (LLMs) has facilitated their application in the field of softwaretesting, demonstrating remarkable performance. As existing fuzzing test techniques are not fully automated and software vulnerabilities continue to evolve, there is a growing interest in leveraging large language models to generate fuzzing test. In this paper, we present a systematic overview of the developments that utilize large language models for the fuzzing test. To our best knowledge, this is the first work that covers the intersection of three areas, including LLMs, fuzzing test, and fuzzing test generated based on LLMs. A statistical analysis and discussion of the literature are conducted by summarizing the state-of-the-art methods up to date of the submission. Our work also investigates the potential for widespread deployment and application of fuzzing test techniques generated by LLMs in the future, highlighting their promise for advancing automated softwaretesting practices.",,"v1 submitted 1 February, 2024"
https://arxiv.org/abs/2401.17626,Generative AI to Generate Test Data Generators,"Benoit Baudry, Khashayar Etemadi, Sen Fang, Yogya Gamage, Yi Liu, Yuxin Liu, Martin Monperrus, Javier Ron, André Silva, Deepika Tiwari",https://arxiv.org/pdf/2401.17626,"Generating fake data is an essential dimension of modern softwaretesting, as demonstrated by the number and significance of data faking libraries. Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains. In this paper, we assess the ability of generative AI for generating test data in different domains. We design three types of prompts for Large Language Models (LLMs), which perform test data generation tasks at different levels of integrability: 1) raw test data generation, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries. We evaluate our approach by prompting LLMs to generate test data for 11 domains. The results show that LLMs can successfully generate realistic test data generators in a wide range of domains at all three levels of integrability.",,"v1 submitted 31 January, 2024"
https://arxiv.org/abs/2401.13924,ChatGPT and Human Synergy in Black-Box Testing: A Comparative Analysis,"Hiroyuki Kirinuki, Haruto Tanno",https://arxiv.org/pdf/2401.13924,"In recent years, large language models (LLMs), such as ChatGPT, have been pivotal in advancing various artificial intelligence applications, including natural language processing and software engineering. A promising yet underexplored area is utilizing LLMs in softwaretesting, particularly in black-box testing. This paper explores the test cases devised by ChatGPT in comparison to those created by human participants. In this study, ChatGPT (GPT-4) and four participants each created black-box test cases for three applications based on specifications written by the authors. The goal was to evaluate the real-world applicability of the proposed test cases, identify potential shortcomings, and comprehend how ChatGPT could enhance human testing strategies. ChatGPT can generate test cases that generally match or slightly surpass those created by human participants in terms of test viewpoint coverage. Additionally, our experiments demonstrated that when ChatGPT cooperates with humans, it can cover considerably more test viewpoints than each can achieve alone, suggesting that collaboration between humans and ChatGPT may be more effective than human pairs working together. Nevertheless, we noticed that the test cases generated by ChatGPT have certain issues that require addressing before use.",,originally announced January 2024.
https://arxiv.org/abs/2401.06580,TestSpark: IntelliJ IDEA's Ultimate Test Generation Companion,"Arkadii Sapozhnikov, Mitchell Olsthoorn, Annibale Panichella, Vladimir Kovalenko, Pouria Derakhshanfar",https://arxiv.org/pdf/2401.06580,"Writing softwaretests is laborious and time-consuming. To address this, prior studies introduced various automated test-generation techniques. A well-explored research direction in this field is unit test generation, wherein artificial intelligence (AI) techniques create tests for a method/class under test. While many of these techniques have primarily found applications in a research context, existing tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly and are tailored to a single technique. This paper introduces TestSpark, a plugin for IntelliJ IDEA that enables users to generate unit tests with only a few clicks directly within their Integrated Development Environment (IDE). Furthermore, TestSpark also allows users to easily modify and run each generated test and integrate them into the project workflow. TestSpark leverages the advances of search-based test generation tools, and it introduces a technique to generate unit tests using Large Language Models (LLMs) by creating a feedback cycle between the IDE and the LLM. Since TestSpark is an open-source (https://github.com/JetBrains-Research/TestSpark), extendable, and well-documented tool, it is possible to add new test generation methods into the plugin with the minimum effort. This paper also explains our future studies related to TestSpark and our preliminary results. Demo video: https://youtu.be/0F4PrxWfiXo",,originally announced January 2024.
https://arxiv.org/abs/2312.12598,A Case Study on Test Case Construction with Large Language Models: Unveiling Practical Insights and Challenges,"Roberto Francisco de Lima Junior, Luiz Fernando Paes de Barros Presta, Lucca Santos Borborema, Vanderson Nogueira da Silva, Marcio Leal de Melo Dahia, Anderson Carlos Sousa e Santos",https://arxiv.org/pdf/2312.12598,"This paper presents a detailed case study examining the application of Large Language Models (LLMs) in the construction of test cases within the context of software engineering. LLMs, characterized by their advanced natural language processing capabilities, are increasingly garnering attention as tools to automate and enhance various aspects of the software development life cycle. Leveraging a case study methodology, we systematically explore the integration of LLMs in the test case construction process, aiming to shed light on their practical efficacy, challenges encountered, and implications for software quality assurance. The study encompasses the selection of a representative software application, the formulation of test case construction methodologies employing LLMs, and the subsequent evaluation of outcomes. Through a blend of qualitative and quantitative analyses, this study assesses the impact of LLMs on test case comprehensiveness, accuracy, and efficiency. Additionally, delves into challenges such as model interpretability and adaptation to diverse software contexts. The findings from this case study contributes with nuanced insights into the practical utility of LLMs in the domain of test case construction, elucidating their potential benefits and limitations. By addressing real-world scenarios and complexities, this research aims to inform software practitioners and researchers alike about the tangible implications of incorporating LLMs into the softwaretesting landscape, fostering a more comprehensive understanding of their role in optimizing the software development process.",,"v1 submitted 19 December, 2023"
https://arxiv.org/abs/2307.07221,"SoftwareTesting with Large Language Models: Survey, Landscape, and Vision","Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, Qing Wang",https://arxiv.org/pdf/2307.07221,"Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, softwaretesting is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective softwaretesting techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in softwaretesting. It analyzes 102 relevant studies that have used LLMs for softwaretesting, from both the softwaretesting and LLMs perspectives. The paper presents a detailed discussion of the softwaretesting tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in softwaretesting.",,"v1 submitted 14 July, 2023"
https://arxiv.org/abs/2305.04764,ChatUniTest: A Framework for LLM-Based Test Generation,"Yinghao Chen, Zehao Hu, Chen Zhi, Junxiao Han, Shuiguang Deng, Jianwei Yin",https://arxiv.org/pdf/2305.04764,"Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests. Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage. Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the softwaretesting domain. ChatUniTest is available at https://github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at https://www.youtube.com/watch?v=GmfxQUqm2ZQ.",,"v1 submitted 8 May, 2023"
https://arxiv.org/abs/2506.12278,Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure,"Zheyuan Yang, Zexi Kuang, Xue Xia, Yilun Zhao",https://arxiv.org/pdf/2506.12278,"We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-casegeneration. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.",,originally announced June 2025.
https://arxiv.org/abs/2506.11014,MultiMind: A Plug-in for the Implementation of Development Tasks Aided by AI Assistants,"Benedetta Donato, Leonardo Mariani, Daniela Micucci, Oliviero Riganelli, Marco Somaschini",https://arxiv.org/pdf/2506.11014,"The integration of AI assistants into software development workflows is rapidly evolving, shifting from automation-assisted tasks to collaborative interactions between developers and AI. Large Language Models (LLMs) have demonstrated their effectiveness in several development activities, including code completion, testcasegeneration, and documentation production. However, embedding AI-assisted tasks within Integrated Development Environments (IDEs) presents significant challenges. It requires designing mechanisms to invoke AI assistants at the appropriate time, coordinate interactions with multiple assistants, process the generated outputs, and present feedback in a way that seamlessly integrates with the development workflow. To address these issues, we introduce MultiMind, a Visual Studio Code plug-in that streamlines the creation of AI-assisted development tasks. MultiMind provides a modular and extensible framework, enabling developers to cost-effectively implement and experiment with new AI-powered interactions without the need for complex IDE customizations. MultiMind has been tested in two use cases: one for the automatic generation of code comments and the other about the definition of AI-powered chat.",,originally announced June 2025.
https://arxiv.org/abs/2506.09289,UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench,"Boxi Yu, Yuxuan Zhu, Pinjia He, Daniel Kang",https://arxiv.org/pdf/2506.09289,"The advent of Large Language Models (LLMs) has spurred the development of coding agents for real-world code generation. As a widely used benchmark for evaluating the code generation capabilities of these agents, SWE-Bench uses real-world problems based on GitHub issues and their corresponding pull requests. However, the manually written test cases included in these pull requests are often insufficient, allowing generated patches to pass the tests without resolving the underlying issue. To address this challenge, we introduce UTGenerator, an LLM-driven testcasegenerator that automatically analyzes codebases and dependencies to generate test cases for real-world Python projects. Building on UTGenerator, we propose UTBoost, a comprehensive framework for test case augmentation. In our evaluation, we identified 36 task instances with insufficient test cases and uncovered 345 erroneous patches incorrectly labeled as passed in the original SWE Bench. These corrections, impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard entries, yield 18 and 11 ranking changes, respectively.",,originally announced June 2025.
https://arxiv.org/abs/2506.06821,Can LLMs Generate Reliable TestCaseGenerators? A Study on Competition-Level Programming Problems,"Yuhan Cao, Zian Chen, Kun Quan, Ziliang Zhang, Yu Wang, Xiaoning Dong, Yeqi Feng, Guanzhong He, Jingcheng Huang, Jianhao Li, Yixuan Tan, Jiafu Tang, Yilin Tang, Junlei Wu, Qianyu Xiao, Can Zheng, Shouchen Zhou, Yuxiang Zhu, Yiming Huang, Tian Xie, Tianxing He",https://arxiv.org/pdf/2506.06821,"Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, capable of tackling complex tasks during inference. However, the extent to which LLMs can be utilized for code checking or debugging through testcasegeneration remains largely unexplored. We investigate this problem from the perspective of competition-level programming (CP) programs and propose TCGBench, a Benchmark for (LLM generation of) TestCaseGenerators. This benchmark comprises two tasks, aimed at studying the capabilities of LLMs in (1) generating valid testcasegenerators for a given CP problem, and further (2) generating targeted testcasegenerators that expose bugs in human-written code. Experimental results indicate that while state-of-the-art LLMs can generate valid testcasegenerators in most cases, most LLMs struggle to generate targeted test cases that reveal flaws in human code effectively. Especially, even advanced reasoning models (e.g., o3-mini) fall significantly short of human performance in the task of generating targeted generators. Furthermore, we construct a high-quality, manually curated dataset of instructions for generating targeted generators. Analysis demonstrates that the performance of LLMs can be enhanced with the aid of this dataset, by both prompting and fine-tuning.",,"v1 submitted 7 June, 2025"
https://arxiv.org/abs/2506.04894,ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests,"Shiyi Xu, Yiwen Hu, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen",https://arxiv.org/pdf/2506.04894,"With the significant progress of large reasoning models in complex coding and reasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are insufficient to evaluate the coding capabilities of large language models (LLMs) in real competition environments. Moreover, current evaluation metrics such as Pass@K fail to capture the reflective abilities of reasoning models. To address these challenges, we propose \textbf{ICPC-Eval}, a top-level competitive coding benchmark designed to probing the frontiers of LLM reasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent ICPC contests held in various regions of the world, offering three key contributions: 1) A challenging realistic ICPC competition scenario, featuring a problem type and difficulty distribution consistent with actual contests. 2) A robust testcasegeneration method and a corresponding local evaluation toolkit, enabling efficient and accurate local evaluation. 3) An effective test-time scaling evaluation metric, Refine@K, which allows iterative repair of solutions based on execution feedback. The results underscore the significant challenge in evaluating complex reasoning abilities: top-tier reasoning models like DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their in-context reasoning potential when compared to non-reasoning counterparts. Furthermore, despite recent advancements in code generation, these models still lag behind top-performing human teams. We release the benchmark at: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs",,originally announced June 2025.
https://arxiv.org/abs/2505.23009,"EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge","Ruskin Raj Manku, Yuzhi Tang, Xingjian Shi, Mu Li, Alex Smola",https://arxiv.org/pdf/2505.23009,"Text-to-Speech (TTS) benchmarks often fail to capture how well models handle nuanced and semantically complex text. Building on \textit{EmergentTTS}
, we introduce \textit{EmergentTTS-Eval}
, a comprehensive benchmark covering six challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation (e.g. URLs, formulas), and questions. Crucially, our framework automates both test-casegeneration and evaluation, making the benchmark easily extensible. Starting from a small set of human-written seed prompts, we iteratively extend them using LLMs to target specific structural, phonetic and prosodic challenges, resulting in 1,645 diverse test cases. Moreover, we employ a model-as-a-judge approach, using a Large Audio Language Model (LALM) to assess the speech across multiple dimensions such as expressed emotion, prosodic, intonational, and pronunciation accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems, such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval, demonstrating its ability to reveal fine-grained performance differences. Results show that the model-as-a-judge approach offers robust TTS assessment and a high correlation with human preferences. We open source the evaluation \href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}
 and the \href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}
.",,originally announced May 2025.
https://arxiv.org/abs/2505.15621,DSCodeBench: A Realistic Benchmark for Data Science Code Generation,"Shuyin Ouyang, Dong Huang, Jingwen Guo, Zeyu Sun, Qihao Zhu, Jie M. Zhang",https://arxiv.org/pdf/2505.15621,"We introduce DSCodeBench, a new benchmark designed to evaluate large language models (LLMs) on complicated and realistic data science code generation tasks. DSCodeBench consists of 1,000 carefully constructed problems sourced from realistic problems from GitHub across ten widely used Python data science libraries. Compared to the current state-of-the-art benchmark DS-1000, DSCodeBench offers a more challenging and representative testbed, longer code solutions, more comprehensive data science libraries, clearer and better structured problem descriptions, and stronger test suites. To construct the DSCodeBench, we develop a robust pipeline that combines task scope selection, code construction, testcasegeneration, and problem description synthesis. The process is paired with rigorous manual editing to ensure alignment and enhance evaluation reliability. Experimental result shows that DSCodeBench exhibits robust scaling behavior, where larger models systematically outperform smaller ones, validating its ability to distinguish model capabilities. The best LLM we test, GPT-4o, has a pass@1 of 0.202, indicating that LLMs still have a large room to improve for realistic data science code generation tasks. We believe DSCodeBench will serve as a rigorous and trustworthy foundation for advancing LLM-based data science programming.",,"v1 submitted 21 May, 2025"
https://arxiv.org/abs/2505.02012,Testing Database Systems with Large Language Model Synthesized Fragments,"Suyang Zhong, Manuel Rigger",https://arxiv.org/pdf/2505.02012,"Various automated testing approaches have been proposed for Database Management Systems (DBMSs). Many such approaches generate pairs of equivalent queries to identify bugs that cause DBMSs to compute incorrect results, and have found hundreds of bugs in mature, widely used DBMSs. Most of these approaches are based on manually written SQL generators; however, their bug-finding capabilities remain constrained by the limited set of SQL features supported by the generators. In this work, we propose ShQveL, an approach that augments existing SQL test-casegenerators by leveraging Large Language Models (LLMs) to synthesize SQL fragments. Our key idea is to systematically incorporate SQL features gained through automated interactions with LLMs into the SQL generators, increasing the features covered while efficiently generating test cases. Specifically, ShQveL uses SQL sketches -- SQL statements with incomplete code segments that LLMs fill -- to integrate LLM-generated content into the generator. We evaluated ShQveL on 5 DBMSs and discovered 55 unique and previously unknown bugs, 50 of which were promptly fixed after our reports.",,originally announced May 2025.
https://arxiv.org/abs/2503.18460,ModiGen: A Large Language Model-Based Workflow for Multi-Task Modelica Code Generation,"Jiahui Xiang, Tong Ye, Peiyu Liu, Yinan Zhang, Wenhai Wang",https://arxiv.org/pdf/2503.18460,"Modelica is a widely adopted language for simulating complex physical systems, yet effective model creation and optimization require substantial domain expertise. Although large language models (LLMs) have demonstrated promising capabilities in code generation, their application to modeling remains largely unexplored. To address this gap, we have developed benchmark datasets specifically designed to evaluate the performance of LLMs in generating Modelica component models and test cases. Our evaluation reveals substantial limitations in current LLMs, as the generated code often fails to simulate successfully. To overcome these challenges, we propose a specialized workflow that integrates supervised fine-tuning, graph retrieval-augmented generation, and feedback optimization to improve the accuracy and reliability of Modelica code generation. The evaluation results demonstrate significant performance gains: the maximum improvement in pass@1 reached 0.3349 for the component generation task and 0.2457 for the testcasegeneration task. This research underscores the potential of LLMs to advance intelligent modeling tools and offers valuable insights for future developments in system modeling and engineering applications.",,originally announced March 2025.
https://arxiv.org/abs/2502.13820,Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning,"Aleksander Ficek, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg",https://arxiv.org/pdf/2502.13820,"Synthetic verification techniques such as generating test cases and reward modelling are common ways to enhance the coding capabilities of large language models (LLM) beyond predefined tests. Additionally, code verification has recently found great success as a critical component in improving reasoning capability of LLMs via reinforcement learning. In this paper, we propose a an approach which can transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. We also propose multiple metrics to measure different aspects of the synthetic verifiers with the proposed benchmarks. By employing the proposed approach, we release four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed synthetic verification methods with standard, reasoning-based, and reward-based LLMs. Our experiments show that reasoning can significantly improve testcasegeneration and that scaling the number of test cases enhances the verification accuracy.",,"v1 submitted 19 February, 2025"
https://arxiv.org/abs/2502.10802,CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation,"Kefan Li, Hongyue Yu, Tingyu Guo, Shijie Cao, Yuan Yuan",https://arxiv.org/pdf/2502.10802,"Large Language Models (LLMs) have shown remarkable performance in automated code generation. However, existing approaches often rely heavily on pre-defined test cases, which become impractical in scenarios where such cases are unavailable. While prior works explore filtering techniques between programs and test cases, they overlook the refinement of test cases. To address this limitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that simultaneously evolves programs and test cases. CoCoEvo eliminates the dependency on pre-defined test cases by generating both programs and test cases directly from natural language problem descriptions and function headers. The framework employs specialized evolutionary operators, including LLM-based crossover and mutation operators for program evolution, along with a testcasegeneration operator for test case evolution. Additionally, we propose optimization strategies such as a crossover rate scheduler to balance exploration and convergence, and a multi-objective optimization method for test case selection. Experimental results on multiple state-of-the-art LLMs demonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art performance in automated code generation and testing. These results underscore the potential of co-evolutionary techniques in advancing the field of automated programming.",,originally announced February 2025.
https://arxiv.org/abs/2502.02827,COFFE: A Code Efficiency Benchmark for Code Generation,"Yun Peng, Jun Wan, Yichen Li, Xiaoxue Ren",https://arxiv.org/pdf/2502.02827,"Code generation has largely improved development efficiency in the era of large language models (LLMs). With the ability to follow instructions, current LLMs can be prompted to generate code solutions given detailed descriptions in natural language. Many research efforts are being devoted to improving the correctness of LLM-generated code, and many benchmarks are proposed to evaluate the correctness comprehensively. Despite the focus on correctness, the time efficiency of LLM-generated code solutions is under-explored. Current correctness benchmarks are not suitable for time efficiency evaluation since their test cases cannot well distinguish the time efficiency of different code solutions. Besides, the current execution time measurement is not stable and comprehensive, threatening the validity of the time efficiency evaluation. To address the challenges in the time efficiency evaluation of code generation, we propose COFFE, a code generation benchmark for evaluating the time efficiency of LLM-generated code solutions. COFFE contains 398 and 358 problems for function-level and file-level code generation, respectively. To improve the distinguishability, we design a novel stressful testcasegeneration approach with contracts and two new formats of test cases to improve the accuracy of generation. For the time evaluation metric, we propose efficienct@k based on CPU instruction count to ensure a stable and solid comparison between different solutions. We evaluate 14 popular LLMs on COFFE and identify four findings. Based on the findings, we draw some implications for LLM researchers and software practitioners to facilitate future research and usage of LLMs in code generation.",,originally announced February 2025.
https://arxiv.org/abs/2501.01329,The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for TestCaseGeneration,"Shuzheng Gao, Chaozheng Wang, Cuiyun Gao, Xiaoqian Jiao, Chun Yong Chong, Shan Gao, Michael Lyu",https://arxiv.org/pdf/2501.01329,"Test cases are essential for validating the reliability and quality of software applications. Recent studies have demonstrated the capability of Large Language Models (LLMs) to generate useful test cases for given source code. However, the existing work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly influenced by the prompts. Moreover, these approaches use the same prompt for all LLMs, overlooking the fact that different LLMs might be best suited to different prompts. Given the wide variety of possible prompt formulations, automatically discovering the optimal prompt for each LLM presents a significant challenge. Although there are methods on automated prompt optimization in the natural language processing field, they are hard to produce effective prompts for the testcasegeneration task. First, the methods iteratively optimize prompts by simply combining and mutating existing ones without proper guidance, resulting in prompts that lack diversity and tend to repeat the same errors in the generated test cases. Second, the prompts are generally lack of domain contextual knowledge, limiting LLMs' performance in the task.",,originally announced January 2025.
https://arxiv.org/abs/2411.18143,Harnessing Large Language Models for Seed Generation in Greybox Fuzzing,"Wenxuan Shi, Yunhang Zhang, Xinyu Xing, Jun Xu",https://arxiv.org/pdf/2411.18143,"Greybox fuzzing has emerged as a preferred technique for discovering software bugs, striking a balance between efficiency and depth of exploration. While research has focused on improving fuzzing techniques, the importance of high-quality initial seeds remains critical yet often overlooked. Existing methods for seed generation are limited, especially for programs with non-standard or custom input formats. Large Language Models (LLMs) has revolutionized numerous domains, showcasing unprecedented capabilities in understanding and generating complex patterns across various fields of knowledge. This paper introduces SeedMind, a novel system that leverages LLMs to boost greybox fuzzing through intelligent seed generation. Unlike previous approaches, SeedMind employs LLMs to create testcasegenerators rather than directly producing test cases. Our approach implements an iterative, feedback-driven process that guides the LLM to progressively refine testcasegeneration, aiming for increased code coverage depth and breadth. In developing SeedMind, we addressed key challenges including input format limitations, context window constraints, and ensuring consistent, progress-aware behavior. Intensive evaluations with real-world applications show that SeedMind effectively harnesses LLMs to generate high-quality test cases and facilitate fuzzing in bug finding, presenting utility comparable to human-created seeds and significantly outperforming the existing LLM-based solutions.",,originally announced November 2024.
https://arxiv.org/abs/2410.14763,Enabling Scalable Evaluation of Bias Patterns in Medical LLMs,"Hamed Fayyaz, Raphael Poulain, Rahmatollah Beheshti",https://arxiv.org/pdf/2410.14763,"Large language models (LLMs) have shown impressive potential in helping with numerous medical challenges. Deploying LLMs in high-stakes applications such as medicine, however, brings in many concerns. One major area of concern relates to biased behaviors of LLMs in medical applications, leading to unfair treatment of individuals. To pave the way for the responsible and impactful deployment of Med LLMs, rigorous evaluation is a key prerequisite. Due to the huge complexity and variability of different medical scenarios, existing work in this domain has primarily relied on using manually crafted datasets for bias evaluation. In this study, we present a new method to scale up such bias evaluations by automatically generating test cases based on rigorous medical evidence. We specifically target the challenges of a) domain-specificity of bias characterization, b) hallucinating while generating the test cases, and c) various dependencies between the health outcomes and sensitive attributes. To that end, we offer new methods to address these challenges integrated with our generative pipeline, using medical knowledge graphs, medical ontologies, and customized general LLM evaluation frameworks in our method. Through a series of extensive experiments, we show that the testcasesgenerated by our proposed method can effectively reveal bias patterns in Med LLMs at larger and more flexible scales than human-crafted datasets. We publish a large bias evaluation dataset using our pipeline, which is dedicated to a few medical case studies. A live demo of our application for vignette generation is available at https://vignette.streamlit.app. Our code is also available at https://github.com/healthylaife/autofair.",,"v1 submitted 18 October, 2024"
https://arxiv.org/abs/2410.07516,Exploring and Lifting the Robustness of LLM-powered Automated Program Repair with Metamorphic Testing,"Pengyu Xue, Linhao Wu, Zhen Yang, Zhongxing Yu, Zhi Jin, Ge Li, Yan Xiao, Shuo Liu, Xinyi Li, Hongyi Lin, Jingwen Wu",https://arxiv.org/pdf/2410.07516,"In recent years, Large language model-powered Automated Program Repair (LAPR) techniques have achieved state-of-the-art bug-fixing performance and have been pervasively applied and studied in both industry and academia. Nonetheless, LLMs were proved to be highly sensitive to input prompts, with slight differences in the expressions of semantically equivalent programs potentially causing repair failures. Therefore, it is crucial to conduct robustness testing on LAPR techniques before their practical deployment. However, related research is scarce. To this end, we propose MT-LAPR, a Metamorphic Testing framework exclusively for LAPR techniques, which summarizes nine widely-recognized Metamorphic Relations (MRs) by developers across three perturbation levels: token, statement, and block. Afterward, our proposed MRs are applied to buggy codes to generate test cases, which are semantically equivalent yet to affect the inference of LAPR. Experiments are carried out on two extensively examined bug-fixing datasets, i.e., Defect4J and QuixBugs, and four bug-fixing abled LLMs released recently, demonstrating that 34.4% - 48.5% of the test cases expose the instability of LAPR techniques on average, showing the effectiveness of MT-LAPR and uncovering a positive correlation between code readability and the robustness of LAPR techniques. Inspired by the above findings, this paper uses the testcasesgenerated by MT-LAPR as samples to train a CodeT5-based code editing model aiming at improving code readability and then embeds it into the LAPR workflow as a data preprocessing step. Extensive experiments demonstrate that this approach significantly enhances the robustness of LAPR by 49.32% at most.",,"v1 submitted 9 October, 2024"
https://arxiv.org/abs/2409.10280,ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More Complex Code,"Jia Feng, Jiachen Liu, Cuiyun Gao, Chun Yong Chong, Chaozheng Wang, Shan Gao, Xin Xia",https://arxiv.org/pdf/2409.10280,"In recent years, the application of large language models (LLMs) to code-related tasks has gained significant attention. However, existing evaluation benchmarks often focus on limited scenarios, such as code generation or completion, which do not reflect the diverse challenges developers face in real-world contexts. To address this, we introduce ComplexCodeEval, a benchmark designed to assess LCMs in various development tasks, including code generation, completion, API recommendation, and testcasegeneration. It includes 3,897 Java samples and 7,184 Python samples from high-star GitHub repositories, each annotated with function signatures, docstrings, and API references to simulate real development environments. Our experiments across ten LCMs reveal that context improves performance and that data leakage can lead to overestimation, highlighting the need for more accurate evaluations.",,originally announced September 2024.
https://arxiv.org/abs/2406.11927,On the Impacts of Contexts on Repository-Level Code Generation,"Nam Le Hai, Dung Manh Nguyen, Nghi D. Q. Bui",https://arxiv.org/pdf/2406.11927,"CodeLLMs have gained widespread adoption for code generation tasks, yet their capacity to handle repository-level code generation with complex contextual dependencies remains underexplored. Our work underscores the critical importance of leveraging repository-level contexts to generate executable and functionally correct code. We present RepoExec, a novel benchmark designed to evaluate repository-level code generation, with a focus on three key aspects: executability, functional correctness through comprehensive testcasegeneration, and accurate utilization of cross-file contexts. Our study examines a controlled scenario where developers specify essential code dependencies (contexts), challenging models to integrate them effectively. Additionally, we introduce an instruction-tuned dataset that enhances CodeLLMs' ability to leverage dependencies, along with a new metric, Dependency Invocation Rate (DIR), to quantify context utilization. Experimental results reveal that while pretrained LLMs demonstrate superior performance in terms of correctness, instruction-tuned models excel in context utilization and debugging capabilities. RepoExec offers a comprehensive evaluation framework for assessing code functionality and alignment with developer intent, thereby advancing the development of more reliable CodeLLMs for real-world applications. The dataset and source code are available at https://github.com/FSoft-AI4Code/RepoExec.",,"v1 submitted 17 June, 2024"
https://arxiv.org/abs/2406.06647,How Efficient is LLM-Generated Code? A Rigorous & High-Standard Benchmark,"Ruizhong Qiu, Weiliang Will Zeng, James Ezick, Christopher Lott, Hanghang Tong",https://arxiv.org/pdf/2406.06647,"The emergence of large language models (LLMs) has significantly pushed the frontiers of program synthesis. Advancement of LLM-based program synthesis calls for a thorough evaluation of LLM-generated code. Most evaluation frameworks focus on the (functional) correctness of generated code; efficiency, as an important measure of code quality, has been overlooked in existing evaluations. In this work, we develop ENAMEL (EfficeNcy AutoMatic EvaLuator), a rigorous and high-standard benchmark for evaluating the capability of LLMs in generating efficient code. Firstly, we propose a new efficiency metric called eff@k, which generalizes the pass@k metric from correctness to efficiency and appropriately handles right-censored execution time. Furthermore, we derive an unbiased and variance-reduced estimator of eff@k via Rao--Blackwellization; we also provide a numerically stable implementation for the new estimator. Secondly, to set a high-standard for efficiency evaluation, we employ a human expert to design best algorithms and implementations as our reference solutions of efficiency, many of which are much more efficient than existing canonical solutions in HumanEval and HumanEval+. Moreover, to ensure a rigorous evaluation, we employ a human expert to curate strong testcasegenerators to filter out wrong code and differentiate suboptimal algorithms. An extensive study across 30 popular LLMs using our benchmark ENAMEL shows that LLMs still fall short of generating expert-level efficient code. Using two subsets of our problem set, we demonstrate that such deficiency is because current LLMs struggle in designing advanced algorithms and are barely aware of implementation optimization. Our benchmark is publicly available at https://github.com/q-rz/enamel .",,"v1 submitted 10 June, 2024"
https://arxiv.org/abs/2406.03636,Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource Programming and Formal Languages,"Federico Mora, Justin Wong, Haley Lepe, Sahil Bhatia, Karim Elmaaroufi, George Varghese, Joseph E. Gonzalez, Elizabeth Polgreen, Sanjit A. Seshia",https://arxiv.org/pdf/2406.03636,"Recent advances in large language models (LLMs) for code applications have demonstrated remarkable zero-shot fluency and instruction following on challenging code related tasks ranging from testcasegeneration to self-repair. Unsurprisingly, however, models struggle to compose syntactically valid programs in programming languages unrepresented in pre-training, referred to as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial settings, including domain-specific languages for internal tools, tool-chains for legacy languages, and formal verification frameworks. Inspired by a technique called natural programming elicitation, we propose designing an intermediate language that LLMs ""naturally"" know how to use and which can be automatically compiled to a target VLPL. When LLMs generate code that lies outside of this intermediate language, we use compiler techniques to repair the code into programs in the intermediate language. Overall, we introduce \emph{synthetic programming elicitation and compilation} (SPEAC), an approach that enables LLMs to generate syntactically valid code even for VLPLs. We empirically evaluate the performance of SPEAC in a case study for the UCLID5 formal verification language and find that, compared to existing retrieval and fine-tuning baselines, SPEAC produces syntactically correct programs more frequently and without sacrificing semantic correctness.",,"v1 submitted 5 June, 2024"
https://arxiv.org/abs/2405.01874,Automated Control Logic TestCaseGeneration using Large Language Models,"Heiko Koziolek, Virendra Ashiwal, Soumyadip Bandyopadhyay, Chandrika K R",https://arxiv.org/pdf/2405.01874,"Testing PLC and DCS control logic in industrial automation is laborious and challenging since appropriate test cases are often complex and difficult to formulate. Researchers have previously proposed several automated testcasegeneration approaches for PLC software applying symbolic execution and search-based techniques. Often requiring formal specifications and performing a mechanical analysis of programs, these approaches may uncover specific programming errors but sometimes suffer from state space explosion and cannot process rather informal specifications. We proposed a novel approach for the automatic generation of PLC test cases that queries a Large Language Model (LLM) to synthesize test cases for code provided in a prompt. Experiments with ten open-source function blocks from the OSCAT automation library showed that the approach is fast, easy to use, and can yield test cases with high statement coverage for low-to-medium complex programs. However, we also found that LLM-generated test cases suffer from erroneous assertions in many cases, which still require manual adaption.",,originally announced May 2024.
https://arxiv.org/abs/2404.13340,Large Language Models as TestCaseGenerators: Performance Evaluation and Enhancement,"Kefan Li, Yuan Yuan",https://arxiv.org/pdf/2404.13340,"Code generation with Large Language Models (LLMs) has been extensively studied and achieved remarkable progress. As a complementary aspect to code generation, testcasegeneration is of crucial importance in ensuring the quality and reliability of code. However, using LLMs as testcasegenerators has been much less explored. Current research along this line primarily focuses on enhancing code generation with assistance from testcasesgenerated by LLMs, while the performance of LLMs in testcasegeneration alone has not been comprehensively examined. To bridge this gap, we conduct extensive experiments to study how well LLMs can generate high-quality test cases. We find that as the problem difficulty increases, state-of-the-art LLMs struggle to generate correct test cases, largely due to their inherent limitations in computation and reasoning. To mitigate this issue, we further propose a multi-agent framework called \emph{TestChain} that decouples the generation of test inputs and test outputs. Notably, TestChain uses a ReAct format conversation chain for LLMs to interact with a Python interpreter in order to provide more accurate test outputs. Our results indicate that TestChain outperforms the baseline by a large margin. Particularly, in terms of the accuracy of test cases, TestChain using GPT-4 as the backbone achieves a 13.84\% improvement over the baseline on the LeetCode-hard dataset.",,originally announced April 2024.
https://arxiv.org/abs/2402.14261,Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming,"Anisha Agarwal, Aaron Chan, Shubham Chandel, Jinu Jang, Shaun Miller, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Neel Sundaresan, Michele Tufano",https://arxiv.org/pdf/2402.14261,"The integration of Large Language Models (LLMs) into Development Environments (IDEs) has become a focal point in modern software development. LLMs such as OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment developer productivity by serving as intelligent, chat-driven programming assistants. However, utilizing LLMs out of the box is unlikely to be optimal for any given scenario. Rather, each system requires the LLM to be honed to its set of heuristics to ensure the best performance. In this paper, we introduce the Copilot evaluation harness: a set of data and tools for evaluating LLM-guided IDE interactions, covering various programming scenarios and languages. We propose our metrics as a more robust and information-dense evaluation than previous state of the art evaluation systems. We design and compute both static and execution based success metrics for scenarios encompassing a wide range of developer tasks, including code generation from natural language (generate), documentation generation from code (doc), testcasegeneration (test), bug-fixing (fix), and workspace understanding and query resolution (workspace). These success metrics are designed to evaluate the performance of LLMs within a given IDE and its respective parameter space. Our learnings from evaluating three common LLMs using these metrics can inform the development and validation of future scenarios in LLM guided IDEs.",,originally announced February 2024.
https://arxiv.org/abs/2402.11910,Enhancing Large Language Models for Text-to-Testcase Generation,"Saranya Alagarsamy, Chakkrit Tantithamthavorn, Wannita Takerngsaksiri, Chetan Arora, Aldeida Aleti",https://arxiv.org/pdf/2402.11910,"Context: Test-driven development (TDD) is a widely employed software development practice that involves developing test cases based on requirements prior to writing the code. Although various methods for automated testcasegeneration have been proposed, they are not specifically tailored for TDD, where requirements instead of code serve as input. Objective: In this paper, we introduce a text-to-testcase generation approach based on a large language model (GPT-3.5) that is fine-tuned on our curated dataset with an effective prompt design. Method: Our approach involves enhancing the capabilities of basic GPT-3.5 for text-to-testcase generation task that is fine-tuned on our curated dataset with an effective prompting design. We evaluated the effectiveness of our approach using a span of five large-scale open-source software projects. Results: Our approach generated 7k test cases for open source projects, achieving 78.5% syntactic correctness, 67.09% requirement alignment, and 61.7% code coverage, which substantially outperforms all other LLMs (basic GPT-3.5, Bloom, and CodeT5). In addition, our ablation study demonstrates the substantial performance improvement of the fine-tuning and prompting components of the GPT-3.5 model. Conclusions: These findings lead us to conclude that fine-tuning and prompting should be considered in the future when building a language model for the text-to-testcase generation task",,"v1 submitted 19 February, 2024"
https://arxiv.org/abs/2401.02115,Using LLM to select the right SQL Query from candidates,"Zhenwen Li, Tao Xie",https://arxiv.org/pdf/2401.02115,"Text-to-SQL models can generate a list of candidate SQL queries, and the best query is often in the candidate list, but not at the top of the list. An effective re-rank method can select the right SQL query from the candidate list and improve the model's performance. Previous studies on code generation automatically generate test cases and use them to re-rank candidate codes. However, automatic testcasegeneration for text-to-SQL is an understudied field. We propose an automatic testcasegeneration method that first generates a database and then uses LLMs to predict the ground truth, which is the expected execution results of the ground truth SQL query on this database. To reduce the difficulty for LLMs to predict, we conduct experiments to search for ways to generate easy databases for LLMs and design easy-to-understand prompts. Based on our testcasegeneration method, we propose a re-rank method to select the right SQL query from the candidate list. Given a candidate list, our method can generate test cases and re-rank the candidate list according to their pass numbers on these test cases and their generation probabilities. The experiment results on the validation dataset of Spider show that the performance of some state-of-the-art models can get a 3.6\% improvement after applying our re-rank method.",,originally announced January 2024.
https://arxiv.org/abs/2312.14898,Enriching Automatic TestCaseGeneration by Extracting Relevant Test Inputs from Bug Reports,"Wendkûuni C. Ouédraogo, Laura Plein, Kader Kaboré, Andrew Habib, Jacques Klein, David Lo, Tegawendé F. Bissyandé",https://arxiv.org/pdf/2312.14898,"The quality of software is closely tied to the effectiveness of the tests it undergoes. Manual test writing, though crucial for bug detection, is time-consuming, which has driven significant research into automated testcasegeneration. However, current methods often struggle to generate relevant inputs, limiting the effectiveness of the tests produced. To address this, we introduce BRMiner, a novel approach that leverages Large Language Models (LLMs) in combination with traditional techniques to extract relevant inputs from bug reports, thereby enhancing automated test generation tools. In this study, we evaluate BRMiner using the Defects4J benchmark and test generation tools such as EvoSuite and Randoop. Our results demonstrate that BRMiner achieves a Relevant Input Rate (RIR) of 60.03% and a Relevant Input Extraction Accuracy Rate (RIEAR) of 31.71%, significantly outperforming methods that rely on LLMs alone. The integration of BRMiner's input enhances EvoSuite ability to generate more effective test, leading to increased code coverage, with gains observed in branch, instruction, method, and line coverage across multiple projects. Furthermore, BRMiner facilitated the detection of 58 unique bugs, including those that were missed by traditional baseline approaches. Overall, BRMiner's combination of LLM filtering with traditional input extraction techniques significantly improves the relevance and effectiveness of automated test generation, advancing the detection of bugs and enhancing code coverage, thereby contributing to higher-quality software development.",,"v1 submitted 22 December, 2023"
https://arxiv.org/abs/2312.13010,AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation,"Dong Huang, Jie M. Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, Heming Cui",https://arxiv.org/pdf/2312.13010,"The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective testcasegeneration and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the programmer. This collaborative system ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies. Our extensive experiments on 9 code generation models and 12 enhancement approaches showcase AgentCoder's superior performance over existing code generation models and prompt engineering techniques across various benchmarks. For example, AgentCoder (GPT-4) achieves 96.3\% and 91.8\% pass@1 in HumanEval and MBPP datasets with an overall token overhead of 56.9K and 66.3K, while state-of-the-art obtains only 90.2\% and 78.9\% pass@1 with an overall token overhead of 138.2K and 206.5K.",,"v1 submitted 20 December, 2023"
https://arxiv.org/abs/2312.08055,Breaking the Silence: the Threats of Using LLMs in Software Engineering,"June Sallou, Thomas Durieux, Annibale Panichella",https://arxiv.org/pdf/2312.08055,"Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs. This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings. In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns. The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of testcasegeneration.",,"v1 submitted 13 December, 2023"
https://arxiv.org/abs/2312.04724,Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models,"Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, Sasha Frolov, Ravi Prakash Giri, Dhaval Kapil, Yiannis Kozyrakis, David LeBlanc, James Milazzo, Aleksandar Straumann, Gabriel Synnaeve, Varun Vontimitta, Spencer Whitman, Joshua Saxe",https://arxiv.org/pdf/2312.04724,"This paper presents CyberSecEval, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large language model families, CyberSecEval effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CyberSecEval, with its automated testcasegeneration and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.",,originally announced December 2023.
https://arxiv.org/abs/2305.14591,ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers,"Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, Lei Li",https://arxiv.org/pdf/2305.14591,"Large language models (LLMs) excel at implementing code from functionality descriptions but struggle with algorithmic problems that require not only implementation but also identification of the suitable algorithm. Moreover, LLM-generated programs lack guaranteed correctness and require human verification. To address these challenges, we propose ALGO, a framework that synthesizes Algorithmic programs with LLM-Generated Oracles to guide the generation and verify their correctness. ALGO first generates a reference oracle by prompting an LLM to exhaustively enumerate all the combinations of relevant variables. This oracle is then utilized to guide an arbitrary search strategy in exploring the algorithm space and to verify the synthesized algorithms. Our study shows that the LLM-generated oracles are correct for 88% of the cases. With the oracles as verifiers, ALGO can be integrated with any existing code generation model in a model-agnostic manner to enhance its performance. Experiments show that when equipped with ALGO, we achieve an 8x better one-submission pass rate over the Codex model and a 2.6x better one-submission pass rate over CodeT, the current state-of-the-art model on CodeContests. We can also get 1.3x better pass rate over the ChatGPT Code Interpreter on unseen problems. The problem set we used for testing, the prompts we used, the verifier and solution programs, and the testcasesgenerated by ALGO are available at https://github.com/zkx06111/ALGO.",,"v1 submitted 23 May, 2023"
https://arxiv.org/abs/2506.16586,"AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions","Ihor Pysmennyi, Roman Kyslyi, Kyrylo Kleshch",https://arxiv.org/pdf/2506.16586,"Traditional quality assurance (QA) methods face significant challenges in addressing the complexity, scale, and rapid iteration cycles of modern software systems and are strained by limited resources available, leading to substantial costs associated with poor quality. The object of this research is the Quality Assurance processes for modern distributed software applications. The subject of the research is the assessment of the benefits, challenges, and prospects of integrating modern AI-oriented tools into quality assurance processes. We performed comprehensive analysis of implications on both verification and validation processes covering exploratory test analyses, equivalence partitioning and boundary analyses, metamorphic testing, finding inconsistencies in acceptance criteria (AC), static analyses, testcasegeneration, unit test generation, test suit optimization and assessment, end to end scenario execution. End to end regression of sample enterprise application utilizing AI-agents over generated test scenarios was implemented as a proof of concept highlighting practical use of the study. The results, with only 8.3% flaky executions of generated test cases, indicate significant potential for the proposed approaches. However, the study also identified substantial challenges for practical adoption concerning generation of semantically identical coverage, ""black box"" nature and lack of explainability from state-of-the-art Large Language Models (LLMs), the tendency to correct mutated test cases to match expected results, underscoring the necessity for thorough verification of both generated artifacts and test execution results. The research demonstrates AI's transformative potential for QA but highlights the importance of a strategic approach to implementing these technologies, considering the identified limitations and the need for developing appropriate verification methodologies.",,originally announced June 2025.
https://arxiv.org/abs/2506.11870,LLM-based Dynamic Differential Testing for Database Connectors with Reinforcement Learning-Guided Prompt Selection,"Ce Lyu, Minghao Zhao, Yanhao Wang, Liang Jie",https://arxiv.org/pdf/2506.11870,"Database connectors are critical components enabling applications to interact with underlying database management systems (DBMS), yet their security vulnerabilities often remain overlooked. Unlike traditional software defects, connector vulnerabilities exhibit subtle behavioral patterns and are inherently challenging to detect. Besides, nonstandardized implementation of connectors leaves potential risks (a.k.a. unsafe implementations) but is more elusive. As a result, traditional fuzzing methods are incapable of finding such vulnerabilities. Even for LLM-enable testcasegeneration, due to a lack of domain knowledge, they are also incapable of generating test cases that invoke all interface and internal logic of connectors. In this paper, we propose reinforcement learning (RL)-guided LLMtest-casegeneration for database connector testing. Specifically, to equip the LLM with sufficient and appropriate domain knowledge, a parameterized prompt template is composed which can be utilized to generate numerous prompts. Test cases are generated via LLM with a prompt, and are dynamically evaluated through differential testing across multiple connectors. The testing is iteratively conducted, with each round RL is adopted to select optimal prompt based on prior-round behavioral feedback, so as to maximize control flow coverage. We implement aforementioned methodology in a practical tool and evaluate it on two widely used JDBC connectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported 16 bugs, among them 10 are officially confirmed and the rest are acknowledged as unsafe implementations.",,originally announced June 2025.
https://arxiv.org/abs/2505.17632,ReqBrain: Task-Specific Instruction Tuning of LLMs for AI-Assisted Requirements Generation,"Mohammad Kasra Habib, Daniel Graziotin, Stefan Wagner",https://arxiv.org/pdf/2505.17632,"Requirements elicitation and specification remains a labor-intensive, manual process prone to inconsistencies and gaps, presenting a significant challenge in modern software engineering. Emerging studies underscore the potential of employing large language models (LLMs) for automated requirements generation to support requirements elicitation and specification; however, it remains unclear how to implement this effectively. In this work, we introduce ReqBrain, an Al-assisted tool that employs a fine-tuned LLM to generate authentic and adequate software requirements. Software engineers can engage with ReqBrain through chat-based sessions to automatically generate software requirements and categorize them by type. We curated a high-quality dataset of ISO 29148-compliant requirements and fine-tuned five 7B-parameter LLMs to determine the most effective base model for ReqBrain. The top-performing model, Zephyr-7b-beta, achieved 89.30\% Fl using the BERT score and a FRUGAL score of 91.20 in generating authentic and adequate requirements. Human evaluations further confirmed ReqBrain's effectiveness in generating requirements. Our findings suggest that generative Al, when fine-tuned, has the potential to improve requirements elicitation and specification, paving the way for future extensions into areas such as defect identification, testcasegeneration, and agile user story creation.",,originally announced May 2025.
https://arxiv.org/abs/2503.01319,ABFS: Natural Robustness Testing for LLM-based NLP Software,"Mingxuan Xiao, Yan Xiao, Shunhui Ji, Yunhe Li, Lei Xue, Pengcheng Zhang",https://arxiv.org/pdf/2503.01319,"Owing to the exceptional performance of Large Language Models (LLMs) in Natural Language Processing (NLP) tasks, LLM-based NLP software has rapidly gained traction across various domains, such as financial analysis and content moderation. However, these applications frequently exhibit robustness deficiencies, where slight perturbations in input (prompt+example) may lead to erroneous outputs. Current robustness testing methods face two main limitations: (1) low testing effectiveness, limiting the applicability of LLM-based software in safety-critical scenarios, and (2) insufficient naturalness of test cases, reducing the practical value of testing outcomes. To address these issues, this paper proposes ABFS, a straightforward yet effective automated testing method that, for the first time, treats the input prompts and examples as a unified whole for robustness testing. Specifically, ABFS formulates the testing process as a combinatorial optimization problem, employing Best-First Search to identify successful test cases within the perturbation space and designing a novel Adaptive control strategy to enhance test case naturalness. We evaluate the robustness testing performance of ABFS on three datasets across five threat models. On Llama2-13b, the traditional StressTest achieves only a 13.273% success rate, while ABFS attains a success rate of 98.064%, supporting a more comprehensive robustness assessment before software deployment. Compared to baseline methods, ABFS introduces fewer modifications to the original input and consistently generates test cases with superior naturalness. Furthermore, testcasesgenerated by ABFS exhibit stronger transferability and higher testing efficiency, significantly reducing testing costs.",,originally announced March 2025.
https://arxiv.org/abs/2412.21016,Assessing the Robustness of LLM-based NLP Software via Automated Testing,"Mingxuan Xiao, Yan Xiao, Shunhui Ji, Hanbo Cai, Lei Xue, Pengcheng Zhang",https://arxiv.org/pdf/2412.21016,"Benefiting from the advancements in LLMs, NLP software has undergone rapid development. Such software is widely employed in various safety-critical tasks, such as financial sentiment analysis, toxic content moderation, and log generation. Unlike traditional software, LLM-based NLP software relies on prompts and examples as inputs. Given the complexity of LLMs and the unpredictability of real-world inputs, quantitatively assessing the robustness of such software is crucial. However, to the best of our knowledge, no automated robustness testing methods have been specifically designed to evaluate the overall inputs of LLM-based NLP software. To this end, this paper introduces the first AutOmated Robustness Testing frAmework, AORTA, which reconceptualizes the testing process into a combinatorial optimization problem. Existing testing methods designed for DNN-based software can be applied to LLM-based software by AORTA, but their effectiveness is limited. To address this, we propose a novel testing method for LLM-based software within AORTA called Adaptive Beam Search. ABS is tailored for the expansive feature space of LLMs and improves testing effectiveness through an adaptive beam width and the capability for backtracking. We successfully embed 18 test methods in the designed framework AORTA and compared the test validity of ABS with three datasets and five threat models. ABS facilitates a more comprehensive and accurate robustness assessment before software deployment, with an average test success rate of 86.138%. Compared to the currently best-performing baseline PWWS, ABS significantly reduces the computational overhead by up to 3441.895 seconds per successful test case and decreases the number of queries by 218.762 times on average. Furthermore, testcasesgenerated by ABS exhibit greater naturalness and transferability.",,"v1 submitted 30 December, 2024"
https://arxiv.org/abs/2504.17203,High-Fidelity And Complex TestDataGeneration For Real-World SQL Code Generation Services,"Shivasankari Kannan, Yeounoh Chung, Amita Gondi, Tristan Swadell, Fatma Ozcan",https://arxiv.org/pdf/2504.17203,"The demand for high-fidelity test data is paramount in industrial settings where access to production data is largely restricted. Traditional data generation methods often fall short, struggling with low-fidelity and the ability to model complex data structures and semantic relationships that are critical for testing complex SQL code generation services like Natural Language to SQL (NL2SQL). In this paper, we address the critical need for generating syntactically correct and semantically ``meaningful'' mock data for complex schema that includes columns with nested structures that we frequently encounter in Google SQL code generation workloads. We highlight the limitations of existing approaches used in production, particularly their inability to handle large and complex schema, as well as the lack of semantically coherent test data that lead to limited test coverage. We demonstrate that by leveraging Large Language Models (LLMs) and incorporating strategic pre- and post-processing steps, we can generate realistic high-fidelity test data that adheres to complex structural constraints and maintains semantic integrity to the test targets (SQL queries/functions). This approach supports comprehensive testing of complex SQL queries involving joins, aggregations, and even deeply nested subqueries, ensuring robust evaluation of SQL code generation services, like NL2SQL and SQL Code Assistant services. Our results demonstrate the practical utility of an out-of-the-box LLM (\textit{gemini}) based testdatageneration for industrial SQL code generation services where generating realistic test data is essential due to the frequent unavailability of production datasets.",,originally announced April 2025.
https://arxiv.org/abs/2506.02529,Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs,"Nguyen-Khang Le, Quan Minh Bui, Minh Ngoc Nguyen, Hiep Nguyen, Trung Vo, Son T. Luu, Shoshin Nomura, Minh Le Nguyen",https://arxiv.org/pdf/2506.02529,"Web applications are critical to modern software ecosystems, yet ensuring their reliability remains challenging due to the complexity and dynamic nature of web interfaces. Recent advances in large language models (LLMs) have shown promise in automating complex tasks, but limitations persist in handling dynamic navigation flows and complex form interactions. This paper presents an automated system for generating test cases for two key aspects of web application testing: site navigation and form filling. For site navigation, the system employs screen transition graphs and LLMs to model navigation flows and generate test scenarios. For form filling, it uses state graphs to handle conditional forms and automates Selenium script generation. Key contributions include: (1) a novel integration of graph structures and LLMs for site navigation testing, (2) a state graph-based approach for automating form-filling test cases, and (3) a comprehensive dataset for evaluating form-interaction testing. Experimental results demonstrate the system's effectiveness in improving testcoverage and robustness, advancing the state of web application testing.",,originally announced June 2025.
https://arxiv.org/abs/2506.00520,Temac: Multi-Agent Collaboration for Automated Web GUI Testing,"Chenxu Liu, Zhiyu Gu, Guoquan Wu, Ying Zhang, Jun Wei, Tao Xie",https://arxiv.org/pdf/2506.00520,"Quality assurance of web applications is critical, as web applications play an essential role in people's daily lives. To reduce labor costs, automated web GUI testing (AWGT) is widely adopted, exploring web applications via GUI actions such as clicks and text inputs. However, these approaches face limitations in generating continuous and meaningful action sequences capable of covering complex functionalities. Recent work incorporates large language models (LLMs) for GUI testing. However, these approaches face various challenges, including low efficiency of LLMs, high complexity of rich web application contexts, and a low success rate of LLMs in executing GUI tasks. To address these challenges, in this paper, we propose Temac, an approach that enhances AWGT using LLM-based multi-agent collaboration to increase code coverage. Temac is motivated by our insight that LLMs can enhance AWGT in executing complex functionalities, while the information discovered during AWGT can, in turn, be provided as the domain knowledge to improve the LLM-based task execution. Specifically, given a web application, Temac initially runs an existing approach to broadly explore application states. When the testingcoverage stagnates, Temac then employs LLM-based agents to summarize the collected information to form a knowledge base and to infer not-covered functionalities. Guided by this knowledge base, Temac finally uses specialized LLM-based agents to target and execute the not-covered functionalities, reaching deeper states beyond those explored by the existing approach. Our evaluation results show that Temac exceeds state-of-the-art approaches from 12.5% to 60.3% on average code coverage on six complex open-source web applications, while revealing 445 unique failures in the top 20 real-world web applications. These results strongly demonstrate the effectiveness and the general applicability of Temac.",,originally announced June 2025.
https://arxiv.org/abs/2503.15079,LogiAgent: Automated Logical Testing for REST Systems with LLM-Based Multi-Agents,"Ke Zhang, Chenxi Zhang, Chong Wang, Chi Zhang, YaChen Wu, Zhenchang Xing, Yang Liu, Qingshan Li, Xin Peng",https://arxiv.org/pdf/2503.15079,"Automated testing for REST APIs has become essential for ensuring the correctness and reliability of modern web services. While existing approaches primarily focus on detecting server crashes and error codes, they often overlook logical issues that arise due to evolving business logic and domain-specific requirements. To address this limitation, we propose LogiAgent, a novel approach for logical testing of REST systems. Built upon a large language model (LLM)-driven multi-agent framework, LogiAgent integrates a Test Scenario Generator, API Request Executor, and API Response Validator to collaboratively generate, execute, and validate API test scenarios. Unlike traditional testing methods that focus on status codes like 5xx, LogiAgent incorporates logical oracles that assess responses based on business logic, ensuring more comprehensive testing. The system is further enhanced by an Execution Memory component that stores historical API execution data for contextual consistency. We conduct extensive experiments across 12 real-world REST systems, demonstrating that LogiAgent effectively identifies 234 logical issues with an accuracy of 66.19%. Additionally, it basically excels in detecting server crashes and achieves superior testcoverage compared to four state-of-the-art REST API testing tools. An ablation study confirms the significant contribution of LogiAgent's memory components to improving testcoverage.",,originally announced March 2025.
https://arxiv.org/abs/2310.15780,Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions,"Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Xing Che, Dandan Wang, Qing Wang",https://arxiv.org/pdf/2310.15780,"Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testingcoverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32% in activity coverage, and detects 31% more bugs at a faster rate. Moreover, GPTDroid identify 53 new bugs on Google Play, of which 35 have been confirmed and fixed.",,originally announced October 2023.
https://arxiv.org/abs/2305.09434,Chatting with GPT-3 for Zero-Shot Human-Like Mobile Automated GUI Testing,"Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Xing Che, Dandan Wang, Qing Wang",https://arxiv.org/pdf/2305.09434,"Mobile apps are indispensable for people's daily life, and automated GUI (Graphical User Interface) testing is widely used for app quality assurance. There is a growing interest in using learning-based techniques for automated GUI testing which aims at generating human-like actions and interactions. However, the limitations such as low testingcoverage, weak generalization, and heavy reliance on training data, make an urgent need for a more effective approach to generate human-like actions to thoroughly test mobile apps. Inspired by the success of the Large Language Model (LLM), e.g., GPT-3 and ChatGPT, in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within it, we extract the static context of the GUI page and the dynamic context of the iterative testing process, design prompts for inputting this information to LLM, and develop a neural matching network to decode the LLM's output into actionable steps to execute the app. We evaluate GPTDroid on 86 apps from Google Play, and its activity coverage is 71%, with 32% higher than the best baseline, and can detect 36% more bugs with faster speed than the best baseline. GPTDroid also detects 48 new bugs on the Google Play with 25 of them being confirmed/fixed. We further summarize the capabilities of GPTDroid behind the superior performance, including semantic text input, compound action, long meaningful test trace, and test case prioritization.",,originally announced May 2023.
https://arxiv.org/abs/2212.04732,Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing,"Zhe Liu, Chunyang Chen, Junjie Wang, Xing Che, Yuekai Huang, Jun Hu, Qing Wang",https://arxiv.org/pdf/2212.04732,"Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page which remains a prominent obstacle for testingcoverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.",,originally announced December 2022.
https://arxiv.org/abs/2506.07594,Evaluating LLMs Effectiveness in Detecting and Correcting Test Smells: An Empirical Study,"E. G. Santana Jr, Jander Pereira Santos Junior, Erlon P. Almeida, Iftekhar Ahmed, Paulo Anselmo da Mota Silveira Neto, Eduardo Santana de Almeida",https://arxiv.org/pdf/2506.07594,"Test smells indicate poor development practices in test code, reducing maintainability and reliability. While developers often struggle to prevent or refactor these issues, existing tools focus primarily on detection rather than automated refactoring. Large Language Models (LLMs) have shown strong potential in code understanding and transformation, but their ability to both identify and refactor test smells remains underexplored. We evaluated GPT-4-Turbo, LLaMA 3 70B, and Gemini-1.5 Pro on Python and Java test suites, using PyNose and TsDetect for initial smell detection, followed by LLM-driven refactoring. Gemini achieved the highest detection accuracy (74.35\% Python, 80.32\% Java), while LLaMA was lowest. All models could refactor smells, but effectiveness varied, sometimes introducing new smells. Gemini also improved testcoverage, unlike GPT-4 and LLaMA, which often reduced it. These results highlight LLMs' potential for automated test smell refactoring, with Gemini as the strongest performer, though challenges remain across languages and smell types.",,originally announced June 2025.
https://arxiv.org/abs/2504.05500,Prism: Dynamic and Flexible Benchmarking of LLMsCode Generation with Monte Carlo Tree Search,"Vahid Majdinasab, Amin Nikanjam, Foutse Khomh",https://arxiv.org/pdf/2504.05500,"The rapid advancement of Large Language Models (LLMs) has outpaced traditional evaluation methods. Static benchmarks fail to capture the depth and breadth of LLM capabilities and eventually become obsolete, while most dynamic approaches either rely too heavily on LLM-based evaluation or remain constrained by predefined test sets. We introduce Prism, a flexible, dynamic benchmarking framework designed for comprehensive LLM assessment. Prism builds on three key components: (1) a tree-based state representation that models evaluation as a Markov Decision Process, (2) a Monte Carlo Tree Search algorithm adapted to uncover challenging evaluation scenarios, and (3) a multi-agent evaluation pipeline that enables simultaneous assessment of diverse capabilities. To ensure robust evaluation, Prism integrates structural measurements of tree exploration patterns with performance metrics across difficulty levels, providing detailed diagnostics of error patterns, testcoverage, and solution approaches. Through extensive experiments on five state-of-the-art LLMs, we analyze how model architecture and scale influence code generation performance across varying task difficulties. Our results demonstrate Prism's effectiveness as a dynamic benchmark that evolves with model advancements while offering deeper insights into their limitations.",,"v1 submitted 7 April, 2025"
https://arxiv.org/abs/2502.19852,ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments,"Hojae Han, Seung-won Hwang, Rajhans Samdani, Yuxiong He",https://arxiv.org/pdf/2502.19852,"Large language models (LLMs) have proven invaluable for code generation, particularly in interactive settings. However, existing code generation benchmarks fail to capture the diverse feedback encountered in multi-turn interactions, limiting our ability to evaluate LLMs in these contexts. To address this gap, we present a set of novel benchmarks that explicitly model the quality of feedback provided to code generation LLMs. Our contributions are threefold: First, we introduce CONVCODEWORLD, a novel and reproducible environment for benchmarking interactive code generation. CONVCODEWORLD simulates 9 distinct interactive code generation scenarios while systematically combining three types of feedback: (a) compilation feedback; (b) execution feedback with varying testcoverage; (c) verbal feedback generated by GPT-4o with different levels of expertise. Second, we introduce CONVCODEBENCH, a fast, static version of benchmark that uses pre-generated feedback logs, eliminating the need for costly dynamic verbal feedback generation while maintaining strong Spearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third, extensive evaluations of both closed-source and open-source LLMs including R1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies significantly based on the feedback provided; (b) Weaker LLMs, with sufficient feedback, can outperform single-turn results of state-of-the-art LLMs without feedback; (c) Training on a specific feedback combination can limit an LLM's ability to utilize unseen combinations; (d) LLMs solve problems in fewer turns (high MRR) may not solve as many problems overall (high Recall), and vice versa. All implementations and benchmarks will be made publicly available at https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld",,originally announced February 2025.
https://arxiv.org/abs/2501.09866,Fine-grained Testing for Autonomous Driving Software: a Study on Autoware with LLM-driven Unit Testing,"Wenhan Wang, Xuan Xie, Yuheng Huang, Renzhi Wang, An Ran Chen, Lei Ma",https://arxiv.org/pdf/2501.09866,"Testing autonomous driving systems (ADS) is critical to ensuring their reliability and safety. Existing ADS testing works focuses on designing scenarios to evaluate system-level behaviors, while fine-grained testing of ADS source code has received comparatively little attention. To address this gap, we present the first study on testing, specifically unit testing, for ADS source code. Our study focuses on an industrial ADS framework, Autoware. We analyze both human-written test cases and those generated by large language models (LLMs). Our findings reveal that human-written test cases in Autoware exhibit limited testcoverage, and significant challenges remain in applying LLM-generated tests for Autoware unit testing. To overcome these challenges, we propose AwTest-LLM, a novel approach to enhance testcoverage and improve test case pass rates across Autoware packages.",,originally announced January 2025.
https://arxiv.org/abs/2410.15037,mHumanEval -- A Multilingual Benchmark to Evaluate Large Language Models for Code Generation,"Nishat Raihan, Antonios Anastasopoulos, Marcos Zampieri",https://arxiv.org/pdf/2410.15037,"Recent advancements in large language models (LLMs) have significantly enhanced code generation from natural language prompts. The HumanEval Benchmark, developed by OpenAI, remains the most widely used code generation benchmark. However, this and other CodeLLM benchmarks face critical limitations, particularly in task diversity, testcoverage, and linguistic scope. Current evaluations primarily focus on English-to-Python conversion tasks with limited test cases, potentially overestimating model performance. While recent works have addressed testcoverage and programming language (PL) diversity, code generation from low-resource language prompts remains largely unexplored. To address this gap, we introduce mHumanEval, an extended benchmark supporting prompts in over 200 natural languages. We employ established machine translation methods to compile the benchmark, coupled with a quality assurance process. Furthermore, we provide expert human translations for 15 diverse natural languages (NLs). We conclude by analyzing the multilingual code generation capabilities of state-of-the-art (SOTA) CodeLLMs, offering insights into the current landscape of cross-lingual code generation.",,"v1 submitted 19 October, 2024"
https://arxiv.org/abs/2409.13642,A Multi-Agent Approach to Fault Localization via Graph-Based Retrieval and Reflexion,"Md Nakhla Rafi, Dong Jae Kim, Tse-Hsun Chen, Shaowei Wang",https://arxiv.org/pdf/2409.13642,"Identifying and resolving software faults remains a challenging and resource-intensive process. Traditional fault localization techniques, such as Spectrum-Based Fault Localization (SBFL), leverage statistical analysis of testcoverage but often suffer from limited accuracy. While learning-based approaches improve fault localization, they demand extensive training datasets and high computational resources. Recent advances in Large Language Models (LLMs) offer new opportunities by enhancing code understanding and reasoning. However, existing LLM-based fault localization techniques face significant challenges, including token limitations, performance degradation with long inputs, and scalability issues in complex software systems. To overcome these obstacles, we propose LLM4FL, a multi-agent fault localization framework that utilizes three specialized LLM agents. First, the Context Extraction Agent applies an order-sensitive segmentation strategy to partition large coverage data within the LLM's token limit, analyze failure context, and prioritize failure-related methods. The Debugger Agent then processes the extracted data, which employs graph-based retrieval-augmented code navigation to reason about failure causes and rank suspicious methods. Finally, the Reviewer Agent re-evaluates the identified faulty methods using verbal reinforcement learning, engaging in self-criticism and iterative refinement. Evaluated on the Defects4J (V2.0.0) benchmark, which includes 675 faults from 14 Java projects, LLM4FL achieves an 18.55\% improvement in Top-1 accuracy over AutoFL and 4.82\% over SoapFL. It outperforms supervised techniques such as DeepFL and Grace, all without requiring task-specific training. Furthermore, its coverage segmentation and prompt chaining strategies enhance performance, increasing Top-1 accuracy by up to 22\%.",,"v1 submitted 20 September, 2024"
https://arxiv.org/abs/2310.01726,Large Language Models for Test-Free Fault Localization,"Aidan Z. H. Yang, Ruben Martins, Claire Le Goues, Vincent J. Hellendoorn",https://arxiv.org/pdf/2310.01726,"Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any testcoverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3%-54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.",,originally announced October 2023.
https://arxiv.org/abs/2308.09895,Knowledge Transfer from High-Resource to Low-Resource Programming Languages for CodeLLMs,"Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Anders Freeman, Carolyn Jane Anderson, Molly Q Feldman, Michael Greenberg, Abhinav Jangda, Arjun Guha",https://arxiv.org/pdf/2308.09895,"Over the past few years, Large Language Models of Code (CodeLLMs) have started to have a significant impact on programming practice. CodeLLMs are also emerging as building blocks for research in programming languages and software engineering. However, CodeLLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available. Low resource languages include OCaml, Racket, and several others. This paper presents an effective approach for boosting the performance of CodeLLMs on low-resource languages using semi-synthetic data. Our approach, MultiPL-T, translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a CodeLLM to synthesize tests for commented code from a high-resource language, filtering out faulty tests and code with low testcoverage. 2) We use a CodeLLM to translate Python code to a target low-resource language, and use tests to validate the translation. We apply this approach to generate tens of thousands of validated training items for Julia, Lua, OCaml, R, and Racket. Furthermore, we use an open model (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done. With MultiPL-T generated data, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket. On established benchmarks (MultiPL-E), these models outperform other open CodeLLMs. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.",,"v1 submitted 18 August, 2023"
https://arxiv.org/abs/2506.17647,Improving Compiler Bug Isolation by Leveraging Large Language Models,"Yixian Qi, Jiajun Jiang, Fengjie Li, Bowen Chen, Hongyu Zhang, Junjie Chen",https://arxiv.org/pdf/2506.17647,"Compilers play a foundational role in building reliable software systems, and bugs within them can lead to catastrophic consequences. The compilation process typically involves hundreds of files, making traditional automated bug isolation techniques inapplicable due to scalability or effectiveness issues. Current mainstream compiler bug localization techniques have limitations in test program mutation and resource consumption. Inspired by the recent advances of pre-trained Large Language Models (LLMs), we propose an innovative approach named AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2) employs specialized prompts to guide LLM in reordering suspicious file rankings. This approach leverages four types of information: the failing test program, source file function summaries, lists of suspicious files identified through analyzing testcoverage, as well as compilation configurations with related output messages, resulting in a refined ranking of suspicious files. Our evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and FuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers demonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%, 300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL, respectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the ablation study underscores the significance of each component in our approach.",,originally announced June 2025.
https://arxiv.org/abs/2407.19053,A Study of Using Multimodal LLMs for Non-Crash Functional BugDetection in Android Apps,"Bangyan Ju, Jin Yang, Tingting Yu, Tamerlan Abdullayev, Yuanyuan Wu, Dingbang Wang, Yu Zhao",https://arxiv.org/pdf/2407.19053,"Numerous approaches employing various strategies have been developed to test the graphical user interfaces (GUIs) of mobile apps. However, traditional GUI testing techniques, such as random and model-based testing, primarily focus on generating test sequences that excel in achieving high code coverage but often fail to act as effective test oracles for non-crash functional (NCF) bugdetection. To tackle these limitations, this study empirically investigates the capability of leveraging large language models (LLMs) to be test oracles to detect NCF bugs in Android apps. Our intuition is that the training corpora of LLMs, encompassing extensive mobile app usage and bug report descriptions, enable them with the domain knowledge relevant to NCF bugdetection. We conducted a comprehensive empirical study to explore the effectiveness of LLMs as test oracles for detecting NCF bugs in Android apps on 71 well-documented NCF bugs. The results demonstrated that LLMs achieve a 49% bugdetection rate, outperforming existing tools for detecting NCF bugs in Android apps. Additionally, by leveraging LLMs to be test oracles, we successfully detected 24 previously unknown NCF bugs in 64 Android apps, with four of these bugs being confirmed or fixed. However, we also identified limitations of LLMs, primarily related to performance degradation, inherent randomness, and false positives. Our study highlights the potential of leveraging LLMs as test oracles for Android NCF bugdetection and suggests directions for future research.",,originally announced July 2024.
https://arxiv.org/abs/2404.08948,Large Language Models for Mobile GUI Text Input Generation: An Empirical Study,"Chenhui Cui, Tao Li, Junjie Wang, Chunyang Chen, Dave Towey, Rubing Huang",https://arxiv.org/pdf/2404.08948,"Mobile applications have become an essential part of our daily lives, making ensuring their quality an important activity. Graphical User Interface (GUI) testing is a quality assurance method that has frequently been used for mobile apps. When conducting GUI testing, it is important to generate effective text inputs for the text-input components. Some GUIs require these text inputs to be able to move from one page to the next: This can be a challenge to achieving complete UI exploration. Recently, Large Language Models (LLMs) have demonstrated excellent text-generation capabilities. To the best of our knowledge, there has not yet been any empirical study to evaluate different pre-trained LLMs' effectiveness at generating text inputs for mobile GUI testing. This paper reports on a large-scale empirical study that extensively investigates the effectiveness of nine state-of-the-art LLMs in Android text-input generation for UI pages. We collected 114 UI pages from 62 open-source Android apps and extracted contextual information from the UI pages to construct prompts for LLMs to generate text inputs. The experimental results show that some LLMs can generate more effective and higher-quality text inputs, achieving a 50.58% to 66.67% page-pass-through rate (PPTR). We also found that using more complete UI contextual information can increase the PPTRs of LLMs for generating text inputs. We conducted an experiment to evaluate the bug-detection capabilities of LLMs by directly generating invalid text inputs. We collected 37 real-world bugs related to text inputs. The results show that using LLMs to directly generate invalid text inputs for bugdetection is insufficient: The bug-detection rates of the nine LLMs are all less than 23%. In addition, we also describe six insights gained regarding the use of LLMs for Android testing: These insights will benefit the Android testing community.",,"v1 submitted 13 April, 2024"
https://arxiv.org/abs/2506.10322,Minimizing False Positives in Static BugDetection via LLM-Enhanced Path Feasibility Analysis,"Xueying Du, Kai Yu, Chong Wang, Yi Zou, Wentai Deng, Zuoyu Ou, Xin Peng, Lingming Zhang, Yiling Lou",https://arxiv.org/pdf/2506.10322,"Static bug analyzers play a crucial role in ensuring software quality. However, existing analyzers for bugdetection in large codebases often suffer from high false positive rates. This is primarily due to the limited capabilities of analyzers in path feasibility validation with multiple conditional branches and complex data dependencies. While current LLM-based approaches attempt to address this issue, their effectiveness remains limited due to insufficient constraint cascade analysis and scalability challenges in large projects. To address this challenge, we propose an iterative path feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted constraint reasoning, and key context-aware analysis driven by agent planning, LLM4PFA effectively enhances complex inter-procedural path feasibility analysis for minimizing false positives in static bugdetection. Evaluation results show that LLM4PFA precisely filters out 72% to 96% false positives reported during static bugdetection, significantly outperforming all the baselines by 41.1% - 105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true positives.",,originally announced June 2025.
https://arxiv.org/abs/2505.10375,Are Sparse Autoencoders Useful for Java Function BugDetection?,"Rui Melo, Claudia Mamede, Andre Catarino, Rui Abreu, Henrique Lopes Cardoso",https://arxiv.org/pdf/2505.10375,"Software vulnerabilities such as buffer overflows and SQL injections are a major source of security breaches. Traditional methods for vulnerability detection remain essential but are limited by high false positive rates, scalability issues, and reliance on manual effort. These constraints have driven interest in AI-based approaches to automated vulnerability detection and secure code generation. While Large Language Models (LLMs) have opened new avenues for classification tasks, their complexity and opacity pose challenges for interpretability and deployment. Sparse Autoencoder offer a promising solution to this problem. We explore whether SAEs can serve as a lightweight, interpretable alternative for bugdetection in Java functions. We evaluate the effectiveness of SAEs when applied to representations from GPT-2 Small and Gemma 2B, examining their capacity to highlight buggy behaviour without fine-tuning the underlying LLMs. We found that SAE-derived features enable bugdetection with an F1 score of up to 89%, consistently outperforming fine-tuned transformer encoder baselines. Our work provides the first empirical evidence that SAEs can be used to detect software bugs directly from the internal representations of pretrained LLMs, without any fine-tuning or task-specific supervision.",,"v1 submitted 15 May, 2025"
https://arxiv.org/abs/2504.21569,A Systematic Literature Review of Parameter-Efficient Fine-Tuning for Large Code Models,"Md Zahidul Haque, Saima Afrin, Antonio Mastropaolo",https://arxiv.org/pdf/2504.21569,"The rise of Artificial Intelligence (AI)-and particularly Large Language Models (LLMs) for code-has reshaped Software Engineering (SE) by enabling the automation of tasks such as code generation, bugdetection, and repair. However, these models require significant computational resources for training and fine-tuning, posing challenges for real-world adoption in resource-constrained environments. To address this, the research community has increasingly turned to Parameter-Efficient Fine-Tuning (PEFT)-a class of techniques that enables the adaptation of large models by updating only a small subset of parameters, rather than the entire model. In this Systematic Literature Review (SLR), we examine the growing application of PEFT techniques-across a wide range of software engineering tasks. We analyze how these methods are used to optimize various deep learning (DL) architectures, focusing on their impact on both performance and efficiency. Our study synthesizes findings from 27 peer-reviewed papers, identifying patterns in configuration strategies and adaptation trade-offs. The outcome of this review is a comprehensive taxonomy that categorizes PEFT usage by task type, distinguishing between generative (e.g., Code Summarization) and non-generative (e.g., Code Clone Detection) scenarios. Our findings aim to inform future research and guide the practical deployment of PEFT in sustainable, AI-powered software development. Our artifacts are publicly available at https://github.com/alvi75/SLR-PEFT",,"v1 submitted 29 April, 2025"
https://arxiv.org/abs/2504.11711,"The Hitchhiker's Guide to Program Analysis, Part II: Deep Thoughts by LLMs","Haonan Li, Hang Zhang, Kexin Pei, Zhiyun Qian",https://arxiv.org/pdf/2504.11711,"Static analysis plays a crucial role in software vulnerability detection, yet faces a persistent precision-scalability tradeoff. In large codebases like the Linux kernel, traditional static analysis tools often generate excessive false positives due to simplified vulnerability modeling and overapproximation of path and data constraints. While large language models (LLMs) demonstrate promising code understanding capabilities, their direct application to program analysis remains unreliable due to inherent reasoning limitations. We introduce BugLens, a post-refinement framework that significantly enhances static analysis precision for bugdetection. BugLens guides LLMs through structured reasoning steps to assess security impact and validate constraints from the source code. When evaluated on Linux kernel taint-style bugsdetected by static analysis tools, BugLens improves precision approximately 7-fold (from 0.10 to 0.72), substantially reducing false positives while uncovering four previously unreported vulnerabilities. Our results demonstrate that a well-structured, fully automated LLM-based workflow can effectively complement and enhance traditional static analysis techniques.",,"v1 submitted 15 April, 2025"
https://arxiv.org/abs/2503.22388,Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors,"Zhiyu Yang, Shuo Wang, Yukun Yan, Yang Deng",https://arxiv.org/pdf/2503.22388,"LLMs are transforming software development, yet current code generation and code repair benchmarks mainly assess syntactic and functional correctness in simple, single-error cases. LLMs' capabilities to autonomously find and fix runtime logical errors in complex data science code remain largely unexplored. To address this gap, we introduce DSDBench: the Data Science Debugging Benchmark, the first benchmark for systematic evaluation of LLMs on multi-hop error tracing and multi-bugdetection in data science code debugging. DSDBench adapts datasets from existing data science task benchmarks, such as DABench and MatPlotBench, featuring realistic data science debugging tasks with automatically synthesized multi-hop, multi-bug code snippets. DSDBench includes 1,117 annotated samples with 741 cause-effect error pairs and runtime error messages. Evaluations of state-of-the-art LLMs on DSDBench show significant performance gaps, highlighting challenges in debugging logical runtime errors in data science code. DSDBench offers a crucial resource to evaluate and improve LLMs' debugging and reasoning capabilities, enabling more reliable AI-assisted data science in the future. DSDBench is publicly available at github.com/KevinCL16/DSDBench.",,"v1 submitted 28 March, 2025"
https://arxiv.org/abs/2412.18531,Automated Code Review In Practice,"Umut Cihan, Vahid Haratian, Arda İçöz, Mert Kaan Gül, Ömercan Devran, Emircan Furkan Bayendur, Baykal Mehmet Uçar, Eray Tüzün",https://arxiv.org/pdf/2412.18531,"Code review is a widespread practice to improve software quality and transfer knowledge. It is often seen as time-consuming due to the need for manual effort and potential delays. Several AI-assisted tools, such as Qodo, GitHub Copilot, and Coderabbit, provide automated reviews using large language models (LLMs). The effects of such tools in the industry are yet to be examined. This study examines the impact of LLM-based automated code review tools in an industrial setting. The study was conducted within a software development environment that adopted an AI-assisted review tool (based on open-source Qodo PR Agent). Around 238 practitioners across ten projects had access to the tool. We focused on three projects with 4,335 pull requests, 1,568 of which underwent automated reviews. Data collection comprised three sources: (1) a quantitative analysis of pull request data, including comment labels indicating whether developers acted on the automated comments, (2) surveys sent to developers regarding their experience with reviews on individual pull requests, and (3) a broader survey of 22 practitioners capturing their general opinions on automated reviews. 73.8% of automated comments were resolved. However, the average pull request closure duration increased from five hours 52 minutes to eight hours 20 minutes, with varying trends across projects. Most practitioners reported a minor improvement in code quality due to automated reviews. The LLM-based tool proved useful in software development, enhancing bugdetection, increasing awareness of code quality, and promoting best practices. However, it also led to longer pull request closure times and introduced drawbacks like faulty reviews, unnecessary corrections, and irrelevant comments.",,"v1 submitted 24 December, 2024"
https://arxiv.org/abs/2412.16620,A Large-scale Empirical Study on Fine-tuning Large Language Models for Unit Testing,"Ye Shang, Quanjun Zhang, Chunrong Fang, Siqi Gu, Jianyi Zhou, Zhenyu Chen",https://arxiv.org/pdf/2412.16620,"Unit testing plays a pivotal role in software development, improving software quality and reliability. However, generating effective test cases manually is time-consuming, prompting interest in unit testing research. Recently, Large Language Models (LLMs) have shown potential in various unit testing tasks, including test generation, assertion generation, and test evolution, but existing studies are limited in scope and lack a systematic evaluation of the effectiveness of LLMs. To bridge this gap, we present a large-scale empirical study on fine-tuning LLMs for unit testing. Our study involves three unit testing tasks, five benchmarks, eight evaluation metrics, and 37 popular LLMs across various architectures and sizes, consuming over 3,000 NVIDIA A100 GPU hours. We focus on three key research questions: (1) the performance of LLMs compared to state-of-the-art methods, (2) the impact of different factors on LLM performance, and (3) the effectiveness of fine-tuning versus prompt engineering. Our findings reveal that LLMs outperform existing state-of-the-art approaches on all three unit testing tasks across nearly all metrics, highlighting the potential of fine-tuning LLMs in unit testing tasks. Furthermore, large-scale, decoder-only models achieve the best results across tasks, while encoder-decoder models perform better under the same parameter scale. Additionally, the comparison of the performance between fine-tuning and prompt engineering approaches reveals the considerable potential capability of the prompt engineering approach in unit testing tasks. We then discuss the concerned issues on the test generation task, including data leakage issues, bugdetection capabilities, and metrics comparisons. Finally, we further pinpoint carious practical guidelines for LLM-based approaches to unit testing tasks in the near future.",,originally announced December 2024.
https://arxiv.org/abs/2411.17981,Engineering Trustworthy Software: A Mission for LLMs,Marco Vieira,https://arxiv.org/pdf/2411.17981,"LLMs are transforming software engineering by accelerating development, reducing complexity, and cutting costs. When fully integrated into the software lifecycle they will drive design, development and deployment while facilitating early bugdetection, continuous improvement, and rapid resolution of critical issues. However, trustworthy LLM-driven software engineering requires addressing multiple challenges such as accuracy, scalability, bias, and explainability.",,originally announced November 2024.
https://arxiv.org/abs/2411.07586,A Comprehensive Survey of AI-Driven Advancements and Techniques in Automated Program Repair and Code Generation,"Avinash Anand, Akshit Gupta, Nishchay Yadav, Shaurya Bajaj",https://arxiv.org/pdf/2411.07586,"Bug fixing and code generation have been core research topics in software development for many years. The recent explosive growth in Large Language Models has completely transformed these spaces, putting in reach incredibly powerful tools for both. In this survey, 27 recent papers have been reviewed and split into two groups: one dedicated to Automated Program Repair (APR) and LLM integration and the other to code generation using LLMs. The first group consists of new methods for bugdetection and repair, which include locating semantic errors, security vulnerabilities, and runtime failure bugs. The place of LLMs in reducing manual debugging efforts is emphasized in this work by APR toward context-aware fixes, with innovations that boost accuracy and efficiency in automatic debugging. The second group dwells on code generation, providing an overview of both general-purpose LLMs fine-tuned for programming and task-specific models. It also presents methods to improve code generation, such as identifier-aware training, fine-tuning at the instruction level, and incorporating semantic code structures. This survey work contrasts the methodologies in APR and code generation to identify trends such as using LLMs, feedback loops to enable iterative code improvement and open-source models. It also discusses the challenges of achieving functional correctness and security and outlines future directions for research in LLM-based software development.",,originally announced November 2024.
https://arxiv.org/abs/2409.12369,Program Slicing in the Era of Large Language Models,"Kimya Khakzad Shahandashti, Mohammad Mahdi Mohajer, Alvine Boaye Belle, Song Wang, Hadi Hemmati",https://arxiv.org/pdf/2409.12369,"Program slicing is a critical technique in software engineering, enabling developers to isolate relevant portions of code for tasks such as bugdetection, code comprehension, and debugging. In this study, we investigate the application of large language models (LLMs) to both static and dynamic program slicing, with a focus on Java programs. We evaluate the performance of four state-of-the-art LLMs- GPT-4o, GPT-3.5 Turbo, Llama-2, and Gemma-7B leveraging advanced prompting techniques, including few-shot learning and chain-of-thought reasoning. Using a dataset of 100 Java programs derived from LeetCode problems, our experiments reveal that GPT-4o performs the best in both static and dynamic slicing across other LLMs, achieving an accuracy of 60.84% and 59.69%, respectively. Our results also show that the LLMs we experimented with are yet to achieve reasonable performance for either static slicing or dynamic slicing. Through a rigorous manual analysis, we developed a taxonomy of root causes and failure locations to explore the unsuccessful cases in more depth. We identified Complex Control Flow as the most frequent root cause of failures, with the majority of issues occurring in Variable Declarations and Assignments locations. To improve the performance of LLMs, we further examined two independent strategies for prompting guided by our taxonomy, including prompt crafting, which involved refining the prompts to better guide the LLM through the slicing process, and iterative prompting, where the model receives feedback on the root cause and location of the failure and re-generates its responses. Our evaluation shows these two prompting enhancement approaches can improve accuracy by 4% and 3.9%, respectively.",,originally announced September 2024.
https://arxiv.org/abs/2409.09464,Measuring the Influence of Incorrect Code on Test Generation,"Dong Huang, Jie M. Zhang, Mark Harman, Mingzhe Du, Heming Cui",https://arxiv.org/pdf/2409.09464,"It is natural to suppose that a Large Language Model is more likely to generate correct test cases when prompted with correct code under test, compared to incorrect code under test. However, the size of this effect has never been previously measured, despite its obvious importance for both practicing software engineers and researchers. To answer the question, we conducted a comprehensive empirical study on 5 open source and 6 closed source language models, with 3 widely-used benchmark data sets together with 41 repo-level real-world examples from two different real-world data sets. Our results reveal that, when compared to incorrect code under test, LLMs prompted with correct code achieve improvements in test accuracy, code coverage, and bugdetection of 57\%, 12\%, and 24\% respectively. We further show that these scientific conclusions carry over from the three benchmark data sets to the real-world code, where tests generated for incorrect code experience a 47\% worse bugdetection rate. Finally, we report that improvements of +18\% in accuracy, +4\% coverage, and +34\% in bugdetection can be achieved by providing natural language code descriptions. These findings have actionable conclusions. For example, the 47\% reduction in real-world bugdetection is a clear concern. Fortunately, it is a concern for which our findings about the added value of descriptions offer an immediately actionable remedy.",,"v1 submitted 14 September, 2024"
https://arxiv.org/abs/2406.11731,PerfCurator: Curating a large-scale dataset of performance bug-related commits from public repositories,"Md Abul Kalam Azad, Manoj Alexender, Matthew Alexender, Syed Salauddin Mohammad Tariq, Foyzul Hassan, Probir Roy",https://arxiv.org/pdf/2406.11731,"Performance bugs challenge software development, degrading performance and wasting computational resources. Software developers invest substantial effort in addressing these issues. Curating these performance bugs can offer valuable insights to the software engineering research community, aiding in developing new mitigation strategies. However, there is no large-scale open-source performance bugs dataset available. To bridge this gap, we propose PerfCurator, a repository miner that collects performance bug-related commits at scale. PerfCurator employs PcBERT-KD, a 125M parameter BERT model trained to classify performance bug-related commits. Our evaluation shows PcBERT-KD achieves accuracy comparable to 7 billion parameter LLMs but with significantly lower computational overhead, enabling cost-effective deployment on CPU clusters. Utilizing PcBERT-KD as the core component, we deployed PerfCurator on a 50-node CPU cluster to mine GitHub repositories. This extensive mining operation resulted in the construction of a large-scale dataset comprising 114K performance bug-fix commits in Python, 217.9K in C++, and 76.6K in Java. Our results demonstrate that this large-scale dataset significantly enhances the effectiveness of data-driven performance bugdetection systems.",,originally announced June 2024.
https://arxiv.org/abs/2310.18532,SkipAnalyzer: A Tool for Static Code Analysis with Large Language Models,"Mohammad Mahdi Mohajer, Reem Aleithan, Nima Shiri Harzevili, Moshi Wei, Alvine Boaye Belle, Hung Viet Pham, Song Wang",https://arxiv.org/pdf/2310.18532,"We introduce SkipAnalyzer, a large language model (LLM)-powered tool for static code analysis. SkipAnalyzer has three components: 1) an LLM-based static bug detector that scans source code and reports specific types of bugs, 2) an LLM-based false-positive filter that can identify false-positive bugs in the results of static bug detectors (e.g., the result of step 1) to improve detection accuracy, and 3) an LLM-based patch generator that can generate patches for the detected bugs above. As a proof-of-concept, SkipAnalyzer is built on ChatGPT, which has exhibited outstanding performance in various software engineering tasks. To evaluate SkipAnalyzer, we focus on two types of typical and critical bugs that are targeted by static bugdetection, i.e., Null Dereference and Resource Leak as subjects. We employ Infer to aid the gathering of these two bug types from 10 open-source projects. Consequently, our experiment dataset contains 222 instances of Null Dereference bugs and 46 instances of Resource Leak bugs. Our study demonstrates that SkipAnalyzer achieves remarkable performance in the mentioned static analysis tasks, including bugdetection, false-positive warning removal, and bug repair. In static bugdetection, SkipAnalyzer achieves accuracy values of up to 68.37% for detecting Null Dereference bugs and 76.95% for detecting Resource Leak bugs, improving the precision of the current leading bug detector, Infer, by 12.86% and 43.13%, respectively. For removing false-positive warnings, SkipAnalyzer can reach a precision of up to 93.88% for Null Dereference bugs and 63.33% for Resource Leak bugs. Additionally, SkipAnalyzer surpasses state-of-the-art false-positive warning removal tools. Furthermore, in bug repair, SkipAnalyzer can generate syntactically correct patches to fix its detected bugs with a success rate of up to 97.30%.",,"v1 submitted 27 October, 2023"
https://arxiv.org/abs/2310.08837,"Static Code Analysis in the AI Era: An In-depth Exploration of the Concept, Function, and Potential of Intelligent Code Analysis Agents","Gang Fan, Xiaoheng Xie, Xunjin Zheng, Yinan Liang, Peng Di",https://arxiv.org/pdf/2310.08837,"The escalating complexity of software systems and accelerating development cycles pose a significant challenge in managing code errors and implementing business logic. Traditional techniques, while cornerstone for software quality assurance, exhibit limitations in handling intricate business logic and extensive codebases. To address these challenges, we introduce the Intelligent Code Analysis Agent (ICAA), a novel concept combining AI models, engineering process designs, and traditional non-AI components. The ICAA employs the capabilities of large language models (LLMs) such as GPT-3 or GPT-4 to automatically detect and diagnose code errors and business logic inconsistencies. In our exploration of this concept, we observed a substantial improvement in bugdetection accuracy, reducing the false-positive rate to 66\% from the baseline's 85\%, and a promising recall rate of 60.8\%. However, the token consumption cost associated with LLMs, particularly the average cost for analyzing each line of code, remains a significant consideration for widespread adoption. Despite this challenge, our findings suggest that the ICAA holds considerable potential to revolutionize software quality assurance, significantly enhancing the efficiency and accuracy of bugdetection in the software development process. We hope this pioneering work will inspire further research and innovation in this field, focusing on refining the ICAA concept and exploring ways to mitigate the associated costs.",,originally announced October 2023.
https://arxiv.org/abs/2307.00588,ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation,"Yutian Tang, Zhijie Liu, Zhichao Zhou, Xiapu Luo",https://arxiv.org/pdf/2307.00588,"Recent advancements in large language models (LLMs) have demonstrated exceptional success in a wide range of general domain tasks, such as question answering and following instructions. Moreover, LLMs have shown potential in various software engineering applications. In this study, we present a systematic comparison of test suites generated by the ChatGPT LLM and the state-of-the-art SBST tool EvoSuite. Our comparison is based on several critical factors, including correctness, readability, code coverage, and bugdetection capability. By highlighting the strengths and weaknesses of LLMs (specifically ChatGPT) in generating unit test cases compared to EvoSuite, this work provides valuable insights into the performance of LLMs in solving software engineering problems. Overall, our findings underscore the potential of LLMs in software engineering and pave the way for further research in this area.",,originally announced July 2023.
https://arxiv.org/abs/2306.03324,How Effective are Large Language Models in Generating Software Specifications?,"Danning Xie, Byungwoo Yoo, Nan Jiang, Mijung Kim, Lin Tan, Xiangyu Zhang, Judy S. Lee",https://arxiv.org/pdf/2306.03324,"Software specifications are essential for many Software Engineering (SE) tasks such as bugdetection and test generation. Many existing approaches are proposed to extract the specifications defined in natural language form (e.g., comments) into formal machine readable form (e.g., first order logic). However, existing approaches suffer from limited generalizability and require manual efforts. The recent emergence of Large Language Models (LLMs), which have been successfully applied to numerous SE tasks, offers a promising avenue for automating this process. In this paper, we conduct the first empirical study to evaluate the capabilities of LLMs for generating software specifications from software comments or documentation. We evaluate LLMs performance with Few Shot Learning (FSL) and compare the performance of 13 state of the art LLMs with traditional approaches on three public datasets. In addition, we conduct a comparative diagnosis of the failure cases from both LLMs and traditional methods, identifying their unique strengths and weaknesses. Our study offers valuable insights for future research to improve specification generation.",,"v1 submitted 5 June, 2023"
https://arxiv.org/abs/2506.17865,LASA: Enhancing SoC Security Verification with LLM-Aided Property Generation,"Dinesh Reddy Ankireddy, Sudipta Paria, Aritra Dasgupta, Sandip Ray, Swarup Bhunia",https://arxiv.org/pdf/2506.17865,"Ensuring the security of modern System-on-Chip (SoC) designs poses significant challenges due to increasing complexity and distributed assets across the intellectual property (IP) blocks. Formal property verification (FPV) provides the capability to model and validate design behaviors through security properties with model checkers; however, current practices require significant manual efforts to create such properties, making them time-consuming, costly, and error-prone. The emergence of Large Language Models (LLMs) has showcased remarkable proficiency across diverse domains, including HDL code generation and verification tasks. Current LLM-based techniques often produce vacuous assertions and lack efficient prompt generation, comprehensive verification, and bugdetection. This paper presents LASA, a novel framework that leverages LLMs and retrieval-augmented generation (RAG) to produce non-vacuous security properties and SystemVerilog Assertions (SVA) from design specifications and related documentation for bus-based SoC designs. LASA integrates commercial EDA tool for FPV to generate coverage metrics and iteratively refines prompts through a feedback loop to enhance coverage. The effectiveness of LASA is validated through various open-source SoC designs, demonstrating high coverage values with an average of ~88\%, denoting comprehensive verification through efficient generation of security properties and SVAs. LASA also demonstrates bugdetection capabilities, identifying five unique bugs in the buggy OpenTitan SoC from Hack@DAC'24 competition.",,originally announced June 2025.
https://arxiv.org/abs/2506.09713,A First Look at Bugs in LLM Inference Engines,"Mugeng Liu, Siqi Zhong, Weichen Bi, Yixuan Zhang, Zhiyang Chen, Zhenpeng Chen, Xuanzhe Liu, Yun Ma",https://arxiv.org/pdf/2506.09713,"Large language model-specific inference engines (in short as \emph{LLM inference engines}) have become a fundamental component of modern AI infrastructure, enabling the deployment of LLM-powered applications (LLM apps) across cloud and local devices. Despite their critical role, LLM inference engines are prone to bugs due to the immense resource demands of LLMs and the complexities of cross-platform compatibility. However, a systematic understanding of these bugs remains lacking. To bridge this gap, we present the first empirical study on bugs in LLM inference engines. We mine official repositories of 5 widely adopted LLM inference engines, constructing a comprehensive dataset of 929 real-world bugs. Through a rigorous open coding process, we analyze these bugs to uncover their symptoms, root causes, and commonality. Our findings reveal six major bug symptoms and a taxonomy of 28 root causes, shedding light on the key challenges in bugdetection and location within LLM inference engines. Based on these insights, we propose a series of actionable implications for researchers, inference engine vendors, and LLM app developers.",,originally announced June 2025.
https://arxiv.org/abs/2412.14399,LLMSA: A Compositional Neuro-Symbolic Approach to Compilation-free and Customizable Static Analysis,"Chengpeng Wang, Yifei Gao, Wuqi Zhang, Xuwei Liu, Qingkai Shi, Xiangyu Zhang",https://arxiv.org/pdf/2412.14399,"Static analysis is essential for program optimization, bugdetection, and debugging, but its reliance on compilation and limited customization hampers practical use. Advances in LLMs enable a new paradigm of compilation-free, customizable analysis via prompting. LLMs excel in interpreting program semantics on small code snippets and allow users to define analysis tasks in natural language with few-shot examples. However, misalignment with program semantics can cause hallucinations, especially in sophisticated semantic analysis upon lengthy code snippets. We propose LLMSA, a compositional neuro-symbolic approach for compilation-free, customizable static analysis with reduced hallucinations. Specifically, we propose an analysis policy language to support users decomposing an analysis problem into several sub-problems that target simple syntactic or semantic properties upon smaller code snippets. The problem decomposition enables the LLMs to target more manageable semantic-related sub-problems, while the syntactic ones are resolved by parsing-based analysis without hallucinations. An analysis policy is evaluated with lazy, incremental, and parallel prompting, which mitigates the hallucinations and improves the performance. It is shown that LLMSA achieves comparable and even superior performance to existing techniques in various clients. For instance, it attains 66.27% precision and 78.57% recall in taint vulnerability detection, surpassing an industrial approach in F1 score by 0.20.",,originally announced December 2024.
https://arxiv.org/abs/2402.10754,LLMDFA: Analyzing Dataflow in Code with Large Language Models,"Chengpeng Wang, Wuqi Zhang, Zian Su, Xiangzhe Xu, Xiaoheng Xie, Xiangyu Zhang",https://arxiv.org/pdf/2402.10754,"Dataflow analysis is a fundamental code analysis technique that identifies dependencies between program values. Traditional approaches typically necessitate successful compilation and expert customization, hindering their applicability and usability for analyzing uncompilable programs with evolving analysis needs in real-world scenarios. This paper presents LLMDFA, an LLM-powered compilation-free and customizable dataflow analysis framework. To address hallucinations for reliable results, we decompose the problem into several subtasks and introduce a series of novel strategies. Specifically, we leverage LLMs to synthesize code that outsources delicate reasoning to external expert tools, such as using a parsing library to extract program values of interest and invoking an automated theorem prover to validate path feasibility. Additionally, we adopt a few-shot chain-of-thought prompting to summarize dataflow facts in individual functions, aligning the LLMs with the program semantics of small code snippets to mitigate hallucinations. We evaluate LLMDFA on synthetic programs to detect three representative types of bugs and on real-world Android applications for customized bugdetection. On average, LLMDFA achieves 87.10% precision and 80.77% recall, surpassing existing techniques with F1 score improvements of up to 0.35. We have open-sourced LLMDFA at https://github.com/chengpeng-wang/LLMDFA.",,"v1 submitted 16 February, 2024"
https://arxiv.org/abs/2311.07957,Language Models are Better Bug Detector Through Code-Pair Classification,"Kamel Alrashedy, Ahmed Binjahlan",https://arxiv.org/pdf/2311.07957,"Large language models (LLMs) such as GPT-3.5 and CodeLlama are powerful models for code generation and understanding. Fine-tuning these models comes with a high computational cost and requires a large labeled dataset. Alternatively, in-context learning techniques allow models to learn downstream tasks with only a few examples. Recently, researchers have shown how in-context learning performs well in bugdetection and repair. In this paper, we propose code-pair classification task in which both the buggy and non-buggy versions are given to the model, and the model identifies the buggy ones. We evaluate our task in real-world dataset of bugdetection and two most powerful LLMs. Our experiments indicate that an LLM can often pick the buggy from the non-buggy version of the code, and the code-pair classification task is much easier compared to be given a snippet and deciding if and where a bug exists.",,"v1 submitted 14 November, 2023"
https://arxiv.org/abs/2307.12469,How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,"Cen Zhang, Yaowen Zheng, Mingqiang Bai, Yeting Li, Wei Ma, Xiaofei Xie, Yuekang Li, Limin Sun, Yang Liu",https://arxiv.org/pdf/2307.12469,"LLM-based (Large Language Model) fuzz driver generation is a promising research area. Unlike traditional program analysis-based method, this text-based approach is more general and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges. To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs ($8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that: - While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications; - LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process; - While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bugdetection. Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.",,"v1 submitted 23 July, 2023"
https://arxiv.org/abs/2306.00597,Analysis of ChatGPT on Source Code,"Ahmed R. Sadik, Antonello Ceravola, Frank Joublin, Jibesh Patra",https://arxiv.org/pdf/2306.00597,"This paper explores the use of Large Language Models (LLMs) and in particular ChatGPT in programming, source code analysis, and code generation. LLMs and ChatGPT are built using machine learning and artificial intelligence techniques, and they offer several benefits to developers and programmers. While these models can save time and provide highly accurate results, they are not yet advanced enough to replace human programmers entirely. The paper investigates the potential applications of LLMs and ChatGPT in various areas, such as code creation, code documentation, bugdetection, refactoring, and more. The paper also suggests that the usage of LLMs and ChatGPT is expected to increase in the future as they offer unparalleled benefits to the programming community.",,"v1 submitted 1 June, 2023"
https://arxiv.org/abs/2505.13766,Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques,Avinash Patil,https://arxiv.org/pdf/2505.13766,"SoftwareQualityAssurance (SQA) is critical for delivering reliable, secure, and efficient software products. The SoftwareQualityAssurance Process aims to provide assurance that work products and processes comply with predefined provisions and plans. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance existing SQA processes by automating tasks like requirement analysis, code review, test generation, and compliance checks. Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010, ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured frameworks for ensuring robust quality practices. This paper surveys the intersection of LLM-based SQA methods and these recognized standards, highlighting how AI-driven solutions can augment traditional approaches while maintaining compliance and process maturity. We first review the foundational software quality standards and the technical fundamentals of LLMs in software engineering. Next, we explore various LLM-based SQA applications, including requirement validation, defect detection, test generation, and documentation maintenance. We then map these applications to key software quality frameworks, illustrating how LLMs can address specific requirements and metrics within each standard. Empirical case studies and open-source initiatives demonstrate the practical viability of these methods. At the same time, discussions on challenges (e.g., data privacy, model bias, explainability) underscore the need for deliberate governance and auditing. Finally, we propose future directions encompassing adaptive learning, privacy-focused deployments, multimodal analysis, and evolving standards for AI-driven software quality.",,originally announced May 2025.
https://arxiv.org/abs/2409.01001,Beyond ChatGPT: Enhancing SoftwareQualityAssurance Tasks with Diverse LLMs and Validation Techniques,"Ratnadira Widyasari, David Lo, Lizi Liao",https://arxiv.org/pdf/2409.01001,"With the advancement of Large Language Models (LLMs), their application in SoftwareQualityAssurance (SQA) has increased. However, the current focus of these applications is predominantly on ChatGPT. There remains a gap in understanding the performance of various LLMs in this critical domain. This paper aims to address this gap by conducting a comprehensive investigation into the capabilities of several LLMs across two SQA tasks: fault localization and vulnerability detection. We conducted comparative studies using GPT-3.5, GPT-4o, and four other publicly available LLMs (LLaMA-3-70B, LLaMA-3-8B, Gemma-7B, and Mixtral-8x7B), to evaluate their effectiveness in these tasks. Our findings reveal that several LLMs can outperform GPT-3.5 in both tasks. Additionally, even the lower-performing LLMs provided unique correct predictions, suggesting the potential of combining different LLMs' results to enhance overall performance. By implementing a voting mechanism to combine the LLMs' results, we achieved more than a 10% improvement over the GPT-3.5 in both tasks. Furthermore, we introduced a cross-validation approach to refine the LLM answer by validating one LLM answer against another using a validation prompt. This approach led to performance improvements of 16% in fault localization and 12% in vulnerability detection compared to the GPT-3.5, with a 4% improvement compared to the best-performed LLMs. Our analysis also indicates that the inclusion of explanations in the LLMs' results affects the effectiveness of the cross-validation technique.",,originally announced September 2024.
https://arxiv.org/abs/2506.07486,A Framework for Creating Non-RegressiveTest Cases via Branch Consistency Analysis Driven by Descriptions,"Yuxiang Zhang, Pengyu Xue, Zhen Yang, Xiaoxue Ren, Xiang Li, Linhao Wu, Jiancheng Zhao, Xingda Yu",https://arxiv.org/pdf/2506.07486,"Automated test-generation research overwhelmingly assumes the correctness of focal methods, yet practitioners routinely face non-regression scenarios where the focal method may be defective. A baseline evaluation of EvoSuite and two leading Large Language Model (LLM)-based generators, namely ChatTester and ChatUniTest, on defective focal methods reveals that despite achieving up to 83% of branch coverage, none of the generated tests expose defects. To resolve this problem, we first construct two new benchmarks, namely Defects4J-Desc and QuixBugs-Desc, for experiments. In particular, each focal method is equipped with an extra Natural Language Description (NLD) for code functionality understanding. Subsequently, we propose DISTINCT, a Description-guided, branch-consistency analysis framework that transforms LLMs into fault-aware test generators. DISTINCT carries three iterative components: (1) a Generator that derives initial tests based on the NLDs and the focal method, (2) a Validator that iteratively fixes uncompilable tests using compiler diagnostics, and (3) an Analyzer that iteratively aligns test behavior with NLD semantics via branch-level analysis. Extensive experiments confirm the effectiveness of our approach. Compared to state-of-the-art methods, DISTINCT achieves an average improvement of 14.64% in Compilation Success Rate (CSR) and 6.66% in Passing Rate (PR) across both benchmarks. It notably enhances Defect Detection Rate (DDR) on both benchmarks, with a particularly significant gain of 149.26% observed on Defects4J-Desc. In terms of code coverage, DISTINCT improves Statement Coverage (SC) by an average of 3.77% and Branch Coverage (BC) by 5.36%. These results set a new baseline for non-regressivetest generation and highlight how description-driven reasoning enables LLMs to move beyond coverage chasing toward effective defect detection.",,originally announced June 2025.
https://arxiv.org/abs/2505.02854,Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for RegressionTesting and Open Datasets,"Masumi Morishige, Ryo Koshihara",https://arxiv.org/pdf/2505.02854,"Reproducibility and reliability remain pressing challenges for generative AI systems whose behavior can drift with each model update or prompt revision. We introduce GPR-bench, a lightweight, extensible benchmark that operationalizes regressiontesting for general purpose use cases. GPR-bench couples an open, bilingual (English and Japanese) dataset covering eight task categories (e.g., text generation, code generation, and information retrieval) and 10 scenarios in each task categories (80 total test cases for each language) with an automated evaluation pipeline that employs ""LLM-as-a-Judge"" scoring of correctness and conciseness. Experiments across three recent model versions - gpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default versus concise-writing instruction) reveal heterogeneous quality. Our results show that newer models generally improve correctness, but the differences are modest and not statistically significant, suggesting that GPR-bench may not be sufficiently challenging to differentiate between recent model versions. In contrast, the concise-writing instruction significantly enhances conciseness (+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with minimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of prompt engineering. Released under the MIT License, GPR- bench lowers the barrier to initiating reproducibility monitoring and provides a foundation for community-driven extensions, while also raising important considerations about benchmark design for rapidly evolving language models.",,originally announced May 2025.
https://arxiv.org/abs/2503.22851,RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation,"Feng Lin, Dong Jae Kim, Zhenhao Li, Jinqiu Yang, Tse-Hsun, Chen",https://arxiv.org/pdf/2503.22851,"When using LLMs to address Non-Functional Requirements (NFRs), developers may behave differently (e.g., expressing the same NFR in different words). Robust LLMs should output consistent results across these variations; however, this aspect remains underexplored. We propose RobuNFR for evaluating the robustness of LLMs in NFR-aware code generation across four NFR dimensions: design, readability, reliability, and performance, using three methodologies: prompt variation, regressiontesting, and diverse workflows. Our experiments show that RobuNFR reveals robustness issues in the tested LLMs when considering NFRs in code generation. Specifically, under prompt variation, including NFRs leads to a decrease in Pass@1 by up to 39 percent and an increase in the standard deviation from 0.48 to 2.48 compared to the baseline without NFRs (i.e., Function-Only). While incorporating NFRs generally improves overall NFR metrics, it also results in higher prompt sensitivity. In regression settings, some LLMs exhibit differences across versions, with improvements in one aspect (e.g., reduced code smells) often accompanied by regressions in another (e.g., decreased correctness), revealing inconsistencies that challenge their robustness. When varying workflows, the tested LLMs show significantly different NFR-aware code generation capabilities between two workflows: (1) integrating NFRs and functional requirements into the initial prompt and (2) enhancing Function-Only-generated code with the same NFR.",,"v1 submitted 28 March, 2025"
https://arxiv.org/abs/2503.18597,RegressionTesting with a Natural Language Oracle,Michael Pradel,https://arxiv.org/pdf/2503.18597,"As software is evolving, code changes can introduce regression bugs or affect the behavior in other unintended ways. Traditional regressiontest generation is impractical for detecting unintended behavioral changes, because it reports all behavioral differences as potential regressions. However, most code changes are intended to change the behavior in some way, e.g., to fix a bug or to add a new feature. This paper presents Testora, an automated approach that detects regressions by comparing the intentions of a code change against behavioral differences caused by the code change. Given a pull request (PR), Testora queries an LLM to generate tests that exercise the modified code, compares the behavior of the original and modified code, and classifies any behavioral differences as intended or unintended. For the classification, we present an LLM-based technique that leverages the natural language information associated with the PR, such as the title, description, and commit messages -- effectively providing a natural language oracle for regressiontesting. Applying Testora to PRs of complex and popular Python projects, we find 19 regression bugs and 11 PRs that, despite having another intention, coincidentally fix a bug. Out of 13 regressions reported to the developers, 10 have been confirmed and 8 have already been fixed. The costs of using Testora are acceptable for real-world deployment, with 12.3 minutes to check a PR and LLM costs of only $0.003 per PR. We envision our approach to be used before or shortly after a code change gets merged into a code base, providing a way to early on detect regressions that are not caught by traditional approaches.",,originally announced March 2025.
https://arxiv.org/abs/2501.11086,Can LLM Generate RegressionTests for Software Commits?,"Jing Liu, Seongmin Lee, Eleonora Losiouk, Marcel Böhme",https://arxiv.org/pdf/2501.11086,"Large Language Models (LLMs) have shown tremendous promise in automated software engineering. In this paper, we investigate the opportunities of LLMs for automatic regressiontest generation for programs that take highly structured, human-readable inputs, such as XML parsers or JavaScript interpreters. Concretely, we explore the following regressiontest generation scenarios for such programs that have so far been difficult to test automatically in the absence of corresponding input grammars: \bullet
 Bug finding. Given a code change (e.g., a commit or pull request), our LLM-based approach generates a test case with the objective of revealing any bugs that might be introduced if that change is applied. \bullet
 Patch testing. Given a patch, our LLM-based approach generates a test case that fails before but passes after the patch. This test can be added to the regressiontest suite to catch similar bugs in the future. We implement Cleverest, a feedback-directed, zero-shot LLM-based regressiontest generation technique, and evaluate its effectiveness on 22 commits to three subject programs: Mujs, Libxml2, and Poppler. For programs using more human-readable file formats, like XML or JavaScript, we found Cleverest performed very well. It generated easy-to-understand bug-revealing or bug-reproduction test cases for the majority of commits in just under three minutes -- even when only the code diff or commit message (unless it was too vague) was given. For programs with more compact file formats, like PDF, as expected, it struggled to generate effective test cases. However, the LLM-supplied test cases are not very far from becoming effective (e.g., when used as a seed by a greybox fuzzer or as a starting point by the developer).",,originally announced January 2025.
https://arxiv.org/abs/2407.19772,Generating Unseen Code Tests In Infinitum,"Marcel Zalmanovici, Orna Raz, Eitan Farchi, Iftach Freund",https://arxiv.org/pdf/2407.19772,"Large Language Models (LLMs) are used for many tasks, including those related to coding. An important aspect of being able to utilize LLMs is the ability to assess their fitness for specific usages. The common practice is to evaluate LLMs against a set of benchmarks. While benchmarks provide a sound foundation for evaluation and comparison of alternatives, they suffer from the well-known weakness of leaking into the training data \cite{Xu2024Benchmarking}. We present a method for creating benchmark variations that generalize across coding tasks and programming languages, and may also be applied to in-house code bases. Our approach enables ongoing generation of test-data thus mitigating the leaking into the training data issue. We implement one benchmark, called \textit{auto-regression}, for the task of text-to-code generation in Python. Auto-regression is specifically created to aid in debugging and in tracking model generation changes as part of the LLMregressiontesting process.",,originally announced July 2024.
https://arxiv.org/abs/2403.16218,CoverUp: Effective High Coverage Test Generation for Python,"Juan Altmayer Pizzorno, Emery D. Berger",https://arxiv.org/pdf/2403.16218,"Testing is an essential part of software development. Test generation tools attempt to automate the otherwise labor-intensive task of test creation, but generating high-coverage tests remains challenging. This paper proposes CoverUp, a novel approach to driving the generation of high-coverage Python regressiontests. CoverUp combines coverage analysis, code context, and feedback in prompts that iteratively guide the LLM to generate tests that improve line and branch coverage. We evaluate our prototype CoverUp implementation across a benchmark of challenging code derived from open-source Python projects and show that CoverUp substantially improves on the state of the art. Compared to CodaMosa, a hybrid search/LLM-based test generator, CoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%). Compared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves an overall line+branch coverage of 89% (vs. 77%). We also demonstrate that CoverUp's performance stems not only from the LLM used but from the combined effectiveness of its components.",,"v1 submitted 24 March, 2024"
https://arxiv.org/abs/2311.11123,(Why) Is My Prompt Getting Worse? Rethinking RegressionTesting for Evolving LLM APIs,"Wanqin Ma, Chenyang Yang, Christian Kästner",https://arxiv.org/pdf/2311.11123,"Large Language Models (LLMs) are increasingly integrated into software applications. Downstream application developers often access LLMs through APIs provided as a service. However, LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices, as evidenced by our case study on toxicity detection. Based on our case study, we emphasize the need for and re-examine the concept of regressiontesting for evolving LLM APIs. We argue that regressiontestingLLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.",,"v1 submitted 18 November, 2023"
https://arxiv.org/abs/2408.00161,Automatic Generation of Behavioral Test Cases For Natural Language Processing Using Clustering and Prompting,"Ying Li, Rahul Singh, Tarun Joshi, Agus Sudjianto",https://arxiv.org/pdf/2408.00161,"Recent work in behavioral testing for natural language processing (NLP) models, such as Checklist, is inspired by related paradigms in software engineering testing. They allow evaluation of general linguistic capabilities and domain understanding, hence can help evaluate conceptual soundness and identify model weaknesses. However, a major challenge is the creation of test cases. The current packages rely on semi-automated approach using manual development which requires domain expertise and can be time consuming. This paper introduces an automated approach to develop test cases by exploiting the power of largelanguagemodels and statistical techniques. It clusters the text representations to carefully construct meaningful groups and then apply prompting techniques to automatically generate Minimal FunctionalityTests (MFT). The well-known Amazon Reviews corpus is used to demonstrate our approach. We analyze the behavioral test profiles across four different classification algorithms and discuss the limitations and strengths of those models.",,"v1 submitted 31 July, 2024"
https://arxiv.org/abs/2407.03037,Seeing is Believing: Vision-driven Non-crash Functional Bug Detection for Mobile Apps,"Zhe Liu, Cheng Li, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Yawen Wang, Jun Hu, Qing Wang",https://arxiv.org/pdf/2407.03037,"Mobile app GUI (Graphical User Interface) pages now contain rich visual information, with the visual semantics of each page helping users understand the application logic. However, these complex visual and functional logic present new challenges to softwaretesting. Existing automated GUI testing methods, constrained by the lack of reliable testing oracles, are limited to detecting crash bugs with obvious abnormal signals. Consequently, many non-crash functional bugs, ranging from unexpected behaviors to logical errors, often evade detection by current techniques. While these non-crash functional bugs can exhibit visual cues that serve as potential testing oracles, they often entail a sequence of screenshots, and detecting them necessitates an understanding of the operational logic among GUI page transitions, which is challenging traditional techniques. Considering the remarkable performance of Multimodal LargeLanguageModels (MLLM) in visual and language understanding, this paper proposes Trident, a novel vision-driven, multi-agent collaborative automated GUI testing approach for detecting non-crash functional bugs. It comprises three agents: Explorer, Monitor, and Detector, to guide the exploration, oversee the testing progress, and spot issues. We also address several challenges, i.e., align visual and textual information for MLLM input, achieve functionality-oriented exploration, and infer test oracles for non-crash bugs, to enhance the performance of functionality bug detection. We evaluate Trident on 590 non-crash bugs and compare it with 12 baselines, it can achieve more than 14%-112% and 108%-147% boost in average recall and precision compared with the best baseline. The ablation study further proves the contribution of each module. Moreover, Trident identifies 43 new bugs on Google Play, of which 31 have been fixed.",,"v1 submitted 3 July, 2024"
https://arxiv.org/abs/2503.17998,Automatic High-Level Test Case Generation using LargeLanguageModels,"Navid Bin Hasan, Md. Ashraful Islam, Junaed Younus Khan, Sanjida Senjik, Anindya Iqbal",https://arxiv.org/pdf/2503.17998,"We explored the challenges practitioners face in softwaretesting and proposed automated solutions to address these obstacles. We began with a survey of local software companies and 26 practitioners, revealing that the primary challenge is not writing test scripts but aligning testing efforts with business requirements. Based on these insights, we constructed a use-case \rightarrow
 (high-level) test-cases dataset to train/fine-tune models for generating high-level test cases. High-level test cases specify what aspects of the software's functionality need to be tested, along with the expected outcomes. We evaluated largelanguagemodels, such as GPT-4o, Gemini, LLaMA 3.1 8B, and Mistral 7B, where fine-tuning (the latter two) yields improved performance. A final (human evaluation) survey confirmed the effectiveness of these generated test cases. Our proactive approach strengthens requirement-testing alignment and facilitates early test case generation to streamline development.",,originally announced March 2025.
https://arxiv.org/abs/2411.02328,Disrupting Test Development with AI Assistants,"Vijay Joshi, Iver Band",https://arxiv.org/pdf/2411.02328,"Recent advancements in largelanguagemodels, including GPT-4 and its variants, and Generative AI-assisted coding tools like GitHub Copilot, ChatGPT, and Tabnine, have significantly transformed software development. This paper analyzes how these innovations impact productivity and softwaretest development metrics. These tools enable developers to generate complete software programs with minimal human intervention before deployment. However, thorough review and testing by developers are still crucial. Utilizing the Test Pyramid concept, which categorizes tests into unit, integration, and end-to-end tests, we evaluate three popular AI coding assistants by generating and comparing unit tests for opensource modules. Our findings show that AI-generated tests are of equivalent quality to original tests, highlighting differences in usage and results among the tools. This research enhances the understanding and capabilities of AI-assistant tools in automated testing.",,originally announced November 2024.
https://arxiv.org/abs/2408.11710,Leveraging LargeLanguageModels for Enhancing the Understandability of Generated Unit Tests,"Amirhossein Deljouyi, Roham Koohestani, Maliheh Izadi, Andy Zaidman",https://arxiv.org/pdf/2408.11710,"Automated unit test generators, particularly search-based softwaretesting tools like EvoSuite, are capable of generating tests with high coverage. Although these generators alleviate the burden of writing unit tests, they often pose challenges for software engineers in terms of understanding the generated tests. To address this, we introduce UTGen, which combines search-based softwaretesting and largelanguagemodels to enhance the understandability of automatically generated test cases. We achieve this enhancement through contextualizing test data, improving identifier naming, and adding descriptive comments. Through a controlled experiment with 32 participants from both academia and industry, we investigate how the understandability of unit tests affects a software engineer's ability to perform bug-fixing tasks. We selected bug-fixing to simulate a real-world scenario that emphasizes the importance of understandable test cases. We observe that participants working on assignments with UTGen test cases fix up to 33% more bugs and use up to 20% less time when compared to baseline test cases. From the post-test questionnaire, we gathered that participants found that enhanced test names, test data, and variable names improved their bug-fixing process.",,originally announced August 2024.
https://arxiv.org/abs/2408.01916,MAO: A Framework for Process Model Generation with Multi-Agent Orchestration,"Leilei Lin, Yumeng Jin, Yingming Zhou, Wenlong Chen, Chen Qian",https://arxiv.org/pdf/2408.01916,"Process models are frequently used in software engineering to describe business requirements, guide softwaretesting and control system improvement. However, traditional process modeling methods often require the participation of numerous experts, which is expensive and time-consuming. Therefore, the exploration of a more efficient and cost-effective automated modeling method has emerged as a focal point in current research. This article explores a framework for automatically generating process models with multi-agent orchestration (MAO), aiming to enhance the efficiency of process modeling and offer valuable insights for domain experts. Our framework MAO leverages largelanguagemodels as the cornerstone for multi-agent, employing an innovative prompt strategy to ensure efficient collaboration among multi-agent. Specifically, 1) generation. The first phase of MAO is to generate a slightly rough process model from the text description; 2) refinement. The agents would continuously refine the initial process model through multiple rounds of dialogue; 3) reviewing. Largelanguagemodels are prone to hallucination phenomena among multi-turn dialogues, so the agents need to review and repair semantic hallucinations in process models; 4) testing. The representation of process models is diverse. Consequently, the agents utilize external tools to test whether the generated process model contains format errors, namely format hallucinations, and then adjust the process model to conform to the output paradigm. The experiments demonstrate that the process models generated by our framework outperform existing methods and surpass manual modeling by 89%, 61%, 52%, and 75% on four different datasets, respectively.",,"v1 submitted 3 August, 2024"
https://arxiv.org/abs/2302.03287,ChatGPT and SoftwareTesting Education: Promises & Perils,"Sajed Jalil, Suzzana Rafi, Thomas D. LaToza, Kevin Moran, Wing Lam",https://arxiv.org/pdf/2302.03287,"Over the past decade, predictive language modeling for code has proven to be a valuable tool for enabling new forms of automation for developers. More recently, we have seen the advent of general purpose ""largelanguagemodels"", based on neural transformer architectures, that have been trained on massive datasets of human written text spanning code and natural language. However, despite the demonstrated representational power of such models, interacting with them has historically been constrained to specific task settings, limiting their general applicability. Many of these limitations were recently overcome with the introduction of ChatGPT, a language model created by OpenAI and trained to operate as a conversational agent, enabling it to answer questions and respond to a wide variety of commands from end users. The introduction of models, such as ChatGPT, has already spurred fervent discussion from educators, ranging from fear that students could use these AI tools to circumvent learning, to excitement about the new types of learning opportunities that they might unlock. However, given the nascent nature of these tools, we currently lack fundamental knowledge related to how well they perform in different educational settings, and the potential promise (or danger) that they might pose to traditional forms of instruction. As such, in this paper, we examine how well ChatGPT performs when tasked with answering common questions in a popular softwaretesting curriculum. Our findings indicate that ChatGPT can provide correct or partially correct answers in 55.6% of cases, provide correct or partially correct explanations of answers in 53.0% of cases, and that prompting the tool in a shared question context leads to a marginally higher rate of correct responses. Based on these findings, we discuss the potential promises and perils related to the use of ChatGPT by students and instructors.",,"v1 submitted 7 February, 2023"
https://arxiv.org/abs/2402.07081,Using LargeLanguageModels for Student-Code Guided TestCaseGeneration in Computer Science Education,"Nischal Ashok Kumar, Andrew Lan",https://arxiv.org/pdf/2402.07081,"In computer science education, test cases are an integral part of programming assignments since they can be used as assessment items to test students' programming knowledge and provide personalized feedback on student-written code. The goal of our work is to propose a fully automated approach for testcasegeneration that can accurately measure student knowledge, which is important for two reasons. First, manually constructing test cases requires expert knowledge and is a labor-intensive process. Second, developing test cases for students, especially those who are novice programmers, is significantly different from those oriented toward professional-level software developers. Therefore, we need an automated process for testcasegeneration to assess student knowledge and provide feedback. In this work, we propose a largelanguagemodel-based approach to automatically generate test cases and show that they are good measures of student knowledge, using a publicly available dataset that contains student-written Java code. We also discuss future research directions centered on using test cases to help students.",,originally announced February 2024.
https://arxiv.org/abs/2406.08665,FuzzAug: Data Augmentation by Coverage-guided Fuzzing for Neural Test Generation,"Yifeng He, Jicheng Wang, Yuyang Rong, Hao Chen",https://arxiv.org/pdf/2406.08665,"Testing is essential to modern software engineering for building reliable software. Given the high costs of manually creating test cases, automated testcasegeneration, particularly methods utilizing largelanguagemodels, has become increasingly popular. These neural approaches generate semantically meaningful tests that are more maintainable compared with traditional automatic testing methods like fuzzing. However, the diversity and volume of unit tests in current datasets are limited, especially for newer but important languages. In this paper, we present a novel data augmentation technique, FuzzAug, that introduces the benefits of fuzzing to largelanguagemodels by introducing valid testing semantics and providing diverse coverage-guided inputs. Doubling the size of training datasets, FuzzAug improves the performance from the baselines significantly. This technique demonstrates the potential of introducing prior knowledge from dynamic software analysis to improve neural test generation, offering significant enhancements in neural test generation.",,"v1 submitted 12 June, 2024"
https://arxiv.org/abs/2504.20801,Unlocking User-oriented Pages: Intention-driven Black-box Scanner for Real-world Web Applications,"Weizhe Wang, Yao Zhang, Kaitai Liang, Guangquan Xu, Hongpeng Bai, Qingyang Yan, Xi Zheng, Bin Wu",https://arxiv.org/pdf/2504.20801,"Black-box scanners have played a significant role in detecting vulnerabilities for web applications. A key focus in current black-box scanning is increasing testcoverage (i.e., accessing more web pages). However, since many web applications are user-oriented, some deep pages can only be accessed through complex user interactions, which are difficult to reach by existing black-box scanners. To fill this gap, a key insight is that web pages contain a wealth of semantic information that can aid in understanding potential user intention. Based on this insight, we propose Hoyen, a black-box scanner that uses the LargeLanguageModel to predict user intention and provide guidance for expanding the scanning scope. Hoyen has been rigorously evaluated on 12 popular open-source web applications and compared with 6 representative tools. The results demonstrate that Hoyen performs a comprehensive exploration of web applications, expanding the attack surface while achieving about 2x than the coverage of other scanners on average, with high request accuracy. Furthermore, Hoyen detected over 90% of its requests towards the core functionality of the application, detecting more vulnerabilities than other scanners, including unique vulnerabilities in well-known web applications. Our data/code is available at https://hoyen.tjunsl.com/",,"v1 submitted 29 April, 2025"
https://arxiv.org/abs/2407.10227,KAT: Dependency-aware Automated API Testing with LargeLanguageModels,"Tri Le, Thien Tran, Duy Cao, Vy Le, Tien Nguyen, Vu Nguyen",https://arxiv.org/pdf/2407.10227,"API testing has increasing demands for software companies. Prior API testing tools were aware of certain types of dependencies that needed to be concise between operations and parameters. However, their approaches, which are mostly done manually or using heuristic-based algorithms, have limitations due to the complexity of these dependencies. In this paper, we present KAT (Katalon API Testing), a novel AI-driven approach that leverages the largelanguagemodel GPT in conjunction with advanced prompting techniques to autonomously generate test cases to validate RESTful APIs. Our comprehensive strategy encompasses various processes to construct an operation dependency graph from an OpenAPI specification and to generate test scripts, constraint validation scripts, test cases, and test data. Our evaluation of KAT using 12 real-world RESTful services shows that it can improve testcoverage, detect more undocumented status codes, and reduce false positives in these services in comparison with a state-of-the-art automated test generation tool. These results indicate the effectiveness of using the largelanguagemodel for generating test scripts and data for API testing.",,originally announced July 2024.
https://arxiv.org/abs/2210.02506,LargeLanguageModels are Pretty Good Zero-Shot Video Game Bug Detectors,"Mohammad Reza Taesiri, Finlay Macklon, Yihe Wang, Hengshuo Shen, Cor-Paul Bezemer",https://arxiv.org/pdf/2210.02506,"Video game testing requires game-specific knowledge as well as common sense reasoning about the events in the game. While AI-driven agents can satisfy the first requirement, it is not yet possible to meet the second requirement automatically. Therefore, video game testing often still relies on manual testing, and human testers are required to play the game thoroughly to detect bugs. As a result, it is challenging to fully automate game testing. In this study, we explore the possibility of leveraging the zero-shot capabilities of largelanguagemodels for video game bugdetection. By formulating the bugdetection problem as a question-answering task, we show that largelanguagemodels can identify which event is buggy in a sequence of textual descriptions of events from a game. To this end, we introduce the GameBugDescriptions benchmark dataset, which consists of 167 buggy gameplay videos and a total of 334 question-answer pairs across 8 games. We extensively evaluate the performance of six models across the OPT and InstructGPT largelanguagemodel families on our benchmark dataset. Our results show promising results for employing language models to detect video game bugs. With the proper prompting technique, we could achieve an accuracy of 70.66%, and on some video games, up to 78.94%. Our code, evaluation data and the benchmark can be found on https://asgaardlab.github.io/LLMxBugs",,originally announced October 2022.
https://www.sciencedirect.com/science/article/pii/S0950584923002185,Towards the definition of a research agenda on mobile application testing based on a tertiary study,,,,https://doi.org/10.1016/j.infsof.2023.107363,
https://www.sciencedirect.com/science/article/pii/S0167642324001849,Integrating behavioral semantic analysis in usage-based equivalent tests generation for mobile applications,,,,https://doi.org/10.1016/j.scico.2024.103261,
https://www.sciencedirect.com/science/article/pii/S1566253525004828,A survey for wearable sensors empowering smart healthcare in the era of large language models,,,,https://doi.org/10.1016/j.inffus.2025.103409,
https://www.sciencedirect.com/science/article/pii/S0164121225000767,"IoT systems testing: Taxonomy, empirical findings, and recommendations",,https://www.sciencedirect.com/science/article/pii/S0164121225000767/pdfft?md5=8743c4f2101dff441144300446329779&pid=1-s2.0-S0164121225000767-main.pdf,,https://doi.org/10.1016/j.jss.2025.112408,
https://www.sciencedirect.com/science/article/pii/S0950584925001387,A multi-year grey literature review on AI-assisted test automation,,,,https://doi.org/10.1016/j.infsof.2025.107799,
https://www.sciencedirect.com/science/article/pii/S1389128625003779,Driving innovation in 6G wireless technologies: The OpenAirInterface approach,,https://www.sciencedirect.com/science/article/pii/S1389128625003779/pdfft?md5=052d8a9ea10dedfbd98c10b665c67252&pid=1-s2.0-S1389128625003779-main.pdf,,https://doi.org/10.1016/j.comnet.2025.111410,
https://www.sciencedirect.com/science/article/pii/S0376042125000260,The state of hybrid artificial intelligence for interstellar missions,,https://www.sciencedirect.com/science/article/pii/S0376042125000260/pdfft?md5=eee2de8a7891b4140e0af5568c887821&pid=1-s2.0-S0376042125000260-main.pdf,,https://doi.org/10.1016/j.paerosci.2025.101100,
https://www.sciencedirect.com/science/article/pii/S1936878X24004820,Full Issue PDF,,https://www.sciencedirect.com/science/article/pii/S1936878X24004820/pdfft?md5=0e5274253658a2f180d642e07ce8980b&pid=1-s2.0-S1936878X24004820-main.pdf,,https://doi.org/10.1016/S1936-878X(24)00482-0,
https://www.sciencedirect.com/science/article/pii/B9780443328404000228,Index,,https://www.sciencedirect.com/science/article/pii/B9780443328404000228/pdfft?md5=1c463e24686d9edbc71a988d95fa25f7&pid=3-s2.0-B9780443328404000228-main.pdf,,https://doi.org/10.1016/B978-0-443-32840-4.00022-8,
https://www.sciencedirect.com/science/article/pii/S0268003323000815,Center of mass-based posturography for free living environment applications,,https://www.sciencedirect.com/science/article/pii/S0268003323000815/pdfft?md5=31838b3d28f70d792d241137d28fa219&pid=1-s2.0-S0268003323000815-main.pdf,,https://doi.org/10.1016/j.clinbiomech.2023.105950,
https://www.sciencedirect.com/science/article/pii/S0020025514011906,A lightweight framework for transparent cross platform communication of controller data in ambient assisted living environments,,,,https://doi.org/10.1016/j.ins.2014.10.070,
https://www.sciencedirect.com/science/article/pii/S0164121223003011,Monitoring tools for DevOps and microservices: A systematic grey literature review,,https://www.sciencedirect.com/science/article/pii/S0164121223003011/pdfft?md5=df31d076fa3090a3260d28b019273b0f&pid=1-s2.0-S0164121223003011-main.pdf,,https://doi.org/10.1016/j.jss.2023.111906,
https://www.sciencedirect.com/science/article/pii/B9781569904466500054,Manufacturing and Machining Methods,,,,https://doi.org/10.3139/9781569905500.004,
https://www.sciencedirect.com/science/article/pii/S0003999399900734,1999 academy annual assembly abstracts,,,,https://doi.org/10.1016/S0003-9993(99)90073-4,
https://www.sciencedirect.com/science/article/pii/S0091674906810122,Abstracts 1-300,,,,https://doi.org/10.1016/S0091-6749(06)81012-2,
https://www.sciencedirect.com/science/article/pii/S1474667017423184,Solving a Hoist Scheduling Problem as a Sequencing Problem,,https://www.sciencedirect.com/science/article/pii/S1474667017423184/pdfft?md5=c6fb575feccdd9edc212deec22d831da&pid=1-s2.0-S1474667017423184-main.pdf,,https://doi.org/10.1016/S1474-6670(17)42318-4,
https://www.sciencedirect.com/science/article/pii/S0735109701800048,"Hypertension, vascular disease, and prevention",,https://www.sciencedirect.com/science/article/pii/S0735109701800048/pdfft?md5=7454ae15a62d91ceedae38dcd0e72542&pid=1-s2.0-S0735109701800048-main.pdf,,https://doi.org/10.1016/S0735-1097(01)80004-8,
https://www.sciencedirect.com/science/article/pii/S2542660525001441,SOLAR: Illuminating LLM performance in API discovery and service ranking for edge AI and IoT,,,,https://doi.org/10.1016/j.iot.2025.101630,
https://www.sciencedirect.com/science/article/pii/S1551741125003730,Harnessing ChatGPT for digital tools in pharmacy practice,,,,https://doi.org/10.1016/j.sapharm.2025.06.106,
https://www.sciencedirect.com/science/article/pii/S0167739X24002449,LLM4VV: Developing LLM-driven testsuite for compiler validation,,,,https://doi.org/10.1016/j.future.2024.05.034,
https://www.sciencedirect.com/science/article/pii/S1438887120002691,Web-Based Intervention Effects on Mild Cognitive Impairment Based on Apolipoprotein E Genotype: Quasi-Experimental Study,,,,https://doi.org/10.2196/14617,
https://www.sciencedirect.com/science/article/pii/S0306457325001803,Red teaming large language models: A comprehensive review and critical analysis,,,,https://doi.org/10.1016/j.ipm.2025.104239,
https://www.sciencedirect.com/science/article/pii/S0045782525000143,A large language model and denoising diffusion framework for targeted design of microstructures with commands in natural language,,https://www.sciencedirect.com/science/article/pii/S0045782525000143/pdfft?md5=9df874c3213b0686d9124aea3fd91d75&pid=1-s2.0-S0045782525000143-main.pdf,,https://doi.org/10.1016/j.cma.2025.117742,
https://www.sciencedirect.com/science/article/pii/S1110016824016776,"Generative artificial intelligence in construction: A Delphi approach, framework, and case study",,https://www.sciencedirect.com/science/article/pii/S1110016824016776/pdfft?md5=12f0ed858d4371186c3b87f5bf04e237&pid=1-s2.0-S1110016824016776-main.pdf,,https://doi.org/10.1016/j.aej.2024.12.079,
https://www.sciencedirect.com/science/article/pii/S0164121224002486,"Exploring the problems, their causes and solutions of AI pair programming: A study on GitHub and Stack Overflow",,,,https://doi.org/10.1016/j.jss.2024.112204,
https://www.sciencedirect.com/science/article/pii/S0167494323003813,"Different resistance training volumes on strength, functional fitness, and body composition of older people: A systematic review with meta-analysis",,,,https://doi.org/10.1016/j.archger.2023.105303,
https://www.sciencedirect.com/science/article/pii/S2255502116300694,Effects of aquatic exercise on muscle strength and functional performance of individuals with osteoarthritis: a systematic review,,https://www.sciencedirect.com/science/article/pii/S2255502116300694/pdfft?md5=0269cfc750f4dda2401f9739e3475abc&pid=1-s2.0-S2255502116300694-main.pdf,,https://doi.org/10.1016/j.rbre.2016.09.003,
https://www.sciencedirect.com/science/article/pii/S1570826824000350,Leveraging Knowledge Graphs for AI System Auditing and Transparency,,https://www.sciencedirect.com/science/article/pii/S1570826824000350/pdfft?md5=bf0b7e38f0df1ccde18c4ac099cd0fa1&pid=1-s2.0-S1570826824000350-main.pdf,,https://doi.org/10.1016/j.websem.2024.100849,
https://www.sciencedirect.com/science/article/pii/S0167642324001345,A systematic literature review on dynamic testing of blockchain oriented software,,,,https://doi.org/10.1016/j.scico.2024.103211,
https://www.sciencedirect.com/science/article/pii/S1386505624003034,A conversational agent for enhanced Self-Management after cardiothoracic surgery,,https://www.sciencedirect.com/science/article/pii/S1386505624003034/pdfft?md5=3f8741712ca7df049bcf7ce1e5d11a08&pid=1-s2.0-S1386505624003034-main.pdf,,https://doi.org/10.1016/j.ijmedinf.2024.105640,
https://www.sciencedirect.com/science/article/pii/S0950584925000928,A systematic literature review on task recommendation systems for crowdsourced software engineering,,https://www.sciencedirect.com/science/article/pii/S0950584925000928/pdfft?md5=aa27fac8edcbc0acec1c80edfc6b8eda&pid=1-s2.0-S0950584925000928-main.pdf,,https://doi.org/10.1016/j.infsof.2025.107753,
https://www.sciencedirect.com/science/article/pii/S0001457525000806,A review of scenario cases for autonomous transportation system: Insights from CAV safety testing and scenario generation,,,,https://doi.org/10.1016/j.aap.2025.107994,
https://www.sciencedirect.com/science/article/pii/S259012302502002X,Bioengineering an AI-Augmented Platform for Remote Mental Health Interventions,,https://www.sciencedirect.com/science/article/pii/S259012302502002X/pdfft?md5=eff717a23f787395f4b073a811394611&pid=1-s2.0-S259012302502002X-main.pdf,,https://doi.org/10.1016/j.rineng.2025.105931,
https://www.sciencedirect.com/science/article/pii/S2772485925000250,AICB: A benchmark for evaluating the communication subsystem of LLM training clusters,,https://www.sciencedirect.com/science/article/pii/S2772485925000250/pdfft?md5=2223db063017de1b7baa41071fa0a435&pid=1-s2.0-S2772485925000250-main.pdf,,https://doi.org/10.1016/j.tbench.2025.100212,
https://www.sciencedirect.com/science/article/pii/S095741742403224X,Collaboration between intelligent agents and large language models: A novel approach for enhancing code generation capability,,,,https://doi.org/10.1016/j.eswa.2024.126357,
https://www.sciencedirect.com/science/article/pii/S0734975024000934,Advancing microbial production through artificial intelligence-aided biology,,,,https://doi.org/10.1016/j.biotechadv.2024.108399,
https://www.sciencedirect.com/science/article/pii/S1467089524000319,Artificial intelligence co-piloted auditing,,,,https://doi.org/10.1016/j.accinf.2024.100698,
https://www.sciencedirect.com/science/article/pii/S136085922030190X,Accuracy of the step test to evaluate lower limb muscle strength in community-dwelling older women,,,,https://doi.org/10.1016/j.jbmt.2020.10.009,
https://www.sciencedirect.com/science/article/pii/S0164121225001475,So much more than test cases – An industrial study on testing of software units and components,,https://www.sciencedirect.com/science/article/pii/S0164121225001475/pdfft?md5=f5616c7c0b71bf7e1f30a679ad21b804&pid=1-s2.0-S0164121225001475-main.pdf,,https://doi.org/10.1016/j.jss.2025.112479,
https://www.sciencedirect.com/science/article/pii/S2635098X2400024X,Machine learning for hypothesis generation in biology and medicine: exploring the latent space of neuroscience and developmental bioelectricity,,,,https://doi.org/10.1039/d3dd00185g,
https://www.sciencedirect.com/science/article/pii/S0951832025002145,Double layer blockchain-assisted trusted data flow model for industrial control systems,,,,https://doi.org/10.1016/j.ress.2025.111013,
https://www.sciencedirect.com/science/article/pii/S0378778825003512,Active multi-mode data analysis to improve fault diagnosis in AHUs,,https://www.sciencedirect.com/science/article/pii/S0378778825003512/pdfft?md5=a942161f4cbdc212098c33295c2f3cfd&pid=1-s2.0-S0378778825003512-main.pdf,,https://doi.org/10.1016/j.enbuild.2025.115621,
https://www.sciencedirect.com/science/article/pii/S1579212917301751,Relationship of Muscle Mass Determined by DEXA with Spirometric Results in Healthy Individuals,,,,https://doi.org/10.1016/j.arbr.2017.05.006,
https://www.sciencedirect.com/science/article/pii/S0531556519301007,The impact of age and frailty on skeletal muscle autophagy markers and specific strength: A cross-sectional comparison,,,,https://doi.org/10.1016/j.exger.2019.110687,
https://www.sciencedirect.com/science/article/pii/S1476558621000075,High dose acetaminophen inhibits STAT3 and has free radical independent anti-cancer stem cell activity,,https://www.sciencedirect.com/science/article/pii/S1476558621000075/pdfft?md5=a1c58b19de22ab4358561c31d7124329&pid=1-s2.0-S1476558621000075-main.pdf,,https://doi.org/10.1016/j.neo.2021.02.001,
https://www.sciencedirect.com/science/article/pii/S0167404825001993,"Network intrusion datasets: A survey, limitations, and recommendations",,,,https://doi.org/10.1016/j.cose.2025.104510,
https://www.sciencedirect.com/science/article/pii/073510979190804I,Time-dependent variation in the cardiac conduction system assessed in young healthy individuals at weeks' interval: Implications for clinical trials,,https://www.sciencedirect.com/science/article/pii/073510979190804I/pdfft?md5=3a4f7ea58aa47d5ce03749515782ec47&pid=1-s2.0-073510979190804I-main.pdf,,https://doi.org/10.1016/0735-1097(91)90804-I,
https://www.sciencedirect.com/science/article/pii/S0043164897001294,Quantitative estimation and prediction of tribological performance of pure additive compounds through computer modelling,,,,https://doi.org/10.1016/S0043-1648(97)00129-4,
https://www.sciencedirect.com/science/article/pii/0168900291910506,Investigations of the small-scale flat field response of microchannel plate detectors in the far and extreme ultraviolet,,,,https://doi.org/10.1016/0168-9002(91)91050-6,
https://www.sciencedirect.com/science/article/pii/S247503792500247X,2025 Poster Abstracts Part 1,,https://www.sciencedirect.com/science/article/pii/S247503792500247X/pdfft?md5=2909ceabb93aee1c38a03ed00e6c1314&pid=1-s2.0-S247503792500247X-main.pdf,,https://doi.org/10.1016/j.rpth.2025.102923,
https://www.sciencedirect.com/science/article/pii/S2090447925002138,Developing SPIM-TA: a maturity-level framework for systematic process improvement in software testing automation,,https://www.sciencedirect.com/science/article/pii/S2090447925002138/pdfft?md5=47749c6cb3e59481df7a8a59ae12bc70&pid=1-s2.0-S2090447925002138-main.pdf,,https://doi.org/10.1016/j.asej.2025.103472,
https://www.sciencedirect.com/science/article/pii/S2667345225000082,Generative AI in cybersecurity: A comprehensive review of LLM applications and vulnerabilities,,https://www.sciencedirect.com/science/article/pii/S2667345225000082/pdfft?md5=da8ff6376b63e59da75af5d1ff5d2818&pid=1-s2.0-S2667345225000082-main.pdf,,https://doi.org/10.1016/j.iotcps.2025.01.001,
https://www.sciencedirect.com/science/article/pii/S0164121224000748,GRACE: Empowering LLM-based software vulnerability detection with graph structure and in-context learning,,,,https://doi.org/10.1016/j.jss.2024.112031,
https://www.sciencedirect.com/science/article/pii/S2666764925000323,"Integrative innovation of large language models in industries: technologies, applications, and challenges",,https://www.sciencedirect.com/science/article/pii/S2666764925000323/pdfft?md5=2fc6fd840b403d673ba684ccce5018de&pid=1-s2.0-S2666764925000323-main.pdf,,https://doi.org/10.1016/j.dsm.2025.06.005,
https://www.sciencedirect.com/science/article/pii/S092054892400120X,The use of large language models for program repair,,https://www.sciencedirect.com/science/article/pii/S092054892400120X/pdfft?md5=1aeebe19c4ea95d9850576ef0c78add4&pid=1-s2.0-S092054892400120X-main.pdf,,https://doi.org/10.1016/j.csi.2024.103951,
https://www.sciencedirect.com/science/article/pii/S0304397524004961,Generating Java code pairing with ChatGPT,,,,https://doi.org/10.1016/j.tcs.2024.114879,
https://www.sciencedirect.com/science/article/pii/S2949719125000044,A survey on chatbots and large language models: Testing and evaluation techniques,,https://www.sciencedirect.com/science/article/pii/S2949719125000044/pdfft?md5=44b97cc8b5f13dba55b52b9cd42a440d&pid=1-s2.0-S2949719125000044-main.pdf,,https://doi.org/10.1016/j.nlp.2025.100128,
https://www.sciencedirect.com/science/article/pii/S0950584925000977,Benchmarking large language models for automated labeling: The case of issue report classification,,,,https://doi.org/10.1016/j.infsof.2025.107758,
https://www.sciencedirect.com/science/article/pii/S0164121225000214,Automatic instantiation of assurance cases from patterns using large language models,,https://www.sciencedirect.com/science/article/pii/S0164121225000214/pdfft?md5=a7639488f3125335311492263682ce74&pid=1-s2.0-S0164121225000214-main.pdf,,https://doi.org/10.1016/j.jss.2025.112353,
https://www.sciencedirect.com/science/article/pii/S0045790625002095,TraceAwareness and dual-strategy fuzz testing: Enhancing path coverage and crash localization with stochastic science and large language models,,,,https://doi.org/10.1016/j.compeleceng.2025.110266,
https://www.sciencedirect.com/science/article/pii/S0950584925001363,A survey of coverage-guided greybox fuzzing with deep neural models,,https://www.sciencedirect.com/science/article/pii/S0950584925001363/pdfft?md5=5b23191de6a5a380167a7214a521afce&pid=1-s2.0-S0950584925001363-main.pdf,,https://doi.org/10.1016/j.infsof.2025.107797,
https://www.sciencedirect.com/science/article/pii/S2210650224002013,When large language model meets optimization,,,,https://doi.org/10.1016/j.swevo.2024.101663,
https://www.sciencedirect.com/science/article/pii/S0950584925000734,Extensive mutation for testing of word sense disambiguation models,,,,https://doi.org/10.1016/j.infsof.2025.107734,
https://www.sciencedirect.com/science/article/pii/S1574013725000437,Insight into code clone management through refactoring: a systematic literature review,,,,https://doi.org/10.1016/j.cosrev.2025.100767,
https://www.sciencedirect.com/science/article/pii/S0164121225001049,RAGVA: Engineering retrieval augmented generation-based virtual assistants in practice,,https://www.sciencedirect.com/science/article/pii/S0164121225001049/pdfft?md5=6dc7746c9b18c6c03e623f48047d3797&pid=1-s2.0-S0164121225001049-main.pdf,,https://doi.org/10.1016/j.jss.2025.112436,
https://www.sciencedirect.com/science/article/pii/S1877050924014297,NLP in SMEs for industry 4.0: opportunities and challenges,,https://www.sciencedirect.com/science/article/pii/S1877050924014297/pdfft?md5=5a57c79a94932518fea6b195df721b38&pid=1-s2.0-S1877050924014297-main.pdf,,https://doi.org/10.1016/j.procs.2024.06.186,
https://www.sciencedirect.com/science/article/pii/S0164121224000141,Promoting open science in test-driven software experiments,,https://www.sciencedirect.com/science/article/pii/S0164121224000141/pdfft?md5=99fa3f092f530c3340d01251ced7586a&pid=1-s2.0-S0164121224000141-main.pdf,,https://doi.org/10.1016/j.jss.2024.111971,
https://www.sciencedirect.com/science/article/pii/S0164121224002760,"Unveiling the microservices testing methods, challenges, solutions, and solutions gaps: A systematic mapping study",,,,https://doi.org/10.1016/j.jss.2024.112232,
https://www.sciencedirect.com/science/article/pii/S1877050925019647,Research on Preprocessing Techniques for Software Defect Prediction Dataset Based on Hybrid Category Balance and Synthetic Sampling Algorithm,,https://www.sciencedirect.com/science/article/pii/S1877050925019647/pdfft?md5=e53ba70233e74fa77007905dca8b1f73&pid=1-s2.0-S1877050925019647-main.pdf,,https://doi.org/10.1016/j.procs.2025.05.117,
https://www.sciencedirect.com/science/article/pii/S0950584924001939,DeepMig: A transformer-based approach to support coupled library and code migrations,,https://www.sciencedirect.com/science/article/pii/S0950584924001939/pdfft?md5=27c9d63fc2e399aaf9bca863e534bdf1&pid=1-s2.0-S0950584924001939-main.pdf,,https://doi.org/10.1016/j.infsof.2024.107588,
https://www.sciencedirect.com/science/article/pii/S0164121225000561,Improving the performance of software fault localization with effective coverage data reduction techniques,,,,https://doi.org/10.1016/j.jss.2025.112388,
https://www.sciencedirect.com/science/article/pii/B9780443405532000071,1: Introduction to bidirectionality in human–AI collaborative systems,,,,https://doi.org/10.1016/B978-0-44-340553-2.00007-1,
https://www.sciencedirect.com/science/article/pii/S1526149223001649,Exploring the Latest Applications of OpenAI and ChatGPT: An In-Depth Survey,,,,https://doi.org/10.32604/cmes.2023.030649,
https://www.sciencedirect.com/science/article/pii/S095058492400082X,Machine learning for requirements engineering (ML4RE): A systematic literature review complemented by practitioners’ voices from Stack Overflow,,https://www.sciencedirect.com/science/article/pii/S095058492400082X/pdfft?md5=63ea9a0df5bff96f324d63b42a81b4cb&pid=1-s2.0-S095058492400082X-main.pdf,,https://doi.org/10.1016/j.infsof.2024.107477,
https://www.sciencedirect.com/science/article/pii/S095058492500117X,GPTs are not the silver bullet: Performance and challenges of using GPTs for security bug report identification,,https://www.sciencedirect.com/science/article/pii/S095058492500117X/pdfft?md5=8c743603a72c6c5540112021f2ed94f8&pid=1-s2.0-S095058492500117X-main.pdf,,https://doi.org/10.1016/j.infsof.2025.107778,
https://www.sciencedirect.com/science/article/pii/S1472811724001757,Teaching mathematical concepts in management with generative artificial intelligence: The power of human oversight in AI-driven learning,,,,https://doi.org/10.1016/j.ijme.2024.101104,
https://www.sciencedirect.com/science/article/pii/S0164121224001043,GPTSniffer: A CodeBERT-based classifier to detect source code written by ChatGPT,,https://www.sciencedirect.com/science/article/pii/S0164121224001043/pdfft?md5=19607f6fe310ee1fc9eb83fec563c886&pid=1-s2.0-S0164121224001043-main.pdf,,https://doi.org/10.1016/j.jss.2024.112059,
https://www.sciencedirect.com/science/article/pii/S2590118425000267,Python’s evolution on Stack Overflow: An empirical analysis of topic trends,,,,https://doi.org/10.1016/j.cola.2025.101340,
https://www.sciencedirect.com/science/article/pii/S0957417424023911,Leveraging Large Language Model ChatGPT for enhanced understanding of end-user emotions in social media feedbacks,,,,https://doi.org/10.1016/j.eswa.2024.125524,
https://www.sciencedirect.com/science/article/pii/S0950584924000612,The need for more informative defect prediction: A systematic literature review,,https://www.sciencedirect.com/science/article/pii/S0950584924000612/pdfft?md5=2caccecba96596f84ab96dbd018feef8&pid=1-s2.0-S0950584924000612-main.pdf,,https://doi.org/10.1016/j.infsof.2024.107456,
https://www.sciencedirect.com/science/article/pii/S2666920X24001462,Assisting quality assurance of examination tasks: Using a GPT model and Bayesian testing for formative assessment,,https://www.sciencedirect.com/science/article/pii/S2666920X24001462/pdfft?md5=592a13411164ef720f3d1f3cc007b9fc&pid=1-s2.0-S2666920X24001462-main.pdf,,https://doi.org/10.1016/j.caeai.2024.100343,
https://www.sciencedirect.com/science/article/pii/S0950584925000242,Production and test bug report classification based on transfer learning,,,,https://doi.org/10.1016/j.infsof.2025.107685,
https://www.sciencedirect.com/science/article/pii/S0950584924001915,Constructing the graphical structure of expert-based Bayesian networks in the context of software engineering: A systematic mapping study,,,,https://doi.org/10.1016/j.infsof.2024.107586,
https://www.sciencedirect.com/science/article/pii/S0167404824004620,Fuzzing drones for anomaly detection: A systematic literature review,,,,https://doi.org/10.1016/j.cose.2024.104157,
https://www.sciencedirect.com/science/article/pii/S0164121223003291,A survey on machine learning techniques applied to source code,,https://www.sciencedirect.com/science/article/pii/S0164121223003291/pdfft?md5=2c05a47d77dd4cdbebf30efb6c41cd02&pid=1-s2.0-S0164121223003291-main.pdf,,https://doi.org/10.1016/j.jss.2023.111934,
https://www.sciencedirect.com/science/article/pii/S0007850625001477,Developing and leveraging digital twins in engineering design,,https://www.sciencedirect.com/science/article/pii/S0007850625001477/pdfft?md5=6fac4a2905d8d6b658d164569bbecb0f&pid=1-s2.0-S0007850625001477-main.pdf,,https://doi.org/10.1016/j.cirp.2025.05.002,
https://www.sciencedirect.com/science/article/pii/S016412122400075X,Research artifacts in software engineering publications: Status and trends,,,,https://doi.org/10.1016/j.jss.2024.112032,
https://www.sciencedirect.com/science/article/pii/S0164121224000542,A/B testing: A systematic literature review,,https://www.sciencedirect.com/science/article/pii/S0164121224000542/pdfft?md5=1b4768b9e92dc40fd047c37d6f3db0be&pid=1-s2.0-S0164121224000542-main.pdf,,https://doi.org/10.1016/j.jss.2024.112011,
https://www.sciencedirect.com/science/article/pii/S0164121225000226,Hierarchical tree-based algorithms for efficient expression parsing and test sequence generation in software models,,,,https://doi.org/10.1016/j.jss.2025.112354,
https://www.sciencedirect.com/science/article/pii/S0167404825001439,Automated penetration testing: Formalization and realization,,https://www.sciencedirect.com/science/article/pii/S0167404825001439/pdfft?md5=a98d72ba8d06e2596a04d9cde87ac5b7&pid=1-s2.0-S0167404825001439-main.pdf,,https://doi.org/10.1016/j.cose.2025.104454,
https://www.sciencedirect.com/science/article/pii/S0164121224002929,An empirical study of developers’ challenges in implementing Workflows as Code: A case study on Apache Airflow,,,,https://doi.org/10.1016/j.jss.2024.112248,
https://www.sciencedirect.com/science/article/pii/S0164121224002243,An empirical study on bug severity estimation using source code metrics and static analysis,,https://www.sciencedirect.com/science/article/pii/S0164121224002243/pdfft?md5=a39163bbaede6f3d8ae30da169d41fa3&pid=1-s2.0-S0164121224002243-main.pdf,,https://doi.org/10.1016/j.jss.2024.112179,
https://www.sciencedirect.com/science/article/pii/S0164121225001463,Text–image fusion template for large language model assisted crowdsourcing test aggregation,,,,https://doi.org/10.1016/j.jss.2025.112478,
https://www.sciencedirect.com/science/article/pii/S2666827023000750,ChatReview: A ChatGPT-enabled natural language processing framework to study domain-specific user reviews,,https://www.sciencedirect.com/science/article/pii/S2666827023000750/pdfft?md5=82dd36b16ed5d43b7a9134111f9ce072&pid=1-s2.0-S2666827023000750-main.pdf,,https://doi.org/10.1016/j.mlwa.2023.100522,
https://www.sciencedirect.com/science/article/pii/S0950584925000904,Copiloting the future: How generative AI transforms Software Engineering,,https://www.sciencedirect.com/science/article/pii/S0950584925000904/pdfft?md5=df9407b7696979dfc8da6c285d21c130&pid=1-s2.0-S0950584925000904-main.pdf,,https://doi.org/10.1016/j.infsof.2025.107751,
https://www.sciencedirect.com/science/article/pii/S2211285525001387,Fountain-inspired triboelectric nanogenerator as rotary energy harvester and self-powered intelligent sensor,,,,https://doi.org/10.1016/j.nanoen.2025.110779,
https://www.sciencedirect.com/science/article/pii/S0164121223003394,A survey of energy concerns for software engineering,,,,https://doi.org/10.1016/j.jss.2023.111944,
https://www.sciencedirect.com/science/article/pii/S266729522400014X,"A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly",,https://www.sciencedirect.com/science/article/pii/S266729522400014X/pdfft?md5=1984f6886539e5ada13eeb8c49a9ef8b&pid=1-s2.0-S266729522400014X-main.pdf,,https://doi.org/10.1016/j.hcc.2024.100211,
https://www.sciencedirect.com/science/article/pii/S2949719123000456,A survey of GPT-3 family large language models including ChatGPT and GPT-4,,https://www.sciencedirect.com/science/article/pii/S2949719123000456/pdfft?md5=72753bb0aac6b7c01d0dc8bddfb62121&pid=1-s2.0-S2949719123000456-main.pdf,,https://doi.org/10.1016/j.nlp.2023.100048,
https://www.sciencedirect.com/science/article/pii/S095058492500076X,Don’t settle for the first! How many GitHub Copilot solutions should you check?,,https://www.sciencedirect.com/science/article/pii/S095058492500076X/pdfft?md5=d7d455e57963c884a5e2b38d22de9994&pid=1-s2.0-S095058492500076X-main.pdf,,https://doi.org/10.1016/j.infsof.2025.107737,
https://www.sciencedirect.com/science/article/pii/S0920548924001119,Evaluating large language models for software testing,,,,https://doi.org/10.1016/j.csi.2024.103942,
https://www.sciencedirect.com/science/article/pii/S0950584925001429,SVA-ICL: Improving LLM-based software vulnerability assessment via in-context learning and information fusion,,,,https://doi.org/10.1016/j.infsof.2025.107803,
https://www.sciencedirect.com/science/article/pii/S2666827025000222,Software engineering meets legal texts: LLMs for auto detection of contract smells,,https://www.sciencedirect.com/science/article/pii/S2666827025000222/pdfft?md5=ce9032622b247413c6e7d2bfd5d88ae9&pid=1-s2.0-S2666827025000222-main.pdf,,https://doi.org/10.1016/j.mlwa.2025.100639,
https://www.sciencedirect.com/science/article/pii/S2212827125004639,Advancing Automotive Production: An LLM-based Impact Analysis for Software Updates,,https://www.sciencedirect.com/science/article/pii/S2212827125004639/pdfft?md5=863af773bbf346ad4c60385e9c09d509&pid=1-s2.0-S2212827125004639-main.pdf,,https://doi.org/10.1016/j.procir.2025.02.134,
https://www.sciencedirect.com/science/article/pii/S0950584924002167,On the road to interactive LLM-based systematic mapping studies,,https://www.sciencedirect.com/science/article/pii/S0950584924002167/pdfft?md5=16e16c87e88e88f65131ad6a4e75484b&pid=1-s2.0-S0950584924002167-main.pdf,,https://doi.org/10.1016/j.infsof.2024.107611,
https://www.sciencedirect.com/science/article/pii/S1877050924025158,LLM-based methods for the creation of unit tests in game development,,https://www.sciencedirect.com/science/article/pii/S1877050924025158/pdfft?md5=ea4f74f08f21a6aa71c00d6c794b61b3&pid=1-s2.0-S1877050924025158-main.pdf,,https://doi.org/10.1016/j.procs.2024.09.473,
https://www.sciencedirect.com/science/article/pii/S266682702400046X,Managing Linux servers with LLM-based AI agents: An empirical evaluation with GPT4,,https://www.sciencedirect.com/science/article/pii/S266682702400046X/pdfft?md5=c84038ecf9feef782cbf788c56d506da&pid=1-s2.0-S266682702400046X-main.pdf,,https://doi.org/10.1016/j.mlwa.2024.100570,
https://www.sciencedirect.com/science/article/pii/S0950584924002507,Robustness evaluation of code generation systems via concretizing instructions,,,,https://doi.org/10.1016/j.infsof.2024.107645,
https://www.sciencedirect.com/science/article/pii/S0164121224003108,Refining software defect prediction through attentive neural models for code understanding,,,,https://doi.org/10.1016/j.jss.2024.112266,
https://www.sciencedirect.com/science/article/pii/S2665963824000757,BHRAMARI: Bug driven highly reusable automated model for automated test bed generation and integration,,https://www.sciencedirect.com/science/article/pii/S2665963824000757/pdfft?md5=3c9a3983d98fc59aa765e0c83b280c82&pid=1-s2.0-S2665963824000757-main.pdf,,https://doi.org/10.1016/j.simpa.2024.100687,
https://www.sciencedirect.com/science/article/pii/S0164121224000451,Few-shot code translation via task-adapted prompt learning,,,,https://doi.org/10.1016/j.jss.2024.112002,
https://www.sciencedirect.com/science/article/pii/S0950584925000382,Assessing and improving syntactic adversarial robustness of pre-trained models for code translation,,,,https://doi.org/10.1016/j.infsof.2025.107699,
https://www.sciencedirect.com/science/article/pii/S0950584924000107,Automatic smart contract comment generation via large language models and in-context learning,,,,https://doi.org/10.1016/j.infsof.2024.107405,
https://www.sciencedirect.com/science/article/pii/S0167404824002992,SCL-CVD: Supervised contrastive learning for code vulnerability detection via GraphCodeBERT,,,,https://doi.org/10.1016/j.cose.2024.103994,
https://www.sciencedirect.com/science/article/pii/S016412122400325X,Integrating neural mutation into mutation-based fault localization: A hybrid approach,,,,https://doi.org/10.1016/j.jss.2024.112281,
https://www.sciencedirect.com/science/article/pii/S092054892500042X,Collaboration with Generative AI to improve Requirements Change,,,,https://doi.org/10.1016/j.csi.2025.104013,
https://www.sciencedirect.com/science/article/pii/S0950584924002490,Locating requirements in backlog items: Content analysis and experiments with large language models,,https://www.sciencedirect.com/science/article/pii/S0950584924002490/pdfft?md5=00a832bca7111fa0fd6b338b8c263c12&pid=1-s2.0-S0950584924002490-main.pdf,,https://doi.org/10.1016/j.infsof.2024.107644,
https://www.sciencedirect.com/science/article/pii/S0164121225001104,Improving distributed learning-based vulnerability detection via multi-modal prompt tuning,,,,https://doi.org/10.1016/j.jss.2025.112442,
https://www.sciencedirect.com/science/article/pii/S0950584925001442,Still just personal assistants? – A multiple case study of generative AI adoption in software organizations,,https://www.sciencedirect.com/science/article/pii/S0950584925001442/pdfft?md5=d0615f49fbfb02b687683baf9fbb0281&pid=1-s2.0-S0950584925001442-main.pdf,,https://doi.org/10.1016/j.infsof.2025.107805,
https://www.sciencedirect.com/science/article/pii/S0164121225000494,Pytester: Deep reinforcement learning for text-to-testcase generation,,https://www.sciencedirect.com/science/article/pii/S0164121225000494/pdfft?md5=624044fdd331b44d8ac6126fe0b5188d&pid=1-s2.0-S0164121225000494-main.pdf,,https://doi.org/10.1016/j.jss.2025.112381,
https://www.sciencedirect.com/science/article/pii/S2215016125001864,A method for IoT devices test case generation using language models,,https://www.sciencedirect.com/science/article/pii/S2215016125001864/pdfft?md5=bc0884b834c9ba1346acc52916999b9c&pid=1-s2.0-S2215016125001864-main.pdf,,https://doi.org/10.1016/j.mex.2025.103340,
https://www.sciencedirect.com/science/article/pii/S0167739X2500024X,Leveraging local and global relationships for corrupted label detection,,,,https://doi.org/10.1016/j.future.2025.107729,
https://www.sciencedirect.com/science/article/pii/S0164121225002225,Operational Profile-based Test Case Generation for Normative MAS,,,,https://doi.org/10.1016/j.jss.2025.112553,
https://www.sciencedirect.com/science/article/pii/S0167642324000649,TerGEC: A graph enhanced contrastive approach for program termination analysis,,,,https://doi.org/10.1016/j.scico.2024.103141,
https://www.sciencedirect.com/science/article/pii/S0164121224001936,BIT: A template-based approach to incremental and bidirectional model-to-text transformation,,,,https://doi.org/10.1016/j.jss.2024.112148,
https://www.sciencedirect.com/science/article/pii/S016764232500098X,Random Test Generators Demystified: Differences and Potential for Compiler Reliability,,,,https://doi.org/10.1016/j.scico.2025.103359,
https://www.sciencedirect.com/science/article/pii/0098300488900027,Procedures for creating a benchmarking data set,,,,https://doi.org/10.1016/0098-3004(88)90002-7,
https://www.sciencedirect.com/science/article/pii/S2666920X24000201,Role of activity-based learning and ChatGPT on students' performance in education,,https://www.sciencedirect.com/science/article/pii/S2666920X24000201/pdfft?md5=4616bc803fdb32f0529e8345c9659d43&pid=1-s2.0-S2666920X24000201-main.pdf,,https://doi.org/10.1016/j.caeai.2024.100219,
https://www.sciencedirect.com/science/article/pii/S0950705125004435,Harnessing code domain insights: Enhancing programming Knowledge Tracing with Large Language Models,,,,https://doi.org/10.1016/j.knosys.2025.113396,
https://www.sciencedirect.com/science/article/pii/S2665963824000071,LangTest: A comprehensive evaluation library for custom LLM and NLP models,,https://www.sciencedirect.com/science/article/pii/S2665963824000071/pdfft?md5=08c3b88d18208044478d2ee4f4d9432b&pid=1-s2.0-S2665963824000071-main.pdf,,https://doi.org/10.1016/j.simpa.2024.100619,
https://www.sciencedirect.com/science/article/pii/S2214635025000486,Reasoning with financial regulatory texts via Large Language Models,,https://www.sciencedirect.com/science/article/pii/S2214635025000486/pdfft?md5=67eb1d4cdaa27eabb830cf3c19f05e26&pid=1-s2.0-S2214635025000486-main.pdf,,https://doi.org/10.1016/j.jbef.2025.101067,
https://www.sciencedirect.com/science/article/pii/S016764232500005X,Filling query-type text inputs for Android applications via inner-app mining and GPT recommendation,,,,https://doi.org/10.1016/j.scico.2025.103266,
https://www.sciencedirect.com/science/article/pii/S2212473X25000148,Generative AI and the future of marketing: A consumer protection perspective,,https://www.sciencedirect.com/science/article/pii/S2212473X25000148/pdfft?md5=b1f802dd733f0c52b63da853ddeaf1f1&pid=1-s2.0-S2212473X25000148-main.pdf,,https://doi.org/10.1016/j.clsr.2025.106141,
https://www.sciencedirect.com/science/article/pii/S235271102400400X,LLM based QA chatbot builder: A generative AI-based chatbot builder for question answering,,https://www.sciencedirect.com/science/article/pii/S235271102400400X/pdfft?md5=d9388817f46d56c1b3784312eab9d244&pid=1-s2.0-S235271102400400X-main.pdf,,https://doi.org/10.1016/j.softx.2024.102029,
https://www.sciencedirect.com/science/article/pii/S1546221825004783,Sensitive Target-Guided Directed Fuzzing for IoT Web Services,,,,https://doi.org/10.32604/cmc.2025.063592,
https://www.sciencedirect.com/science/article/pii/S2949948823000033,"A study on ChatGPT for Industry 4.0: Background, potentials, challenges, and eventualities",,https://www.sciencedirect.com/science/article/pii/S2949948823000033/pdfft?md5=a29a7ee67c3e05fa7018dac392c697c3&pid=1-s2.0-S2949948823000033-main.pdf,,https://doi.org/10.1016/j.ject.2023.08.001,
https://www.sciencedirect.com/science/article/pii/S1568494625001164,Path analysis for effective fault localization in deep neural networks,,,,https://doi.org/10.1016/j.asoc.2025.112805,
https://www.sciencedirect.com/science/article/pii/S0952197624007802,A neural network transformer model for composite microstructure homogenization,,,,https://doi.org/10.1016/j.engappai.2024.108622,
https://www.sciencedirect.com/science/article/pii/S0164121224003741,Effectiveness of symmetric metamorphic relations on validating the stability of code generation LLM,,,,https://doi.org/10.1016/j.jss.2024.112330,
https://www.sciencedirect.com/science/article/pii/S0167642324000789,TR-Fuzz: A syntax valid tool for fuzzing C compilers,,,,https://doi.org/10.1016/j.scico.2024.103155,
https://www.sciencedirect.com/science/article/pii/S0950584925000175,Classification and challenges of non-functional requirements in ML-enabled systems: A systematic literature review,,https://www.sciencedirect.com/science/article/pii/S0950584925000175/pdfft?md5=7a5fe6dfdb4a0f8ecb81e0781cb6e9ac&pid=1-s2.0-S0950584925000175-main.pdf,,https://doi.org/10.1016/j.infsof.2025.107678,
https://www.sciencedirect.com/science/article/pii/S1877050925011913,Graph-Based Generation and Validation of Use Case Diagrams,,https://www.sciencedirect.com/science/article/pii/S1877050925011913/pdfft?md5=4725f3c9632bfbdd8912fbf3b0676521&pid=1-s2.0-S1877050925011913-main.pdf,,https://doi.org/10.1016/j.procs.2025.04.090,
https://www.sciencedirect.com/science/article/pii/S0925231224015170,H3NI: Non-target-specific node injection attacks on hypergraph neural networks via genetic algorithm,,,,https://doi.org/10.1016/j.neucom.2024.128746,
https://www.sciencedirect.com/science/article/pii/S0164121225001050,CubeAgent: Efficient query-based video adversarial examples generation through deep reinforcement learning,,,,https://doi.org/10.1016/j.jss.2025.112437,
https://www.sciencedirect.com/science/article/pii/S2405896324000016,Natural Language Processing based Auto Generation of Proof Obligations for Formal Verification of Control Requirements in Safety-Critical Systems,,https://www.sciencedirect.com/science/article/pii/S2405896324000016/pdfft?md5=0ad25c02725ab7f832b1e909885184c7&pid=1-s2.0-S2405896324000016-main.pdf,,https://doi.org/10.1016/j.ifacol.2024.05.001,
https://www.sciencedirect.com/science/article/pii/S0957417424017196,"A systematic review of trimodal affective computing approaches: Text, audio, and visual integration in emotion recognition and sentiment analysis",,,,https://doi.org/10.1016/j.eswa.2024.124852,
https://www.sciencedirect.com/science/article/pii/B9781555580759500156,9: LONG-LIFE SYSTEMS,,,,https://doi.org/10.1016/B978-1-55558-075-9.50015-6,
https://www.sciencedirect.com/science/article/pii/S0094576597001148,Project Galileo at Jupiter,,,,https://doi.org/10.1016/S0094-5765(97)00114-8,
https://www.sciencedirect.com/science/article/pii/S0031320323007148,"FoodMask: Real-time food instance counting, segmentation and recognition",,,,https://doi.org/10.1016/j.patcog.2023.110017,
https://www.sciencedirect.com/science/article/pii/S2405844024003207,RETRACTED: The application and challenges of ChatGPT in educational transformation: New demands for teachers' roles,,https://www.sciencedirect.com/science/article/pii/S2405844024003207/pdfft?md5=1cc131f48dae40102d1f2defd73d4b45&pid=1-s2.0-S2405844024003207-main.pdf,,https://doi.org/10.1016/j.heliyon.2024.e24289,
https://www.sciencedirect.com/science/article/pii/S2665963825000211,EduXgame: Gamified learning for secondary education,,https://www.sciencedirect.com/science/article/pii/S2665963825000211/pdfft?md5=1355313e6031107140eef52bcb42dc73&pid=1-s2.0-S2665963825000211-main.pdf,,https://doi.org/10.1016/j.simpa.2025.100761,
https://www.sciencedirect.com/science/article/pii/S0167926024001305,A 3D-stack DRAM-based PNM architecture design,,,,https://doi.org/10.1016/j.vlsi.2024.102266,
https://www.sciencedirect.com/science/article/pii/S2215016124004291,Exploring the change management framework: An in-depth investigation,,https://www.sciencedirect.com/science/article/pii/S2215016124004291/pdfft?md5=b97e01105083c5bda2efdfb57cc87d36&pid=1-s2.0-S2215016124004291-main.pdf,,https://doi.org/10.1016/j.mex.2024.102978,
https://www.sciencedirect.com/science/article/pii/S2949719124000682,The performance of the LSTM-based code generated by Large Language Models (LLMs) in forecasting time series data,,https://www.sciencedirect.com/science/article/pii/S2949719124000682/pdfft?md5=3bc96402ae6e00f32c65f9e2287b1953&pid=1-s2.0-S2949719124000682-main.pdf,,https://doi.org/10.1016/j.nlp.2024.100120,
https://www.sciencedirect.com/science/article/pii/S0007681324000697,Navigating software development in the ChatGPT and GitHub Copilot era,,https://www.sciencedirect.com/science/article/pii/S0007681324000697/pdfft?md5=5a9279376151ad1be35134828ecf3e36&pid=1-s2.0-S0007681324000697-main.pdf,,https://doi.org/10.1016/j.bushor.2024.05.009,
https://www.sciencedirect.com/science/article/pii/S2452414X25000585,Advancing software security: DCodeBERT for automatic vulnerability detection and repair,,,,https://doi.org/10.1016/j.jii.2025.100834,
https://www.sciencedirect.com/science/article/pii/S0016508579802590,Abstracts of Papers Submitted to the American Gastroenterological Association,,,,https://doi.org/10.1016/S0016-5085(79)80259-0,
https://www.sciencedirect.com/science/article/pii/S0164121200000509,A method and tool for assessing object-oriented projects and metrics management,,,,https://doi.org/10.1016/S0164-1212(00)00050-9,
https://www.sciencedirect.com/science/article/pii/S0951832097001063,A probabilistic methodology for the design of radiological confinement of tokamak reactors,,,,https://doi.org/10.1016/S0951-8320(97)00106-3,
https://www.sciencedirect.com/science/article/pii/S147466701763427X,Preface,,https://www.sciencedirect.com/science/article/pii/S147466701763427X/pdfft?md5=5a393e8835bc957a4a581ea987e45f8e&pid=1-s2.0-S147466701763427X-main.pdf,,https://doi.org/10.1016/S1474-6670(17)63427-X,
https://www.sciencedirect.com/science/article/pii/002954939501059Q,Operational monitoring in German nuclear power plants,,,,https://doi.org/10.1016/0029-5493(95)01059-Q,
https://www.sciencedirect.com/science/article/pii/S0167404824004565,SecureQwen: Leveraging LLMs for vulnerability detection in python codebases,,,,https://doi.org/10.1016/j.cose.2024.104151,
https://www.sciencedirect.com/science/article/pii/S0950584925001053,SolBERT: Advancing solidity smart contract similarity analysis via self-supervised pre-training and contrastive fine-tuning,,,,https://doi.org/10.1016/j.infsof.2025.107766,
https://www.sciencedirect.com/science/article/pii/S2096579624000639,InputJump: Augmented reality-facilitated cross-device input fusion based on spatial and semantic information,,https://www.sciencedirect.com/science/article/pii/S2096579624000639/pdfft?md5=a03f734664dd2f949855d22d9fe2d5b9&pid=1-s2.0-S2096579624000639-main.pdf,,https://doi.org/10.1016/j.vrih.2024.10.001,
https://www.sciencedirect.com/science/article/pii/S1383762124001309,Transformers in source code generation: A comprehensive survey,,,,https://doi.org/10.1016/j.sysarc.2024.103193,
https://www.sciencedirect.com/science/article/pii/S0950584923001155,RoseMatcher: Identifying the impact of user reviews on app updates,,,,https://doi.org/10.1016/j.infsof.2023.107261,
https://www.sciencedirect.com/science/article/pii/S1546221825002553,Amalgamation of Classical and Large Language Models for Duplicate Bug Detection: A Comparative Study,,,,https://doi.org/10.32604/cmc.2025.057792,
https://www.sciencedirect.com/science/article/pii/S2772485925000171,LLMs: A game-changer for software engineers?,,https://www.sciencedirect.com/science/article/pii/S2772485925000171/pdfft?md5=4858e8fae3c834593b062dada825a961&pid=1-s2.0-S2772485925000171-main.pdf,,https://doi.org/10.1016/j.tbench.2025.100204,
https://www.sciencedirect.com/science/article/pii/S1546221825000992,A Critical Review of Methods and Challenges in Large Language Models,,,,https://doi.org/10.32604/cmc.2025.061263,
https://www.sciencedirect.com/science/article/pii/S1568494624014376,"Large language models for cyber resilience: A comprehensive review, challenges, and future perspectives",,,,https://doi.org/10.1016/j.asoc.2024.112663,
https://www.sciencedirect.com/science/article/pii/S0950705125005490,A comprehensive survey on integrating large language models with knowledge-based methods,,https://www.sciencedirect.com/science/article/pii/S0950705125005490/pdfft?md5=0d95833ee725c5af455e91b32c8df990&pid=1-s2.0-S0950705125005490-main.pdf,,https://doi.org/10.1016/j.knosys.2025.113503,
https://www.sciencedirect.com/science/article/pii/S0950584925001405,Does it smell? A homogeneous stacking approach for code smell prediction,,,,https://doi.org/10.1016/j.infsof.2025.107801,
https://www.sciencedirect.com/science/article/pii/S0164121224002255,An architecture for model-based and intelligent automation in DevOps,,https://www.sciencedirect.com/science/article/pii/S0164121224002255/pdfft?md5=d54471e0503d6c6e1beee84152a7b5ac&pid=1-s2.0-S0164121224002255-main.pdf,,https://doi.org/10.1016/j.jss.2024.112180,
https://www.sciencedirect.com/science/article/pii/S095741742303470X,An amalgamation of cognitive aspects in software engineering: A content analysis,,,,https://doi.org/10.1016/j.eswa.2023.122968,
https://www.sciencedirect.com/science/article/pii/S0164121225001876,Comparative analysis of design pattern implementation validity in LLM-based code refactoring,,,,https://doi.org/10.1016/j.jss.2025.112519,
https://www.sciencedirect.com/science/article/pii/S0950584925001247,Translating code with Large Language Models and human-in-the-loop feedback,,https://www.sciencedirect.com/science/article/pii/S0950584925001247/pdfft?md5=99da5a824ba27d394990345f467455e9&pid=1-s2.0-S0950584925001247-main.pdf,,https://doi.org/10.1016/j.infsof.2025.107785,
https://www.sciencedirect.com/science/article/pii/S0950584925001260,Assessing output reliability and similarity of large language models in software development: A comparative case study approach,,,,https://doi.org/10.1016/j.infsof.2025.107787,
https://www.sciencedirect.com/science/article/pii/S154622182400746X,KubeFuzzer: Automating RESTful API Vulnerability Detection in Kubernetes,,,,https://doi.org/10.32604/cmc.2024.055180,
https://www.sciencedirect.com/science/article/pii/S0099133325000813,Research on library management paradigm in the AIGC era: Theoretical construction and practical exploration,,,,https://doi.org/10.1016/j.acalib.2025.103085,
https://www.sciencedirect.com/science/article/pii/S0957417423021425,Aligning XAI explanations with software developers’ expectations: A case study with code smell prioritization,,,,https://doi.org/10.1016/j.eswa.2023.121640,
https://www.sciencedirect.com/science/article/pii/S0029549324006265,Knowledge representation to support EMDAP implementation in advanced reactor licensing applications,,,,https://doi.org/10.1016/j.nucengdes.2024.113526,
https://www.sciencedirect.com/science/article/pii/S0272735824001363,"Efficacy of app-based mobile health interventions for stress management: A systematic review and meta-analysis of self-reported, physiological, and neuroendocrine stress-related outcomes",,https://www.sciencedirect.com/science/article/pii/S0272735824001363/pdfft?md5=3d4c18c53c1bb2fc08034b0551cbea6d&pid=1-s2.0-S0272735824001363-main.pdf,,https://doi.org/10.1016/j.cpr.2024.102515,
https://www.sciencedirect.com/science/article/pii/S1041608025000226,Chatbots in education: Hype or help? A meta-analysis,,https://www.sciencedirect.com/science/article/pii/S1041608025000226/pdfft?md5=234cd4126d57171c04d7e9ec15444849&pid=1-s2.0-S1041608025000226-main.pdf,,https://doi.org/10.1016/j.lindif.2025.102646,
https://www.sciencedirect.com/science/article/pii/S2468502X24000226,AVA: An automated and AI-driven intelligent visual analytics framework,,https://www.sciencedirect.com/science/article/pii/S2468502X24000226/pdfft?md5=d535cfeb7d4bca4f8b918b02581ff6a3&pid=1-s2.0-S2468502X24000226-main.pdf,,https://doi.org/10.1016/j.visinf.2024.06.002,
https://www.sciencedirect.com/science/article/pii/S0360131524002380,Does ChatGPT enhance student learning? A systematic review and meta-analysis of experimental studies,,https://www.sciencedirect.com/science/article/pii/S0360131524002380/pdfft?md5=16997d165adca0bce16cf449f94ab79b&pid=1-s2.0-S0360131524002380-main.pdf,,https://doi.org/10.1016/j.compedu.2024.105224,
https://www.sciencedirect.com/science/article/pii/S0306437915001179,Impact analysis and change propagation in service-oriented enterprises: A systematic review,,,,https://doi.org/10.1016/j.is.2015.06.003,
https://www.sciencedirect.com/science/article/pii/B9780443159916000170,Chapter Eleven: Designing meaningful metrics to demonstrate ethical supervision of autonomous systems: How do you measure that?,,,,https://doi.org/10.1016/B978-0-44-315991-6.00017-0,
https://www.sciencedirect.com/science/article/pii/S0167814006810194,Posters,,,,https://doi.org/10.1016/S0167-8140(06)81019-4,
https://www.sciencedirect.com/science/article/pii/S2307187724002177,The potential of LLMs in hardware design,,https://www.sciencedirect.com/science/article/pii/S2307187724002177/pdfft?md5=29d6e1611fbb5c518a7f65cb35d6c0f7&pid=1-s2.0-S2307187724002177-main.pdf,,https://doi.org/10.1016/j.jer.2024.08.001,
https://www.sciencedirect.com/science/article/pii/S0950584925001193,An empirical study on capability of Large Language Models in understanding code semantics,,,,https://doi.org/10.1016/j.infsof.2025.107780,
https://www.sciencedirect.com/science/article/pii/S0950705123010766,On the effectiveness of graph data augmentation for source code learning,,,,https://doi.org/10.1016/j.knosys.2023.111328,
https://www.sciencedirect.com/science/article/pii/S0950584924002076,E-code: Mastering efficient code generation through pretrained models and expert encoder group,,,,https://doi.org/10.1016/j.infsof.2024.107602,
https://www.sciencedirect.com/science/article/pii/S0950584925001259,Generating vulnerability security fixes with Code Language Models,,https://www.sciencedirect.com/science/article/pii/S0950584925001259/pdfft?md5=e42759a506161c9494fba345e8382b15&pid=1-s2.0-S0950584925001259-main.pdf,,https://doi.org/10.1016/j.infsof.2025.107786,
https://www.sciencedirect.com/science/article/pii/S0950584925000618,A systematic mapping study on graph machine learning for static source code analysis,,https://www.sciencedirect.com/science/article/pii/S0950584925000618/pdfft?md5=f55810e9fc0931072891b72f8d4ed7e6&pid=1-s2.0-S0950584925000618-main.pdf,,https://doi.org/10.1016/j.infsof.2025.107722,
https://www.sciencedirect.com/science/article/pii/S0164121225000524,Poisoned source code detection in code models,,https://www.sciencedirect.com/science/article/pii/S0164121225000524/pdfft?md5=131599722ed4043719df963620f78e16&pid=1-s2.0-S0164121225000524-main.pdf,,https://doi.org/10.1016/j.jss.2025.112384,
https://www.sciencedirect.com/science/article/pii/S0164121223003369,On the impact of multiple source code representations on software engineering tasks — An empirical study,,,,https://doi.org/10.1016/j.jss.2023.111941,
https://www.sciencedirect.com/science/article/pii/S1570826824000441,Knowledge Graphs as a source of trust for LLM-powered enterprise question answering,,https://www.sciencedirect.com/science/article/pii/S1570826824000441/pdfft?md5=05f2c90d1a9b90c0531433fb64115234&pid=1-s2.0-S1570826824000441-main.pdf,,https://doi.org/10.1016/j.websem.2024.100858,
https://www.sciencedirect.com/science/article/pii/S2162098924001178,Accuracy of large language models in answering ophthalmology board-style questions: A meta-analysis,,https://www.sciencedirect.com/science/article/pii/S2162098924001178/pdfft?md5=0efb5252c5a21aba2fea02c1ccf688ca&pid=1-s2.0-S2162098924001178-main.pdf,,https://doi.org/10.1016/j.apjo.2024.100106,
https://www.sciencedirect.com/science/article/pii/S2666920X25000578,Retrieval-augmented generation for educational application: A systematic survey,,https://www.sciencedirect.com/science/article/pii/S2666920X25000578/pdfft?md5=ecf8c7c2b7404daa031b951d26e5b8aa&pid=1-s2.0-S2666920X25000578-main.pdf,,https://doi.org/10.1016/j.caeai.2025.100417,
https://www.sciencedirect.com/science/article/pii/S0927538X24003846,Finance-specific large language models: Advancing sentiment analysis and return prediction with LLaMA 2,,,,https://doi.org/10.1016/j.pacfin.2024.102632,
https://www.sciencedirect.com/science/article/pii/S1532046424000388,Evaluation of ChatGPT-generated medical responses: A systematic review and meta-analysis,,https://www.sciencedirect.com/science/article/pii/S1532046424000388/pdfft?md5=5458319e1eb6602ccfde79a68117f6f2&pid=1-s2.0-S1532046424000388-main.pdf,,https://doi.org/10.1016/j.jbi.2024.104620,
https://www.sciencedirect.com/science/article/pii/S0169260725002871,Visual-language foundation models in medical imaging: A systematic review and meta-analysis of diagnostic and analytical applications,,,,https://doi.org/10.1016/j.cmpb.2025.108870,
https://www.sciencedirect.com/science/article/pii/S0020653925001479,AI-Powered Problem- and Case-based Learning in Medical and Dental Education: A Systematic Review and Meta-analysis,,https://www.sciencedirect.com/science/article/pii/S0020653925001479/pdfft?md5=a6595dbf4bdb50897399e2b19fadc715&pid=1-s2.0-S0020653925001479-main.pdf,,https://doi.org/10.1016/j.identj.2025.100858,
https://www.sciencedirect.com/science/article/pii/S0048733323002019,Epidemic effects in the diffusion of emerging digital technologies: evidence from artificial intelligence adoption,,https://www.sciencedirect.com/science/article/pii/S0048733323002019/pdfft?md5=52bffc65e05868551d05578ceba1f5fd&pid=1-s2.0-S0048733323002019-main.pdf,,https://doi.org/10.1016/j.respol.2023.104917,
https://www.sciencedirect.com/science/article/pii/S1936878X17305193,The Identification of Calcified Coronary Plaque Is Associated With Initiation and Continuation of Pharmacological and Lifestyle Preventive Therapies: A Systematic Review and Meta-Analysis,,https://www.sciencedirect.com/science/article/pii/S1936878X17305193/pdfft?md5=f221c5728b164264f339cd44f5ea0a8f&pid=1-s2.0-S1936878X17305193-main.pdf,,https://doi.org/10.1016/j.jcmg.2017.01.030,
https://www.sciencedirect.com/science/article/pii/S0167494322001327,Effects of multi-component non-pharmacological interventions on cognition in participants with mild cognitive impairment: A systematic review and meta-analysis,,,,https://doi.org/10.1016/j.archger.2022.104751,
https://www.sciencedirect.com/science/article/pii/S1929074825002719,The Prevalence of Hypertension Among Children and Antihypertensive Use: Protocol for a Systematic Review and Meta-Analysis,,,,https://doi.org/10.2196/65807,
https://www.sciencedirect.com/science/article/pii/S0016508525049376,Su1730: TRANSARTERIAL CHEMOEMBOLIZATION COMBINED WITH SORAFENIB VERSUS MONOTHERAPY FOR UNRESECTABLE HEPATOCELLULAR CARCINOMA: AN UMBRELLA REVIEW OF META-ANALYSES,,,,https://doi.org/10.1016/S0016-5085(25)04937-6,
https://www.sciencedirect.com/science/article/pii/S0957417424021572,Software bug prediction using graph neural networks and graph-based text representations,,,,https://doi.org/10.1016/j.eswa.2024.125290,
https://www.sciencedirect.com/science/article/pii/S0950584924001484,Automated description generation for software patches,,,,https://doi.org/10.1016/j.infsof.2024.107543,
https://www.sciencedirect.com/science/article/pii/S2589537025002020,Interventions to improve adherence to lipid-lowering drugs: a systematic review and meta-analysis,,https://www.sciencedirect.com/science/article/pii/S2589537025002020/pdfft?md5=69b634b7bf01f863c3c14faaec1055c3&pid=1-s2.0-S2589537025002020-main.pdf,,https://doi.org/10.1016/j.eclinm.2025.103270,
https://www.sciencedirect.com/science/article/pii/S2666557325000084,The Impact of Generative AI on Essay Revisions and Student Engagement,,https://www.sciencedirect.com/science/article/pii/S2666557325000084/pdfft?md5=1c6511e82cff2ecfb59d370ea944a561&pid=1-s2.0-S2666557325000084-main.pdf,,https://doi.org/10.1016/j.caeo.2025.100249,
https://www.sciencedirect.com/science/article/pii/S001650852403587X,Tu1650 CHANGES IN ANTI-VINCULIN ANTIBODIES OVER TIME CORRELATE WITH SYMPTOM IMPROVEMENT IN IRRITABLE BOWEL SYDROME (IBS),,,,https://doi.org/10.1016/S0016-5085(24)03587-X,
https://www.sciencedirect.com/science/article/pii/S2949882125000465,AI literacy and trust: A multi-method study of Human-GAI team collaboration,,https://www.sciencedirect.com/science/article/pii/S2949882125000465/pdfft?md5=1f0c902f355bbb07c130f500cd3bffa6&pid=1-s2.0-S2949882125000465-main.pdf,,https://doi.org/10.1016/j.chbah.2025.100162,
https://www.sciencedirect.com/science/article/pii/S2667325824001043,A review of deep learning methods for ligand based drug virtual screening,,https://www.sciencedirect.com/science/article/pii/S2667325824001043/pdfft?md5=40bae6513c2c4be760a59d433b191409&pid=1-s2.0-S2667325824001043-main.pdf,,https://doi.org/10.1016/j.fmre.2024.02.011,
https://www.sciencedirect.com/science/article/pii/S0531556524002857,"Lower extremity muscle hypertrophy in response to resistance training in older adults: Systematic review, meta-analysis, and meta-regression of randomized controlled trials",,https://www.sciencedirect.com/science/article/pii/S0531556524002857/pdfft?md5=e98c499bacb9e551b04adf4d301fb483&pid=1-s2.0-S0531556524002857-main.pdf,,https://doi.org/10.1016/j.exger.2024.112639,
https://www.sciencedirect.com/science/article/pii/S0735109724084572,Artificial Intelligence in Cardiovascular Clinical Trials,,https://www.sciencedirect.com/science/article/pii/S0735109724084572/pdfft?md5=0d679095aec680c0cedb3a37e10b995a&pid=1-s2.0-S0735109724084572-main.pdf,,https://doi.org/10.1016/j.jacc.2024.08.069,
https://www.sciencedirect.com/science/article/pii/S1268773124002674,"Diagnosis, treatment, and prevention of ankle sprains: Comparing free chatbot recommendations with clinical guidelines",,https://www.sciencedirect.com/science/article/pii/S1268773124002674/pdfft?md5=f0a3d70405e8b87dd4c7d0241a863e82&pid=1-s2.0-S1268773124002674-main.pdf,,https://doi.org/10.1016/j.fas.2024.12.003,
https://www.sciencedirect.com/science/article/pii/S095741742500288X,NtNDet: Hardware Trojan detection based on pre-trained language models,,,,https://doi.org/10.1016/j.eswa.2025.126666,
https://www.sciencedirect.com/science/article/pii/S0164121224002462,An intelligent test management system for optimizing decision making during software testing,,,,https://doi.org/10.1016/j.jss.2024.112202,
https://www.sciencedirect.com/science/article/pii/S1438887125002420,Automated Process for Monitoring of Amiodarone Treatment: Development and Evaluation,,,,https://doi.org/10.2196/65473,
https://www.sciencedirect.com/science/article/pii/S1877050925003989,Leveraging Personal Assistants for Enhanced Access to Cultural Knowledge: A Case Study,,https://www.sciencedirect.com/science/article/pii/S1877050925003989/pdfft?md5=95c6d783628ea7f4961c001e1db363b7&pid=1-s2.0-S1877050925003989-main.pdf,,https://doi.org/10.1016/j.procs.2025.02.055,
https://www.sciencedirect.com/science/article/pii/S2291969425000195,Digital Representation of Patients as Medical Digital Twins: Data-Centric Viewpoint,,,,https://doi.org/10.2196/53542,
https://www.sciencedirect.com/science/article/pii/S0164121224000037,Flexible control flow graph alignment for delivering data-driven feedback to novice programming learners,,,,https://doi.org/10.1016/j.jss.2024.111960,
https://www.sciencedirect.com/science/article/pii/S2949882123000221,"ChatGPT in education: Methods, potentials, and limitations",,https://www.sciencedirect.com/science/article/pii/S2949882123000221/pdfft?md5=f9aa184eb8668e5dbec672d9482aabfb&pid=1-s2.0-S2949882123000221-main.pdf,,https://doi.org/10.1016/j.chbah.2023.100022,
https://www.sciencedirect.com/science/article/pii/S0167739X24004643,"Cybersecurity for tactical 6G networks: Threats, architecture, and intelligence",,https://www.sciencedirect.com/science/article/pii/S0167739X24004643/pdfft?md5=fc9365c86570d800986a175631a6a13d&pid=1-s2.0-S0167739X24004643-main.pdf,,https://doi.org/10.1016/j.future.2024.107500,
https://www.sciencedirect.com/science/article/pii/S294976122400124X,Global Harmonization of Artificial Intelligence-Enabled Software as a Medical Device Regulation: Addressing Challenges and Unifying Standards,,https://www.sciencedirect.com/science/article/pii/S294976122400124X/pdfft?md5=d9487a0e7d6915f75193e73d2effc3ec&pid=1-s2.0-S294976122400124X-main.pdf,,https://doi.org/10.1016/j.mcpdig.2024.100191,
https://www.sciencedirect.com/science/article/pii/S0306454924000604,Machine learning at the edge to improve in-field safeguards inspections,,,,https://doi.org/10.1016/j.anucene.2024.110398,
https://www.sciencedirect.com/science/article/pii/S0167404824002931,Adversarial Machine Learning in Industry: A Systematic Literature Review,,https://www.sciencedirect.com/science/article/pii/S0167404824002931/pdfft?md5=a4b0427fd2f32dea4e959de2e314829e&pid=1-s2.0-S0167404824002931-main.pdf,,https://doi.org/10.1016/j.cose.2024.103988,
https://www.sciencedirect.com/science/article/pii/S0167642325000565,Testing non-commutativity of reduce functions with multi-column inputs,,,,https://doi.org/10.1016/j.scico.2025.103317,
https://www.sciencedirect.com/science/article/pii/S1389128625002270,A model checking-based framework for testing security properties of protocols under development,,,,https://doi.org/10.1016/j.comnet.2025.111259,
https://www.sciencedirect.com/science/article/pii/S1877050924006306,BiT5: A Bidirectional NLP Approach for Advanced Vulnerability Detection in Codebase,,https://www.sciencedirect.com/science/article/pii/S1877050924006306/pdfft?md5=0ab754addab1b8b10989377ccb28b2ff&pid=1-s2.0-S1877050924006306-main.pdf,,https://doi.org/10.1016/j.procs.2024.03.270,
https://www.sciencedirect.com/science/article/pii/S0164121224003649,An empirical investigation into the capabilities of anomaly detection approaches for test smell detection,,,,https://doi.org/10.1016/j.jss.2024.112320,
https://www.sciencedirect.com/science/article/pii/S0164121224001821,Impermanent identifiers: Enhanced source code comprehension and refactoring,,,,https://doi.org/10.1016/j.jss.2024.112137,
https://www.sciencedirect.com/science/article/pii/S0164121224001584,Automating the correctness assessment of AI-generated code for security contexts,,https://www.sciencedirect.com/science/article/pii/S0164121224001584/pdfft?md5=c7734d2003ab22f80edc9da32fb97026&pid=1-s2.0-S0164121224001584-main.pdf,,https://doi.org/10.1016/j.jss.2024.112113,
https://www.sciencedirect.com/science/article/pii/S1084804524001978,A survey on fuzz testing technologies for industrial control protocols,,,,https://doi.org/10.1016/j.jnca.2024.104020,
https://www.sciencedirect.com/science/article/pii/S1526149224001930,C-CORE: Clustering by Code Representation to Prioritize Test Cases in Compiler Testing,,,,https://doi.org/10.32604/cmes.2023.043248,
https://www.sciencedirect.com/science/article/pii/S0950584922001070,A deep learning-based automated framework for functional User Interface testing,,,,https://doi.org/10.1016/j.infsof.2022.106969,
https://www.sciencedirect.com/science/article/pii/S0957417422012908,UISMiner: Mining UI suggestions from user reviews,,,,https://doi.org/10.1016/j.eswa.2022.118095,
https://www.sciencedirect.com/science/article/pii/S0920548924000722,"Prototype, method, and experiment for evaluating usability of smart home user interfaces",,,,https://doi.org/10.1016/j.csi.2024.103903,
https://www.sciencedirect.com/science/article/pii/S0950584921001488,Developing Mobile Applications Via Model Driven Development: A Systematic Literature Review,,,,https://doi.org/10.1016/j.infsof.2021.106693,
https://www.sciencedirect.com/science/article/pii/S0164121216300140,A systematic mapping study of mobile application testing techniques,,,,https://doi.org/10.1016/j.jss.2016.03.065,
https://www.sciencedirect.com/science/article/pii/S0950584916301434,Improving the delivery cycle: A multiple-case study of the toolchains in Finnish software intensive enterprises,,,,https://doi.org/10.1016/j.infsof.2016.09.001,
https://www.sciencedirect.com/science/article/pii/S0950584922001719,A taxonomy of metrics for GUI-based testing research: A systematic literature review,,,,https://doi.org/10.1016/j.infsof.2022.107062,
https://www.sciencedirect.com/science/article/pii/S0950584913000669,Graphical user interface (GUI) testing: Systematic mapping and repository,,,,https://doi.org/10.1016/j.infsof.2013.03.004,
https://www.sciencedirect.com/science/article/pii/S0164121219300779,A systematic literature review on crowdsourcing in software engineering,,,,https://doi.org/10.1016/j.jss.2019.04.027,
https://www.sciencedirect.com/science/article/pii/S0957417421003419,"Expert systems: Definitions, advantages and issues in medical field applications",,,,https://doi.org/10.1016/j.eswa.2021.114900,
https://www.sciencedirect.com/science/article/pii/S1574119222000591,Perses: A framework for the continuous evaluation of the QoS of distributed mobile applications,,https://www.sciencedirect.com/science/article/pii/S1574119222000591/pdfft?md5=2c9283e580e9bea5f6cd750c6c6cd920&pid=1-s2.0-S1574119222000591-main.pdf,,https://doi.org/10.1016/j.pmcj.2022.101627,
https://www.sciencedirect.com/science/article/pii/S0167739X18310410,On using collaborative economy for test cost reduction in high fragmented environments,,,,https://doi.org/10.1016/j.future.2019.01.023,
https://www.sciencedirect.com/science/article/pii/S0950584923002495,Test Code Flakiness in Mobile Apps: The Developer’s Perspective,,https://www.sciencedirect.com/science/article/pii/S0950584923002495/pdfft?md5=44ad4c76cc1ea46f4186223514d4f17f&pid=1-s2.0-S0950584923002495-main.pdf,,https://doi.org/10.1016/j.infsof.2023.107394,
https://www.sciencedirect.com/science/article/pii/S229196942300025X,Machine Learning–Enabled Clinical Information Systems Using Fast Healthcare Interoperability Resources Data Standards: Scoping Review,,,,https://doi.org/10.2196/48297,
https://www.sciencedirect.com/science/article/pii/S2949719124000621,Validating pretrained language models for content quality classification with semantic-preserving metamorphic relations,,https://www.sciencedirect.com/science/article/pii/S2949719124000621/pdfft?md5=c4eb67477c4d7dcee77747ddc4474843&pid=1-s2.0-S2949719124000621-main.pdf,,https://doi.org/10.1016/j.nlp.2024.100114,
https://www.sciencedirect.com/science/article/pii/S0167404824004899,Towards prompt tuning-based software vulnerability assessment with continual learning,,,,https://doi.org/10.1016/j.cose.2024.104184,
https://www.sciencedirect.com/science/article/pii/S0164121225001372,FedMVA: Enhancing software vulnerability assessment via federated multimodal learning,,,,https://doi.org/10.1016/j.jss.2025.112469,
https://www.sciencedirect.com/science/article/pii/S0950584924000545,Technical risk model of machine learning based software project development - A multinational empirical study using modified Delphi-AHP method,,,,https://doi.org/10.1016/j.infsof.2024.107449,
https://www.sciencedirect.com/science/article/pii/S0164121224001973,Automated program repair for variability bugs in software product line systems,,,,https://doi.org/10.1016/j.jss.2024.112152,
https://www.sciencedirect.com/science/article/pii/S1574013724000650,ISO/IEC quality standards for AI engineering,,,,https://doi.org/10.1016/j.cosrev.2024.100681,
https://www.sciencedirect.com/science/article/pii/S0950584923001179,Metamorphic testing of chess engines,,https://www.sciencedirect.com/science/article/pii/S0950584923001179/pdfft?md5=395c0630ed06ab8bac2daa2e9744f531&pid=1-s2.0-S0950584923001179-main.pdf,,https://doi.org/10.1016/j.infsof.2023.107263,
https://www.sciencedirect.com/science/article/pii/S0164121224001912,End-to-end log statement generation at block-level,,,,https://doi.org/10.1016/j.jss.2024.112146,
https://www.sciencedirect.com/science/article/pii/S0957417424029464,Enhancing deep learning-based side-channel analysis using feature engineering in a fully simulated IoT system,,,,https://doi.org/10.1016/j.eswa.2024.126079,
https://www.sciencedirect.com/science/article/pii/S2949761224001019,Assessments of Generative Artificial Intelligence as Clinical Decision Support Ought to be Incorporated Into Randomized Controlled Trials of Electronic Alerts for Acute Kidney Injury,,https://www.sciencedirect.com/science/article/pii/S2949761224001019/pdfft?md5=5a6aab1a2f9cca0df4d9de135b81f6cf&pid=1-s2.0-S2949761224001019-main.pdf,,https://doi.org/10.1016/j.mcpdig.2024.09.004,
https://www.sciencedirect.com/science/article/pii/S2817170524000206,Generating Synthetic Electronic Health Record Data Using Generative Adversarial Networks: Tutorial,,,,https://doi.org/10.2196/52615,
https://www.sciencedirect.com/science/article/pii/S000145752500048X,Generation of critical pedestrian scenarios for autonomous vehicle testing,,,,https://doi.org/10.1016/j.aap.2025.107962,
https://www.sciencedirect.com/science/article/pii/S2352301825000785,The future of HIV diagnostics: an exemplar in infectious diseases,,,,https://doi.org/10.1016/S2352-3018(25)00078-5,
https://www.sciencedirect.com/science/article/pii/S1319157824002076,An empirical study on the state-of-the-art methods for requirement-to-code traceability link recovery,,https://www.sciencedirect.com/science/article/pii/S1319157824002076/pdfft?md5=8ae41f972f5fcb180b95390116c548b9&pid=1-s2.0-S1319157824002076-main.pdf,,https://doi.org/10.1016/j.jksuci.2024.102118,
https://www.sciencedirect.com/science/article/pii/S236879592400088X,Self-Administered Interventions Based on Natural Language Processing Models for Reducing Depressive and Anxiety Symptoms: Systematic Review and Meta-Analysis,,,,https://doi.org/10.2196/59560,
https://www.sciencedirect.com/science/article/pii/S0363811124000869,Beyond the code: The impact of AI algorithm transparency signaling on user trust and relational satisfaction,,,,https://doi.org/10.1016/j.pubrev.2024.102507,
https://www.sciencedirect.com/science/article/pii/S0893608024009961,Promises and perils of using Transformer-based models for SE research,,https://www.sciencedirect.com/science/article/pii/S0893608024009961/pdfft?md5=3c8784ecc2ac1bd24ef27f6cbfc38c95&pid=1-s2.0-S0893608024009961-main.pdf,,https://doi.org/10.1016/j.neunet.2024.107067,
https://www.sciencedirect.com/science/article/pii/S0140366422002377,Neural language models for network configuration: Opportunities and reality check,,,,https://doi.org/10.1016/j.comcom.2022.06.035,
https://www.sciencedirect.com/science/article/pii/S0164121224003017,"A model-driven, metrics-based approach to assessing support for quality aspects in MLOps system architectures",,https://www.sciencedirect.com/science/article/pii/S0164121224003017/pdfft?md5=28db5a8ea45de669f7ad7b8be9be8c69&pid=1-s2.0-S0164121224003017-main.pdf,,https://doi.org/10.1016/j.jss.2024.112257,
