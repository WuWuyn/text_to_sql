filename,overall_score,letter_grade,research_basics_score,methodology_score,evaluation_score,llm_implementation_score,critical_analysis_score,research_objective_score,method_description_score,research_context_score,evaluation_design_score,metrics_score,baseline_comparison_score,results_presentation_score,llm_integration_score,research_contribution_score,limitations_discussion_score,future_work_score,input_output_definition_score,llm_role,llm_models_used,contribution_types,novelty_level,limitations_depth,research_directions,input_clarity,output_clarity
A Case Study on Test Case Construction with Large Language Models_ Unveiling Practical Insights and .pdf,86.4,B,81.0,83.3,100.0,100.0,72.2,Yes,Partial,Yes,Yes,Yes,Yes,Yes,Yes,Partial,Partial,Yes,Yes,Core component for test case generation,GPT-3.5 Turbo,"Empirical, Methodological (specific prompt engineering approach)","Incremental (application of existing LLMs/techniques in a specific, under-explored context)","Moderate (explicitly mentions several limitations and reasons, e.g., for GPT-3.5 Turbo's difficulty with large inputs)","Comparing Time, Cost and Learning Curve of LLM vs. manual test case creation, Improving prompt design for handling multiple features and integration tests.","Clear (template structure, three-part input).","Clear (JSON for test conditions, markdown for final test cases with specific fields)."
AgentCoder_ Multi-Agent-based Code Generation with Iterative Testing and Optimisation.pdf,76.3,C,100.0,83.3,84.0,100.0,0.0,Yes,Yes,Yes,Partial,Yes,Partial,Yes,Yes,Yes,No,No,Yes,Core component (all agents except executor),"GPT-4, GPT-4-turbo, GPT-3.5-turbo, PaLM Coder, Claude-instant-1, AlphaCode, Incoder, CodeGeeX, StarCoder, CodeLlama, Llama3, CodeGen-Mono, CodeX","Methodological, Empirical",Incremental (novel design choices within existing multi-agent paradigm),Not analyzed for their own work.,,Clear,Clear
