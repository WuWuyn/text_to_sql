link,pdf_link,title,authors,abstract,submitted
https://arxiv.org/abs/2503.20576,https://arxiv.org/pdf/2503.20576,Optimizing Case-Based Reasoning System for FunctionalTest Script Generation with LargeLanguageModels,"Siyuan Guo, Huiwu Liu, Xiaolong Chen, Yuming Xie, Liang Zhang, Tao Han, Hechang Chen, Yi Chang, Jun Wang","In this work, we explore the potential of largelanguagemodels (LLMs) for generating functionaltest scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs.","Submitted 27 May, 2025; v1 submitted 26 March, 2025; originally announced March 2025."
https://arxiv.org/abs/2408.00161,https://arxiv.org/pdf/2408.00161,Automatic Generation of Behavioral Test Cases For Natural Language Processing Using Clustering and Prompting,"Ying Li, Rahul Singh, Tarun Joshi, Agus Sudjianto","Recent work in behavioral testing for natural language processing (NLP) models, such as Checklist, is inspired by related paradigms in software engineering testing. They allow evaluation of general linguistic capabilities and domain understanding, hence can help evaluate conceptual soundness and identify model weaknesses. However, a major challenge is the creation of test cases. The current packages rely on semi-automated approach using manual development which requires domain expertise and can be time consuming. This paper introduces an automated approach to develop test cases by exploiting the power of largelanguagemodels and statistical techniques. It clusters the text representations to carefully construct meaningful groups and then apply prompting techniques to automatically generate Minimal FunctionalityTests (MFT). The well-known Amazon Reviews corpus is used to demonstrate our approach. We analyze the behavioral test profiles across four different classification algorithms and discuss the limitations and strengths of those models.","Submitted 8 August, 2024; v1 submitted 31 July, 2024; originally announced August 2024."
