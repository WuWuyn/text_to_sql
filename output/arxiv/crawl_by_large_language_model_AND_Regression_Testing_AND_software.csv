link,pdf_link,title,authors,abstract,submitted
https://arxiv.org/abs/2501.11086,https://arxiv.org/pdf/2501.11086,Can LLM Generate RegressionTests for Software Commits?,"Jing Liu, Seongmin Lee, Eleonora Losiouk, Marcel Böhme","LargeLanguageModels (LLMs) have shown tremendous promise in automated software engineering. In this paper, we investigate the opportunities of LLMs for automatic regressiontest generation for programs that take highly structured, human-readable inputs, such as XML parsers or JavaScript interpreters. Concretely, we explore the following regressiontest generation scenarios for such programs that have so far been difficult to test automatically in the absence of corresponding input grammars: \bullet
 Bug finding. Given a code change (e.g., a commit or pull request), our LLM-based approach generates a test case with the objective of revealing any bugs that might be introduced if that change is applied. \bullet
 Patch testing. Given a patch, our LLM-based approach generates a test case that fails before but passes after the patch. This test can be added to the regressiontest suite to catch similar bugs in the future. We implement Cleverest, a feedback-directed, zero-shot LLM-based regressiontest generation technique, and evaluate its effectiveness on 22 commits to three subject programs: Mujs, Libxml2, and Poppler. For programs using more human-readable file formats, like XML or JavaScript, we found Cleverest performed very well. It generated easy-to-understand bug-revealing or bug-reproduction test cases for the majority of commits in just under three minutes -- even when only the code diff or commit message (unless it was too vague) was given. For programs with more compact file formats, like PDF, as expected, it struggled to generate effective test cases. However, the LLM-supplied test cases are not very far from becoming effective (e.g., when used as a seed by a greybox fuzzer or as a starting point by the developer).","Submitted 19 January, 2025; originally announced January 2025."
https://arxiv.org/abs/2311.11123,https://arxiv.org/pdf/2311.11123,(Why) Is My Prompt Getting Worse? Rethinking RegressionTesting for Evolving LLM APIs,"Wanqin Ma, Chenyang Yang, Christian Kästner","LargeLanguageModels (LLMs) are increasingly integrated into software applications. Downstream application developers often access LLMs through APIs provided as a service. However, LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices, as evidenced by our case study on toxicity detection. Based on our case study, we emphasize the need for and re-examine the concept of regressiontesting for evolving LLM APIs. We argue that regressiontesting LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.","Submitted 6 February, 2024; v1 submitted 18 November, 2023; originally announced November 2023."
