link,pdf_link,title,authors,abstract,submitted
https://arxiv.org/abs/2401.17626,https://arxiv.org/pdf/2401.17626,Generative AI to Generate TestDataGenerators,"Benoit Baudry, Khashayar Etemadi, Sen Fang, Yogya Gamage, Yi Liu, Yuxin Liu, Martin Monperrus, Javier Ron, Andr√© Silva, Deepika Tiwari","Generating fake data is an essential dimension of modern software testing, as demonstrated by the number and significance of data faking libraries. Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains. In this paper, we assess the ability of generative AI for generating test data in different domains. We design three types of prompts for LargeLanguageModels (LLMs), which perform testdatageneration tasks at different levels of integrability: 1) raw testdatageneration, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries. We evaluate our approach by prompting LLMs to generate test data for 11 domains. The results show that LLMs can successfully generate realistic testdatagenerators in a wide range of domains at all three levels of integrability.","Submitted 14 June, 2024; v1 submitted 31 January, 2024; originally announced January 2024."
