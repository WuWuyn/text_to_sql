link,pdf_link,title,authors,abstract,submitted,functional_testing,functional,llm,functional_testing_count,functional_count,llm_count,relevance_score,t2sql,security
https://arxiv.org/abs/2408.00161,https://arxiv.org/pdf/2408.00161,Automatic Generation of Behavioral Test Cases For Natural Language Processing Using Clustering and Prompting,"Ying Li, Rahul Singh, Tarun Joshi, Agus Sudjianto","Recent work in behavioral testing for natural language processing (NLP) models, such as Checklist, is inspired by related paradigms in software engineering testing. They allow evaluation of general linguistic capabilities and domain understanding, hence can help evaluate conceptual soundness and identify model weaknesses. However, a major challenge is the creation of test cases. The current packages rely on semi-automated approach using manual development which requires domain expertise and can be time consuming. This paper introduces an automated approach to develop test cases by exploiting the power of largelanguagemodels and statistical techniques. It clusters the text representations to carefully construct meaningful groups and then apply prompting techniques to automatically generate Minimal FunctionalityTests (MFT). The well-known Amazon Reviews corpus is used to demonstrate our approach. We analyze the behavioral test profiles across four different classification algorithms and discuss the limitations and strengths of those models.",,True,True,True,140,161,20,107.0,True,True
https://arxiv.org/abs/2402.15238,https://arxiv.org/pdf/2402.15238,GPT-HateCheck: Can LLMs Write Better FunctionalTests for Hate Speech Detection?,"Yiping Jin, Leo Wanner, Alexander Shvets","Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind ""You are just a [slur] to me."" However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functionaltests from scratch by instructing largelanguagemodels (LLMs). We employ an additional natural language inference (NLI) model to verify the generations. Crowd-sourced annotation demonstrates that the generated test cases are of high quality. Using the new functionaltests, we can uncover model weaknesses that would be overlooked using the original HateCheck dataset.",,True,True,True,144,143,34,107.0,True,True
https://arxiv.org/abs/2406.17132,https://arxiv.org/pdf/2406.17132,LLM-Aided Testbench Generation and Bug Detection for Finite-State Machines,"Jitendra Bhandari, Johann Knechtel, Ramesh Narayanaswamy, Siddharth Garg, Ramesh Karri","This work investigates the potential of tailoring LargeLanguageModels (LLMs), specifically GPT3.5 and GPT4, for the domain of chip testing. A key aspect of chip design is functionaltesting, which relies on testbenches to evaluate the functionality and coverage of Register-Transfer Level (RTL) designs. We aim to enhance testbench generation by incorporating feedback from commercial-grade Electronic Design Automation (EDA) tools into LLMs. Through iterative feedback from these tools, we refine the testbenches to achieve improved test coverage. Our case studies present promising results, demonstrating that this approach can effectively enhance test coverage. By integrating EDA tool feedback, the generated testbenches become more accurate in identifying potential issues in the RTL design. Furthermore, we extended our study to use this enhanced test coverage framework for detecting bugs in the RTL implementations",,True,True,True,167,123,22,104.0,True,True
https://arxiv.org/abs/2503.20576,https://arxiv.org/pdf/2503.20576,Optimizing Case-Based Reasoning System for FunctionalTest Script Generation with LargeLanguageModels,"Siyuan Guo, Huiwu Liu, Xiaolong Chen, Yuming Xie, Liang Zhang, Tao Han, Hechang Chen, Yi Chang, Jun Wang","In this work, we explore the potential of largelanguagemodels (LLMs) for generating functionaltest scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs.",,True,True,True,162,105,39,102.0,True,True
https://arxiv.org/abs/2305.13276,https://arxiv.org/pdf/2305.13276,Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection,"Mithun Das, Saurabh Kumar Pandey, Animesh Mukherjee","Hate speech is a severe issue that affects many online platforms. So far, several studies have been performed to develop robust hate speech detection systems. Largelanguagemodels like ChatGPT have recently shown a great promise in performing several tasks, including hate speech detection. However, it is crucial to comprehend the limitations of these models to build robust hate speech detection systems. To bridge this gap, our study aims to evaluate the strengths and weaknesses of the ChatGPT model in detecting hate speech at a granular level across 11 languages. Our evaluation employs a series of functionalitytests that reveals various intricate failures of the model which the aggregate metrics like macro F1 or accuracy are not able to unfold. In addition, we investigate the influence of complex emotions, such as the use of emojis in hate speech, on the performance of the ChatGPT model. Our analysis highlights the shortcomings of the generative models in detecting certain types of hate speech and highlighting the need for further research and improvements in the workings of these models.",,True,True,True,51,116,20,62.333333333333336,True,True
https://arxiv.org/abs/2306.12643,https://arxiv.org/pdf/2306.12643,FLAG: Finding Line Anomalies (in code) with Generative AI,"Baleegh Ahmad, Benjamin Tan, Ramesh Karri, Hammond Pearce","Code contains security and functional bugs. The process of identifying and localizing them is difficult and relies on human labor. In this work, we present a novel approach (FLAG) to assist human debuggers. FLAG is based on the lexical capabilities of generative AI, specifically, LargeLanguageModels (LLMs). Here, we input a code file then extract and regenerate each line within that file for self-comparison. By comparing the original code with an LLM-generated alternative, we can flag notable differences as anomalies for further inspection, with features such as distance from comments and LLM confidence also aiding this classification. This reduces the inspection search space for the designer. Unlike other automated approaches in this area, FLAG is language-agnostic, can work on incomplete (and even non-compiling) code and requires no creation of security properties, functionaltests or definition of rules. In this work, we explore the features that help LLMs in this classification and evaluate the performance of FLAG on known bugs. We use 121 benchmarks across C, Python and Verilog; with each benchmark containing a known security or functional weakness. We conduct the experiments using two state of the art LLMs in OpenAI's code-davinci-002 and gpt-3.5-turbo, but our approach may be used by other models. FLAG can identify 101 of the defects and helps reduce the search space to 12-17% of source code.",,True,True,True,26,118,41,61.666666666666664,True,True
https://arxiv.org/abs/2405.01842,https://arxiv.org/pdf/2405.01842,SGHateCheck: FunctionalTests for Detecting Hate Speech in Low-Resource Languages of Singapore,"Ri Chi Ng, Nirmalendu Prakash, Ming Shan Hee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee","To address the limitations of current hate speech detection models, we introduce \textsf{SGHateCheck}, a novel framework designed for the linguistic and cultural context of Singapore and Southeast Asia. It extends the functionaltesting approach of HateCheck and MHC, employing largelanguagemodels for translation and paraphrasing into Singapore's main languages, and refining these with native annotators. \textsf{SGHateCheck} reveals critical flaws in state-of-the-art models, highlighting their inadequacy in sensitive content moderation. This work aims to foster the development of more effective hate speech detection tools for diverse linguistic environments, particularly for Singapore and Southeast Asia contexts.",,True,True,True,21,58,12,30.333333333333332,True,True
https://arxiv.org/abs/2505.06149,https://arxiv.org/pdf/2505.06149,Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study,"Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser","Despite growing interest in automated hate speech detection, most existing approaches overlook the linguistic diversity of online content. Multilingual instruction-tuned largelanguagemodels such as LLaMA, Aya, Qwen, and BloomZ offer promising capabilities across languages, but their effectiveness in identifying hate speech through zero-shot and few-shot prompting remains underexplored. This work evaluates LLM prompting-based detection across eight non-English languages, utilizing several prompting techniques and comparing them to fine-tuned encoder models. We show that while zero-shot and few-shot prompting lag behind fine-tuned encoder models on most of the real-world evaluation sets, they achieve better generalization on functionaltests for hate speech detection. Our study also reveals that prompt design plays a critical role, with each language often requiring customized prompting techniques to maximize performance.",,True,True,True,11,44,23,26.0,True,True
