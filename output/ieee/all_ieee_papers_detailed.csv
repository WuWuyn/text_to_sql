link,title,authors,pdf_link,abstract,doi,submitted
https://ieeexplore.ieee.org/document/11024318/,To Mock or Not to Mock: Divergence in Mocking Practices Between LLM and Developers,['Hanbin Qin'],,"Mock objects are essential for isolating unit tests and reducing dependencies in software testing. However, deciding what to mock requires careful judgment to balance isolation and maintainability. This study evaluates OpenAI's GPT-4o for automating mock decisions by comparing its outputs with developer choices. The findings reveal that while the LLM excels in identifying dependencies, their broader isolation strategy often results in Over-mocking compared to the developers. These insights suggest the potential for LLM-based tools to generate test cases with accurate and well-balanced mocking strategies.",https://doi.org/10.1109/ICSE-Companion66252.2025.00077,27 April 2025 - 03 May 2025
https://ieeexplore.ieee.org/document/10988981/,Leveraging Large Language Models for Explicit Wait Management in End-to-End Web Testing,"['Dario Olianas', 'Maurizio Leotta', 'Filippo Ricca']",,"End-to-end (E2E) testing is an approach in which an application is automatically tested through scripts that simulate the actions a user would perform. Properly managing asynchronous interactions is crucial in this approach to avoid test failures and flakiness. In the Selenium WebDriver framework, this is typically addressed by using thread sleeps (which pause the test for a fixed time) or explicit waits (function calls that pause the test execution until a specified condition is met). Explicit waits require the selection of both a condition to wait for (e.g., element visibility, element clickability) and an element on which that condition applies. Since thread sleeps are unreliable and replacing them with appropriate explicit waits is a time consuming task, in this work, we leverage a Large Language Model (LLM) to assist testers in selecting the most appropriate explicit waits. We defined a structured procedure (a series of prompts) for engaging with the LLM and validated this approach empirically on three test suites affected by asynchronous waiting issues, as well as on 12 synthetic examples. Additionally, we compared our approach with SleepReplacer, the current state-of-the-art tool for replacing thread sleeps with explicit waits in E2E web test suites. The results show that the LLM-based approach can automatically replace the majority of thread sleeps in a test suite on the first attempt, outperforming SleepReplacer.",https://doi.org/10.1109/ICST62969.2025.10988981,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10904141/,Exploring the Integration of Generative AI Tools in Software Testing Education: A Case Study on ChatGPT and Copilot for Preparatory Testing Artifacts in Postgraduate Learning,"['Susmita Haldar', 'Mary Pierce', 'Luiz Fernando Capretz']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10904141,"Software testing education is important for building qualified testing professionals. To ensure that software testing graduates are ready for real-world challenges, it is necessary to integrate modern tools and technologies into the curriculum. With the emergence of Large Language Models (LLMs), their potential use in software engineering has become a focus, but their application in software testing education remains largely unexplored. This study, conducted in the Capstone Project course of a postgraduate software testing program, was carried out over two semesters with two distinct groups of students. A custom-built Travel Application limited to a web platform was used in the first semester. In the second semester, a new set of students worked with an open-source application, offering a larger-scale, multi-platform experience across web, desktop, and mobile platforms. Students initially created preparatory testing artifacts manually as a group deliverable. Following this, they were assigned an individual assignment to generate the same artifacts using LLM tools such as ChatGPT 3.5 in the first semester and Microsoft Copilot in the second. This process directly compared manually created artifacts and those generated using LLMs, leveraging AI for faster outputs. After completion, they responded to a set of assigned questions. The students’ responses were assessed using an integrated methodology, including quantitative and qualitative assessments, sentiment analysis to understand emotions, and a thematic approach to extract deeper insights. The findings revealed that while LLMs can assist and augment manual testing efforts, they cannot entirely replace the need for manual testing. By incorporating innovative technology into the curriculum, this study highlights how Generative AI can support active learning, connect theoretical concepts with practical applications, and align educational practices with industry needs.",https://doi.org/10.1109/ACCESS.2025.3545882,
https://ieeexplore.ieee.org/document/10989038/,Evaluation of the Choice of LLM in a Multi-Agent Solution for GUI-Test Generation,"['Stevan Tomic', 'Emil Alégroth', 'Maycel Isaac']",,"Automated testing, particularly for GUI-based systems, remains a costly and labor-intensive process and prone to errors. Despite advancements in automation, manual testing still dominates in industrial practice, resulting in delays, higher costs, and increased error rates. Large Language Models (LLMs) have shown great potential to automate tasks traditionally requiring human intervention, leveraging their cognitive-like abilities for test generation and evaluation. In this study, we present PathFinder, a Multi-Agent LLM (MALLM) framework that incorporates four agents responsible for (a) perception and summarization, (b) decision-making, (c) input handling and extraction, and (d) validation, which work collaboratively to automate exploratory web-based GUI testing. The goal of this study is to assess how different LLMs, applied to different agents, affect the efficacy of automated exploratory GUI testing. We evaluate PathFinder with three models, Mistral-Nemo, Gemma2, and Llama3.1, on four e-commerce websites. Thus, 27 permutations of the LLMs, across three agents (excluding the validation agent), to test the hypothesis that a solution with multiple agents, each using different LLMs, is more efficacious (efficient and effective) than a multi-agent solution where all agents use the same LLM. The results indicate that the choice of LLM constellation (combination of LLMs) significantly impacts efficacy, suggesting that a single LLM across agents may yield the best balance of efficacy (measured by F1-score). Hypothesis to explain this result include, but are not limited to: improved decision-making consistency and reduced task coordination discrepancies. The contributions of this study are an architecture for MALLM-based GUI testing, empirical results on its performance, and novel insights into how LLM selection impacts the efficacy of automated testing.",https://doi.org/10.1109/ICST62969.2025.10989038,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10876166/,A Retrospective on Whole Test Suite Generation: On the Role of SBST in the Age of LLMs,"['Gordon Fraser', 'Andrea Arcuri']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10876166,"This paper presents a retrospective of the article “Whole Test Suite Generation”, published in the IEEE Transactions on Software Engineering, in 2012. We summarize its main contributions, and discuss how this work impacted the research field of Search-Based Software Testing (SBST) in the last 12 years. The novel techniques presented in the paper were implemented in the tool EvoSuite, which has been so far the state-of-the-art in unit test generation for Java programs using SBST. SBST has shown practical and impactful applications, creating the foundations to open the doors to tackle several other software testing problems besides unit testing, like for example system testing of Web APIs with EvoMaster. We conclude our retrospective with our reflections on what lies ahead, especially considering the important role that SBST still plays even in the age of Large Language Models (LLMs).",https://doi.org/10.1109/TSE.2025.3539458,
https://ieeexplore.ieee.org/document/10910287/,Comparing the Adaptability of a Genetic Algorithm and an LLM-Based Framework for Automated Software Test Data Generation: In the Context of Web Applications,"['Sashini Wanigasekara', 'Dinesh Asanka', 'Chathura Rajapakse', 'Dilani Wickramaarachchi', 'Abhiru Wijesinghe']",,"In the fast-paced world of software development, ensuring software quality is paramount. Software Quality Assurance (SQA) plays a vital role, primarily through testing, which can be carried out manually or automatically. Yet, creating comprehensive test data (TD) for web applications can be a formidable task. Manual test data generation (TDG) is time-consuming and error prone. Automation of TDG has become increasingly important in the realm of software quality assurance as it enables efficient and effective testing of software systems. The need for an appropriate framework for automated TDG is critical to achieve comprehensive and reliable test coverage. Automated TDG offers significant advantages, including time and resource savings, improved test coverage, and seamless integration into the software development process. The core aim of this research is to bridge the gap between manual and existing automated methods, resulting in time and cost savings, heightened testing efficiency, and elevated software quality. Research objectives encompass comparing the adaptability of an AGA based automated TDG model and a LLM based automated TDG model to a web application. The results from the LLM model for triangle classification program was found to be potentially acceptable and accurate than the AGA model's results. This research discusses the challenges encountered when implementing and using the AGA-based framework in the web application context and how an LLM model could overcome the challenges. The study highlights the benefits of using the LLM approach, demonstrating its relevance and accuracy in generating test data compared to the Genetic Algorithm-based model. The practical implications for software quality assurance practices are discussed, emphasizing the enhanced efficiency and effectiveness of the LLM model in improving software quality.",https://doi.org/10.1109/ICDDS62937.2024.10910287,05-07 December 2024
https://ieeexplore.ieee.org/document/10989025/,LLMs in the Heart of Differential Testing: A Case Study on a Medical Rule Engine,"['Erblin Isaku', 'Christoph Laaber', 'Hassan Sartaj', 'Shaukat Ali', 'Thomas Schwitalla', 'Jan F. Nygård']",,"The Cancer Registry of Norway (CRN) uses an automated cancer registration support system (CaReSS) to support core cancer registry activities, i.e., data capture, data curation, and producing data products and statistics for various stakeholders. GURI is a core component of CaReSS, which is responsible for validating incoming data with medical rules. Such medical rules are manually implemented by medical experts based on medical standards, regulations, and research. Since large language models (LLMs) have been trained on a large amount of public information, including these documents, they can be employed to generate tests for GURI. Thus, we propose an LLM-based test generation and differential testing approach (LLMeDiff) to test GURI. We experimented with four different LLMs, two medical rule engine implementations, and 58 real medical rules to investigate the hallucination, success, time efficiency, and robustness of the LLMs to generate tests, and these tests' ability to find potential issues in GURI. Our results showed that GPT-3.5 hallucinates the least, is the most successful, and is generally the most robust; however, it has the worst time efficiency. Our differential testing revealed 22 medical rules where implementation inconsistencies were discovered (e.g., regarding handling rule versions). Finally, we provide insights for practitioners and researchers based on the results.",https://doi.org/10.1109/ICST62969.2025.10989025,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10660826/,Towards LLM-Assisted System Testing for Microservices,"['Mustafa Almutawa', 'Qusai Ghabrah', 'Marco Canini']",,"As modern applications are being designed in a distributed, Microservices Architecture (MSA), it becomes increasingly difficult to debug and test those systems. Typically, it is the role of software testing engineers or Quality Assurance (QA) engineers to write software tests to ensure the reliability of applications, but such a task can be labor-intensive and time-consuming. In this paper, we explore the potential of Large Language Models (LLMs) in assisting software engineers in generating test cases for software systems, with a particular focus on performing end-to-end (black-box) system testing on web-based MSA applications. We present our experience building Kashef, a software testing tool that utilizes the advanced capabilities of current LLMs in code generation and reasoning, and builds on top of the concept of communicative agents.",https://doi.org/10.1109/ICDCSW63686.2024.00011,23-23 July 2024
https://ieeexplore.ieee.org/document/10962456/,SleepReplacer-GPT: AI-Based Thread Sleep Replacement in Selenium WebDriver Tests,"['Dario Olianas', 'Maurizio Leotta', 'Filippo Ricca']",,"Ensuring the quality of modern web applications through end-to-end (E2E) testing is crucial, especially for dynamic systems like single-page applications. Managing asynchronous calls effectively is a key challenge, often addressed using thread sleeps or explicit waits. While thread sleeps are simple to use, they cause inefficiencies and flakiness, whereas explicit waits are more efficient but demand careful implementation.This work explores extending SleepReplacer, a tool that automatically replaces thread sleeps with explicit waits in Selenium WebDriver test suites. We aim to enhance its capabilities by integrating it with ChatGPT, enabling intelligent and automated replacement of thread sleeps with optimal explicit waits. This integration aims to improve code quality and reduce flakiness.We developed a structured procedure for interacting with ChatGPT and validated it on three test suites and synthetic examples covering diverse cases.Results show that the LLM-based approach correctly replaces thread sleeps with explicit waits on the first attempt, consistently outperforming SleepReplacer. These findings support integrating ChatGPT with SleepReplacer to create a smarter, more efficient tool for managing asynchronous behavior in test suites.",https://doi.org/10.1109/ICSTW64639.2025.10962456,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10831260/,Try-Then-Eval: Equipping an LLM-based Agent with a Two-Phase Mechanism to Solve Computer Tasks,"['Duy Cao', 'Phu Nguyen', 'Vy Le', 'Long Nguyen', 'Vu Nguyen']",,"Building an autonomous intelligent agent capable of carrying out web automation tasks from descriptions in natural language offers a wide range of applications, including software testing, virtual assistants, and task automation in general. However, recent studies addressing this problem often require manually constructing of prior human demonstrations. In this paper, we approach the problem by leveraging the idea of reinforcement learning (RL) with the two-phase mechanism to form an agent using LLMs for automating computer tasks without relying on human demonstrations. We evaluate our LLM-based agent using the MiniWob++ dataset of web-based application tasks, showing that our approach achieves 85% success rate without prior demonstrations. The results also demonstrate the agent's capability of self-improvement through training.",https://doi.org/10.1109/SMC54092.2024.10831260,06-10 October 2024
https://ieeexplore.ieee.org/document/10764850/,SoVAR: Building Generalizable Scenarios from Accident Reports for Autonomous Driving Testing,"['An Guo', 'Yuan Zhou', 'Haoxiang Tian', 'Chunrong Fang', 'Yunjian Sun', 'Weisong Sun']",,"Autonomous driving systems (ADSs) have undergone remarkable development and are increasingly employed in safety-critical applications. However, recently reported data on fatal accidents involving ADSs suggests that the desired level of safety has not yet been fully achieved. Consequently, there is a growing need for more comprehensive and targeted testing approaches to ensure safe driving. Scenarios from real-world accident reports provide valuable resources for ADS testing, including critical scenarios and high-quality seeds. However, existing scenario reconstruction methods from accident reports often exhibit limited accuracy in information extraction. Moreover, due to the diversity and complexity of road environments, matching current accident information with the simulation map data for reconstruction poses significant challenges.In this paper, we design and implement SoVAR, a tool for automatically generating road-generalizable scenarios from accident reports. SoVAR utilizes well-designed prompts with linguistic patterns to guide the large language model (LLM) in extracting accident information from textual data. Subsequently, it formulates and solves accident-related constraints in conjunction with the extracted accident information to generate accident trajectories. Finally, SoVAR reconstructs accident scenarios on various map structures and converts them into test scenarios to evaluate its capability to detect defects in industrial ADSs. We experiment with SoVAR, using the accident reports from the National Highway Traffic Safety Administration’s (NHTSA) database to generate test scenarios for the industrial-grade ADS Apollo. The experimental findings demonstrate that SoVAR can effectively generate generalized accident scenarios across different road structures. Furthermore, the results confirm that SoVAR identified 5 distinct safety violation types that contributed to the crash of Baidu Apollo.CCS Concepts• Software and its engineering → Software testing an...",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11007529/,Automated Testing for Service-Oriented Architecture: Leveraging Large Language Models for Enhanced Service Composition,"['Mahsun Altin', 'Behcet Mutlu', 'Deniz Kilinc', 'Altan Cakir']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11007529,"This article explores the application of Large Language Models (LLMs), including proprietary models such as OpenAI’s ChatGPT 4o and ChatGPT 4o-mini, Anthropic’s Claude 3.5 Sonnet and Claude 3.7 Sonnet, and Google’s Gemini 1.5 Pro, Gemini 2.0 Flash, and Gemini 2.0 Flash-Lite, as well as open-source alternatives including Qwen2.5-14B-Instruct-1M, and commercially accessed models such as DeepSeek R1 and DeepSeek V3, which were tested via APIs despite having open-source variants, to automate validation and verification in Application Programming Interface (API) testing within a Service-Oriented Architecture (SOA). Our system compares internal responses from the Enuygun Web Server against third-party API outputs in both JSON and XML formats, validating critical parameters such as flight prices, baggage allowances, and seat availability. We generated 100 diverse test scenarios across varying complexities (1-4 flight results) by randomly altering request and response parameters. Experimental results show that Google Gemini 2.0 Flash achieved high accuracy (up to 99.98%) with the lowest completion time (85.34 seconds), while Qwen2.5-14B-Instruct-1M exhibited limited capability in processing complex formats. Models such as OpenAI’s ChatGPT and Anthropic’s Claude Sonnet models also demonstrated strong performance in single-flight validation scenarios, making them suitable for low-latency, high-precision tasks. Our findings indicate that some open-source models can offer promising cost-effective alternatives, though performance significantly varies. This integration of LLMs reduced manual workload, improved test scalability, and enabled real-time validation across large-scale datasets. As LLM technologies mature, we anticipate further advances in automation, accuracy, and efficiency in software validation systems.",https://doi.org/10.1109/ACCESS.2025.3571994,
https://ieeexplore.ieee.org/document/10893343/,Enhancing User Story Generation in Agile Software Development Through Open AI and Prompt Engineering,"['Vijayalakshmi Ramasamy', 'Suganya Ramamoorthy', 'Gursimran Singh Walia', 'Eli Kulpinski', 'Aaron Antreassian']",,"This innovative practice full paper explores the use of AI technologies in user story generation. With the emergence of agile software development, generating comprehensive user stories that capture all necessary functionalities and perspectives has become crucial for software development. Every computing program in the United States requires a semester-or year-long senior capstone project, which requires student teams to gather and document technical requirements. Effective user story generation is crucial for successfully implementing software projects. However, user stories written in natural language can be prone to inherent defects such as incompleteness and incorrectness, which may creep in during the downstream development activities like software designs, construction, and testing. One of the challenges faced by software engineering educators is to teach students how to elicit and document requirements, which serve as a blueprint for software development. Advanced AI technologies have increased the popularity of large language models (LLMs) trained on large multimodal datasets. Therefore, utilizing LLM-based techniques can assist educators in helping students discover aspects of user stories that may have been overlooked or missed during the manual analysis of requirements from various stakeholders. The main goal of this research study is to investigate the potential application of OpenAI techniques in software development courses at two academic institutions to enhance software design and development processes, aiming to improve innovation and efficiency in team project-based educational settings. The data used for the study constitute student teams generating user stories by traditional methods (control) vs. student teams using OpenAI agents (treatment) such as gpt-4-turbo for generating user stories. The overarching research questions include: RQ-l) What aspects of user stories generated using OpenAI prompt engineering differ significantly from those gene...",https://doi.org/10.1109/FIE61694.2024.10893343,13-16 October 2024
https://ieeexplore.ieee.org/document/10366647/,"LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities","['Shengcheng Yu', 'Chunrong Fang', 'Yuchen Ling', 'Chentian Wu', 'Zhenyu Chen']",,"This paper investigates the application of large language models (LLM) in the domain of mobile application test script generation. Test script generation is a vital component of software testing, enabling efficient and reliable automation of repetitive test tasks. However, existing generation approaches often encounter limitations, such as difficulties in accurately capturing and reproducing test scripts across diverse devices, platforms, and applications. These challenges arise due to differences in screen sizes, input modalities, platform behaviors, API inconsistencies, and application architectures. Overcoming these limitations is crucial for achieving robust and comprehensive test automation.By leveraging the capabilities of LLMs, we aim to address these challenges and explore its potential as a versatile tool for test automation. We investigate how well LLMs can adapt to diverse devices and systems while accurately capturing and generating test scripts. Additionally, we evaluate its cross-platform generation capabilities by assessing its ability to handle operating system variations and platform-specific behaviors. Furthermore, we explore the application of LLMs in cross-app migration, where it generates test scripts across different applications and software environments based on existing scripts.Throughout the investigation, we analyze its adaptability to various user interfaces, app architectures, and interaction patterns, ensuring accurate script generation and compatibility. The findings of this research contribute to the understanding of LLMs’ capabilities in test automation. Ultimately, this research aims to enhance software testing practices, empowering app developers to achieve higher levels of software quality and development efficiency.",https://doi.org/10.1109/QRS60937.2023.00029,22-26 October 2023
https://ieeexplore.ieee.org/document/10989041/,Integrating LLM-Based Text Generation with Dynamic Context Retrieval for GUI Testing,"['Juyeon Yoon', 'Seah Kim', 'Somin Kim', 'Sukchul Jung', 'Shin Yoo']",,"Automated GUI testing plays a crucial role for smartphone vendors who have to ensure that the widely used mobile apps-that are not essentially developed by the vendors-are compatible with new devices and system updates. While existing testing techniques can automatically generate event sequences to reach different GUI views, inputs such as strings and numbers remain difficult to generate, as their generation often involves semantic understanding of the app functionality. Recently, Large Language Models (LLMs) have been successfully adopted to generate string inputs that are semantically relevant to the test case. This paper evaluates the LLM-based input generation in the industrial context of vendor testing of both in-house and 3rd party mobile apps. We present DROIDFILLER, an LLM based input generation technique that builds upon existing work with more sophisticated prompt engineering and customisable context retrieval. DROIDFILLER is empirically evaluated using a total of 120 textfields collected from a total of 45 apps, including both in-house and 3rd party ones. The results show that DROIDFILLER can outperform both vanilla LLM based input generation as well as the existing resource pool approach. We integrate DROIDFILLER into the existing GUI testing framework used at Samsung, evaluate its performance, and discuss the challenges and considerations for practical adoption of LLM-based input generation in the industry.",https://doi.org/10.1109/ICST62969.2025.10989041,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10844085/,LLM-Guided Crowdsourced Test Report Clustering,"['Ying Li', 'Ye Zhong', 'Lijuan Yang', 'Yanbo Wang', 'Penghua Zhu']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10844085,"This paper proposes a clustering method for crowdsourced test reports based on a large language model to solve the limitations of existing methods in processing repeated reports and utilizing multi-modal information. Existing crowdsourced test report clustering methods have significant shortcomings in handling duplicate reports, ignoring the semantic information of screenshots, and underutilizing the relationship between text and images. The emergence of LLM provides a new way to solve these problems. By integrating the semantic understanding ability of LLM, key information can be extracted from the test report more accurately, and the semantic relationship between screenshots and text descriptions can be used to guide the clustering process, thus improving the accuracy and effectiveness of clustering. The method in this paper uses a pre-trained LLM (such as GPT-4) to encode the text in the test report, and uses a visual model such as CLIP to encode the application screenshots, converting the text descriptions and images into high-dimensional semantic vectors. The cosine similarity is then used to calculate the similarity between the vectors, and semantic binding rules are constructed to guide the clustering process, ensuring that semantically related reports are assigned to the same cluster and semantically different reports are assigned to different clusters. Through experimental verification, this method is significantly superior to traditional methods in several evaluation indicators, demonstrating its great potential in improving the efficiency and quality of crowdsourced test report processing. In the future, this method is expected to be widely used in the process of software testing and maintenance, and further promote technological progress.",https://doi.org/10.1109/ACCESS.2025.3530960,
https://ieeexplore.ieee.org/document/10548767/,Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model,"['Zhe Liu', 'Chunyang Chen', 'Junjie Wang', 'Mengzhuo Chen', 'Boyu Wu', 'Zhilin Tian']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10548767,"Mobile applications have become a ubiquitous part of our daily life, providing users with access to various services and utilities. Text input, as an important interaction channel between users and applications, plays an important role in core functionality such as search queries, authentication, messaging, etc. However, certain special text (e.g., -18 for Font Size) can cause the app to crash, and generating diversified unusual inputs for fully testing the app is highly demanded. Nevertheless, this is also challenging due to the combination of explosion dilemma, high context sensitivity, and complex constraint relations. This paper proposes InputBlaster which leverages the LLM to automatically generate unusual text inputs for mobile app crash detection. It formulates the unusual inputs generation problem as a task of producing a set of test generators, each of which can yield a batch of unusual text inputs under the same mutation rule. In detail, InputBlaster leverages LLM to produce the test generators together with the mutation rules serving as the reasoning chain, and utilizes the in-context learning schema to demonstrate the LLM with examples for boosting the performance. InputBlaster is evaluated on 36 text input widgets with cash bugs involving 31 popular Android apps, and results show that it achieves 78% bug detection rate, with 136% higher than the best baseline. Besides, we integrate it with the automated GUI testing tool and detect 37 unseen crashes in real-world apps.",https://doi.org/10.1145/3597503.3639118,14-20 April 2024
https://ieeexplore.ieee.org/document/10638556/,Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing,"['Robson Santos', 'Italo Santos', 'Cleyton Magalhaes', 'Ronnie de Souza Santos']",,"A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates content, including grammatical sentences, human-like paragraphs, and syntactically code snippets. LLMs can play a pivotal role in soft-ware development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts-specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.",https://doi.org/10.1109/ICST60714.2024.00039,27-31 May 2024
https://ieeexplore.ieee.org/document/10989033/,"Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation","['Azat Abdullin', 'Pouria Derakhshanfar', 'Annibale Panichella']",,"Generating tests automatically is a key and ongoing area of focus in software engineering research. The emergence of Large Language Models (LLMs) has opened up new op-portunities, given their ability to perform a wide spectrum of tasks. However, the effectiveness of LLM -based approaches compared to traditional techniques such as search-based software testing (SBST) and symbolic execution remains uncertain. In this paper, we perform an extensive study of automatic test generation approaches based on three tools: EvoSuite for SBST, Kex for symbolic execution, and TestSpark for LLM-based test generation. We evaluate tools' performance on the GitBug Java dataset and compare them using various execution-based and feature-based metrics. Our results show that while LLM-based test generation is promising, it falls behind traditional methods w.r.t. coverage. However, it significantly outperforms them in mutation scores, suggesting that LLMs provide a deeper semantic understanding of code. LLM-based approach performed worse than SBST and symbolic execution-based approaches w.r.t. fault detection capabilities. Additionally, our feature-based analysis shows that all tools are affected by the complexity and internal dependencies of the class under test (CUT), with LLM-based approaches being especially sensitive to the CUT size.",https://doi.org/10.1109/ICST62969.2025.10989033,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/11042526/,Static Analysis and LLM for Comprehensive Java Unit Test Generation,['Wei Wei'],,"Software testing is crucial in ensuring the reliability and correctness of software applications. However, generating comprehensive test cases manually can be time-consuming and error-prone. This paper introduces SAGEN, a tool designed to automate Java unit test generation by leveraging static analysis of Syntax Trees (AST) and large language models (LLMs). SAGEN identifies literal values and their ranges, generating test cases that improve coverage and quality. In our experiments, SAGEN outperforms traditional test case generation tools such as EvoSuite and Randoop. It demonstrates a 10%
 improvement in code coverage and a 13%
 enhancement in test case quality. Furthermore, SAGEN achieves a compile pass rate of 89.7%
, proving its effectiveness in producing both high-quality and reliable test cases.",https://doi.org/10.1109/AEMCSE65292.2025.11042526,09-11 May 2025
https://ieeexplore.ieee.org/document/10440574/,"Software Testing With Large Language Models: Survey, Landscape, and Vision","['Junjie Wang', 'Yuchao Huang', 'Chunyang Chen', 'Zhe Liu', 'Song Wang', 'Qing Wang']",,"Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.",https://doi.org/10.1109/TSE.2024.3368208,
https://ieeexplore.ieee.org/document/10765048/,Using LLM for Mining and Testing Constraints in API Testing,"['Minh-Hieu Huynh', 'Quoc-Tri Le', 'Tien N. Nguyen', 'Vu Nguyen']",,CCS Concepts• Software and its engineering → Software testing and debugging,,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11029812/,NIODebugger: A Novel Approach to Repair Non-Idempotent-Outcome Tests with LLM-Based Agent,['Kaiyao Ke'],,"Flaky tests, characterized by inconsistent results across repeated executions, present significant challenges in software testing, especially during regression testing. Recently, there has been emerging research interest in non-idempotentoutcome (NIO) flaky tests-tests that pass on the initial run but fail on subsequent executions within the same environment. Despite progress in utilizing Large Language Models (LLMs) to address flaky tests, existing methods have not tackled NIO flaky tests. The limited context window of LLMs restricts their ability to incorporate relevant source code beyond the test method itself, often overlooking crucial information needed to address state pollution, which is the root cause of NIO flakiness. This paper introduces NIODebugger, the first framework to utilize an LLM-based agent to repair flaky tests. NIODebugger features a three-phase design: detection, exploration, and fixing. In the detection phase, dynamic analysis collects stack traces and custom test execution logs from multiple test runs, which helps in understanding accumulative state pollution. During the exploration phase, the LLM-based agent provides instructions for extracting relevant source code associated with test flakiness. In the fixing phase, NIODebugger repairs the tests using the information gathered from the previous phases. NIODebugger can be integrated with multiple LLMs, achieving patching success rates ranging from 11.63% to 58.72%. Its best-performing variant, NIODebugger-GPT-4, successfully generated correct patches for 101 out of 172 previously unknown NIO tests across 20 largescale open-source projects. We submitted pull requests for all generated patches; 58 have been merged, only 1 was rejected, and the remaining 42 are pending. The Java implementation of NIODebugger is provided as a Maven plugin accessible at https://github.com/kaiyaok2/NIOInspector.",https://doi.org/10.1109/ICSE55347.2025.00226,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10429345/,Evaluating LLM’s Code Reading Abilities in Big Data Contexts using Metamorphic Testing,"['Ziyu Li', 'Zhendu Li', 'Kaiming Xiao', 'Xuan Li']",,"With the explosive growth of Big Data, understanding complex data-centering algorithms and software has become essential. Large Language Models (LLMs) especially ChatGPT models have been increasingly deployed in Big Data environments to improve workflow in various tasks, including code reading. The current testing method on LLMs’ code reading ability focuses more on code structural understanding and sentiment understanding, all tests are conducted on different prompts with different assumptions. This paper analyzes current LLM code-reading testing methods and presents an innovative evaluation of Metamorphic Testing to evaluate LLMs’ code-reading abilities. We proposed two new metamorphic relations specified f or code reading challenges and evaluated the ChatGPT-3.5 on its code understanding capabilities. Our study offers insights into LLMs’ capabilities to correctly understand diverse code bases and maintain validity.",https://doi.org/10.1109/BigDIA60676.2023.10429345,15-17 December 2023
https://ieeexplore.ieee.org/document/10962507/,LLM Prompt Engineering for Automated White-Box Integration Test Generation in REST APIs,"['André Mesquita Rincon', 'Auri Marcelo Rizzo Vincenzi', 'João Pascoal Faria']",,"This study explores prompt engineering for automated white-box integration testing of RESTful APIs using Large Language Models (LLMs). Four versions of prompts were designed and tested across three OpenAI models (GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o) to assess their impact on code coverage, token consumption, execution time, and financial cost. The results indicate that different prompt versions, especially with more advanced models, achieved up to 90% coverage, although at higher costs. Additionally, combining test sets from different models increased coverage, reaching 96% in some cases. We also compared the results with EvoMaster, a specialized tool for generating tests for REST APIs, where LLM-generated tests achieved comparable or higher coverage in the benchmark projects. Despite higher execution costs, LLMs demonstrated superior adaptability and flexibility in test generation.",https://doi.org/10.1109/ICSTW64639.2025.10962507,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10988971/,Evaluating Correct-Consistency and Robustness in Code-Generating LLMs,['Shahin Honarvar'],,"Ensuring the reliability of large language models (LLMs) for code generation is crucial for their safe integration into software engineering practices, especially in safety-critical domains. Despite advances in LLM tuning, they frequently generate incorrect code, raising concerns about robustness and trustworthiness. This PhD research introduces Turbulence, a novel benchmark and evaluation framework designed to assess both the correct-consistency and robustness of LLMs through a structured coding question neighbourhood approach. By evaluating model performance across sets of semantically related but non-equivalent coding tasks, Turbulence identifies discontinuities in LLM generalisation, revealing patterns of success and failure that standard correctness evaluations often overlook. Applied to 22 instruction-tuned LLMs across Python coding question neighbourhoods, the benchmark highlights significant variability in correctness, including error patterns persisting even under deterministic settings. Future work will extend the question neighbourhood concept to Capture The Flag (CTF) challenges, enabling a deeper analysis of model reasoning capabilities in progressively complex tasks. This extension has attracted interest from the UK AI Safety Institute, which recognises the frame-work's potential for advancing rigorous evaluation methodologies in the context of safe and trusted AI for software engineering.",https://doi.org/10.1109/ICST62969.2025.10988971,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10989005/,Turbulence: Systematically and Automatically Testing Instruction-Tuned Large Language Models for Code,"['Shahin Honarvar', 'Mark van der Wilk', 'Alastair F. Donaldson']",,"We present a method for systematically evaluating the correctness and robustness of instruction-tuned large language models (LLMs) for code generation via a new benchmark, Turbulence. Turbulence consists of a large set of natural language question templates, each of which is a programming problem, parameterised so that it can be asked in many different forms. Each question template has an associated test oracle that judges whether a code solution returned by an LLM is correct. Thus, from a single question template, it is possible to ask an LLM a neighbourhood of very similar programming questions, and assess the correctness of the result returned for each question. This allows gaps in an LLM's code generation abilities to be identified, including anomalies where the LLM correctly solves almost all questions in a neighbourhood but fails for particular parameter instantiations. We present experiments against five LLMs from OpenAI, Cohere and Meta, each at two temperature configurations. Our findings show that, across the board, Turbulence is able to reveal gaps in LLM reasoning ability. This goes beyond merely highlighting that LLMs sometimes produce wrong code (which is no surprise): by systematically identifying cases where LLMs are able to solve some problems in a neighbourhood but do not manage to generalise to solve the whole neighbourhood, our method is effective at highlighting robustness issues. We present data and examples that shed light on the kinds of mistakes that LLMs make when they return incorrect code results.",https://doi.org/10.1109/ICST62969.2025.10989005,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10675844/,“No Free Lunch” when using Large Language Models to Verify Self-Generated Programs,"['Sol Zilberman', 'H. C. Betty Cheng']",,"Large Language Models (LLMs) have shown great success in a wide range of text-generation tasks including the synthesis of code from natural language descriptions. As LLMbased techniques continue to grow in popularity, especially amongst entry-level developers, LLM-generated code has the potential to be deployed in a diverse set of application domains. While LLMs can generate syntactically correct code output, recent work has shown the presence of nonsensical and faulty reasoning in LLM-generated text. As such, overreliance on LLMs for software generation may potentially result in the deployment of faulty software leading to critical system failures. This study explores the capabilities of a single LLM to generate both software and corresponding test suites from the same initial program descriptions, which can be considered analogous to an individual developer coding and unit testing for a given piece of software. We present an empirical framework and evaluation methodology to assess the usefulness of LLM-generated test cases for verifying programs generated by the same LLM. Our findings indicate that LLMs frequently generate irrelevant tests that suffer from numerous quality concerns.",https://doi.org/10.1109/ICSTW60967.2024.00018,27-31 May 2024
https://ieeexplore.ieee.org/document/11029328/,,[],,,,
https://ieeexplore.ieee.org/document/11011986/,,[],,,,
https://ieeexplore.ieee.org/document/10638568/,,[],,,,
https://ieeexplore.ieee.org/document/11044883/,,[],,,,
https://ieeexplore.ieee.org/document/10967317/,Optimizing LLMs for Code Generation: Which Hyperparameter Settings Yield the Best Results?,"['Chetan Arora', 'Ahnaf Ibn Sayeed', 'Sherlock Licorish', 'Fanyu Wang', 'Christoph Treude']",,"Large Language Models (LLMs), such as GPT models, are increasingly used in software engineering for various tasks, such as code generation, requirements management, and debugging. While automating these tasks has garnered significant attention, a systematic study on the impact of varying hyperparameters on code generation outcomes remains unexplored. This study aims to assess LLMs' code generation performance by exhaustively exploring the impact of various hyperparameters. Hyperparameters for LLMs are adjustable settings that affect the model's behaviour and performance. Specifically, we investigated how changes to the hyperparameters-temperature, top probability (top_p), frequency penalty, and presence penalty-affect code generation outcomes. We systematically adjusted all hyperparameters together, exploring every possible combination by making small increments to each hyperparameter at a time. This exhaustive approach was applied to 13 Python code generation tasks, yielding one of four outcomes for each hyperparameter combination: no output from the LLM, non-executable code, code that fails unit tests, or correct and functional code. We analysed these outcomes for a total of 14,742 generated Python code segments, focusing on correctness, to determine how the hyperparameters influence the LLM to arrive at each outcome. Using correlation coefficient and regression tree analyses, we ascertained which hyperparameters influence which aspect of the LLM. Our results indicate that optimal performance is achieved with a temperature below 0.5, top probability below 0.75, frequency penalty above -1 and below 1.5, and presence penalty above -1. We make our dataset and results available to facilitate replication.",https://doi.org/10.1109/APSEC65559.2024.00039,03-06 December 2024
https://ieeexplore.ieee.org/document/10989036/,Impact of Large Language Models of Code on Fault Localization,"['Suhwan Ji', 'Sanghwa Lee', 'Changsup Lee', 'Yo-Sub Han', 'Hyeonseung Im']",,"Identifying the point of error is imperative in software debugging. Traditional fault localization (FL) techniques rely on executing the program and using the code coverage matrix in tandem with test case results to calculate a suspiciousness score for each method or line. Recently, learning-based FL techniques have harnessed machine learning models to extract meaningful features from the code coverage matrix and improve FL performance. These techniques, however, require compilable source code, existing test cases, and specialized tools for generating the code coverage matrix for each programming language of interest. In this paper, we propose, for the first time, a simple but effective sequence generation approach for fine-tuning large language models of code (LLMCs) for FL tasks. LLMCs have recently received much attention for various software engineering problems. In line with these, we leverage the innate understanding of code that LLMCs have acquired through pre-training on large code corpora. Specifically, we fine-tune 13 representative encoder, encoder-decoder, and decoder-based LLMCs (across 7 different architectures) for FL tasks. Unlike previous approaches, LLM Cs can analyze code sequences that do not compile. Still, they have a limitation on the length of the input data. Therefore, for a fair comparison with existing FL techniques, we extract methods with errors from the project-level benchmark, Defects4J, and analyze them at the line level. Experimental results show that LLMCs fine-tuned with our approach successfully pinpoint error positions in 50.6%, 64.2%, and 72.3% of 1,291 methods in Defects4J for Top-1/3/5 prediction, outperforming the best learning-based state-of-the-art technique by up to 1.35, 1.12, and 1.08 times, respectively. We also conduct an in-depth investigation of key factors that may affect the FL performance of LLMCs. Our findings suggest promising research directions for FL and automated program repair tasks using LLMCs.",https://doi.org/10.1109/ICST62969.2025.10989036,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10669832/,Assured LLM-Based Software Engineering,"['Nadia Alshahwan', 'Mark Harman', 'Inna Harper', 'Alexandru Marginean', 'Shubho Sengupta', 'Eddy Wang']",,"In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code(1)does not regress the properties of the original code ?(2)improves the original in a verifiable and measurable way ?To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of LLM’s propensity to hallucinate. It allows us to generate code using LLMs, independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.",,15-15 April 2024
https://ieeexplore.ieee.org/document/10988926/,AugmenTest: Enhancing Tests with LLM-Driven Oracles,"['Shaker Mahmud Khandaker', 'Fitsum Kifetew', 'Davide Prandi', 'Angelo Susi']",,"Automated test generation is crucial for ensuring the reliability and robustness of software applications while at the same time reducing the effort needed. While significant progress has been made in test generation research, generating valid test oracles still remains an open problem. To address this challenge, we present AugmenTest, an approach leveraging Large Language Models (LLMs) to infer correct test oracles based on available documentation of the software under test. Unlike most existing methods that rely on code, AugmenTest utilizes the semantic capabilities of LLMs to infer the intended behavior of a method from documentation and developer comments, without looking at the code. AugmenTest includes four variants: Simple Prompt, Extended Prompt, RAG with a generic prompt (without the context of class or method under test), and RAG with Simple Prompt, each offering different levels of contextual information to the LLMs. To evaluate our work, we selected 142 Java classes and generated multiple mutants for each. We then generated tests from these mutants, focusing only on tests that passed on the mutant but failed on the original class, to ensure that the tests effectively captured bugs. This resulted in 203 unique tests with distinct bugs, which were then used to evaluate AugmenTest. Results show that in the most conservative scenario, AugmenTest's Extended Prompt consistently outperformed the Simple Prompt, achieving a success rate of 30% for generating correct assertions. In comparison, the state-of-the-art TOGA approach achieved 8.2%. Contrary to our expectations, the RAG-based approaches did not lead to improvements, with performance of 18.2% success rate for the most conservative scenario. Our study demonstrates the potential of LLMs in improving the reliability of automated test generation tools, while also highlighting areas for future enhancement.",https://doi.org/10.1109/ICST62969.2025.10988926,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10988995/,An Analysis of LLM Fine-Tuning and Few-Shot Learning for Flaky Test Detection and Classification,"['Riddhi More', 'Jeremy S. Bradbury']",,"Flaky tests exhibit non-deterministic behavior during execution and they may pass or fail without any changes to the program under test. Detecting and classifying these flaky tests is crucial for maintaining the robustness of automated test suites and ensuring the overall reliability and confidence in the testing. However, flaky test detection and classification is challenging due to the variability in test behavior, which can depend on environmental conditions and subtle code interactions. Large Language Models (LLMs) offer promising approaches to address this challenge, with fine-tuning and few-shot learning (FSL) emerging as viable techniques. With enough data fine-tuning a pre-trained LLM can achieve high accuracy, making it suitable for organizations with more resources. Alternatively, we introduce FlakyXbert, an FSL approach that employs a Siamese network architecture to train efficiently with limited data. To understand the performance and cost differences between these two methods, we compare fine-tuning on larger datasets with FSL in scenarios restricted by smaller datasets. Our evaluation involves two existing flaky test datasets, FlakyCat and IDoFT. Our results suggest that while fine-tuning can achieve high accuracy, FSL provides a cost-effective approach with competitive accuracy, which is especially beneficial for organizations or projects with limited historical data available for training. These findings underscore the viability of both fine-tuning and FSL in flaky test detection and classification with each suited to different organizational needs and resource availability.",https://doi.org/10.1109/ICST62969.2025.10988995,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10989020/,Improving the Readability of Automatically Generated Tests Using Large Language Models,"['Matteo Biagiola', 'Gianluca Ghislotti', 'Paolo Tonella']",,"Search-based test generators are effective at producing unit tests with high coverage. However, such automatically generated tests have no meaningful test and variable names, making them hard to understand and interpret by developers. On the other hand, large language models (LLMs) can generate highly readable test cases, but they are not able to match the effectiveness of search-based generators, in terms of achieved code coverage. In this paper, we propose to combine the effectiveness of search-based generators with the readability of LLM generated tests. Our approach focuses on improving test and variable names produced by search-based tools, while keeping their semantics (i.e., their coverage) unchanged. Our evaluation on nine industrial and open source LLMs show that our readability improvement transformations are overall semantically-preserving and stable across multiple repetitions. Moreover, a human study with ten professional developers, show that our LLM-improved tests are as readable as developer-written tests, regardless of the LLM employed.",https://doi.org/10.1109/ICST62969.2025.10989020,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10704582/,FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair,"['Sakina Fatima', 'Hadi Hemmati', 'Lionel C. Briand']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10704582,"Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting development effort. While machine learning models have been used to predict flakiness and its root causes, there is much less work on providing support to fix the problem. To address this gap, in this paper, we focus on predicting the type of fix that is required to remove flakiness and then repair the test code on that basis. We do this for a subset of flaky tests where the root cause of flakiness is in the test itself and not in the production code. One key idea is to guide the repair process with additional knowledge about the test's flakiness in the form of its predicted fix category. Thus, we first propose a framework that automatically generates labeled datasets for 13 fix categories and trains models to predict the fix category of a flaky test by analyzing the test code only. Our experimental results using code models and few-shot learning show that we can correctly predict most of the fix categories. To show the usefulness of such fix category labels for automatically repairing flakiness, we augment the prompts of GPT 3.5 Turbo, a Large Language Model (LLM), with such extra knowledge to request repair suggestions. The results show that our suggested fix category labels, complemented with in-context learning, significantly enhance the capability of GPT 3.5 Turbo in generating fixes for flaky tests. Based on the execution and analysis of a sample of GPT-repaired flaky tests, we estimate that a large percentage of such repairs, (roughly between 51% and 83%) can be expected to pass. For the failing repaired tests, on average, 16% of the test code needs to be further changed for them to pass.",https://doi.org/10.1109/TSE.2024.3472476,
https://ieeexplore.ieee.org/document/10962503/,From Implemented to Expected Behaviors: Leveraging Regression Oracles for Non-regression Fault Detection using LLMs,"['Stefano Ruberto', 'Judith Perera', 'Gunel Jahangirova', 'Valerio Terragni']",,"Automated test generation tools often produce assertions that reflect implemented behavior, limiting their usage to regression testing. In this paper, we propose LLMProphet, a black-box approach that applies Few-Shot Learning with LLMs, using automatically generated regression tests as context to identify non-regression faults without relying on source code. By employing iterative cross-validation and a leave-one-out strategy, LLMProphet identifies regression assertions that are misaligned with expected behaviors. We outline LLMProphet’s workflow, feasibility, and preliminary findings, demonstrating its potential for LLM-driven fault detection.",https://doi.org/10.1109/ICSTW64639.2025.10962503,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/11029738/,Rug: Turbo Llm for Rust Unit Test Generation,"['Xiang Cheng', 'Fan Sang', 'Yizhuo Zhai', 'Xiaokuan Zhang', 'Taesoo Kim']",,"Unit testing improves software quality by evaluating isolated sections of the program. This approach alleviates the need for comprehensive program-wide testing and confines the potential error scope within the software. However, unit test development is time-consuming, requiring developers to create appropriate test contexts and determine input values to cover different code regions. This problem is particularly pronounced in Rust due to its intricate type system, making traditional unit test generation tools ineffective in Rust projects. Recently, large language models (LLMs) have demonstrated their proficiency in understanding programming language and completing software engineering tasks. However, merely prompting LLMs with a basic prompt like “generate unit test for the following source code” often results in code with compilation errors. In addition, LLM-generated unit tests often have limited test coverage. To bridge this gap and harness the capabilities of LLM, we design and implement RUG, an end-to-end solution to automatically generate the unit test for Rust projects. To help LLM's generated test pass Rust strict compilation checks, RUG designs a semantic-aware bottom-up approach to divide the context construction problem into dependent sub-problems. It solves these sub-problems sequentially using an LLM and merges them to a complete context. To increase test coverage, RUG integrates coverage-guided fuzzing with LLM to prepare fuzzing harnesses. Applying RUG on 17 real-world Rust programs (average 24,937LoC
), we show that RUG can achieve a high code coverage, up to 71.37%
, closely comparable to human effort (73.18%)
. We submitted 113 unit tests generated by RUG covering the new code: 53 of them have been accepted, 17 rejected, and 43 are pending for review.",https://doi.org/10.1109/ICSE55347.2025.00097,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10765025/,Automated Validation of COBOL to Java Transformation,"['Atul Kumar', 'Diptikalyan Saha', 'Toshikai Yasue', 'Kohichi Ono', 'Saravanan Krishnan', 'Sandeep Hans']",,"Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterprise-level code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code. We propose a framework and a tool to help validate the equivalence of COBOL and translated Java. The results can also help repair the code if there are some issues and provide feedback to the AI model to improve. We have developed a symbolic-execution-based test generation to automatically generate unit tests for the source COBOL programs which also mocks the external resource calls. We generate equivalent JUnit test cases with equivalent mocking as COBOL and run them to check semantic equivalence between original and translated programs. Demo Video: https://youtu.be/aqF_agNP-lUCCS CONCEPTS• Software and its engineering → Software testing and debugging.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10556138/,Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs,"['Ziyu Li', 'Donghwan Shin']",,"Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.In this paper, we propose a novel method, called Mutation-based Consistency Testing (MCT), to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. MCT uses different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. MCT then uses these pairs to test the ability of LLMs to detect the inconsistencies correctly.We conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of 164 programming problems written in six programming languages (Python, C++, Java, Go, JavaScript, and Rust). The results show that the LLMs have significant variations in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language. We further explain conditions under which the LLMs result in correct answers using input characteristics (e.g., number of tokens) and investigate to what extent the test results can be improved using one-shot prompts (i.e., providing an additional example). Our MCT method and the case study results provide valuable implications for future research and development ...",,14-15 April 2024
https://ieeexplore.ieee.org/document/10893407/,WIP: ARTful Insights from a Pilot Study on GPT-Based Automatic Code Reviews in Undergraduate Computer Science Programs,"['Aaron S. Crandall', 'Bryan J. Fischer', 'Johannah L. Crandall']",,"This work in progress research paper describes a pilot study using a Large Language Model (LLM) Generative Pre-Trained Transformer-based (GPT) system that generates industry-style code reviews for student feedback on software development projects in Computer Science 2nd, 3rd, and 4th+ semester classes (CS2, CS3, CS4+) at an ABET accredited baccalaureate institution. Code reviews are a valuable, but work-intensive, component of the software engineering process and provide important training to undergraduate students in the form of mentor-peer knowledge transfer. Participants in this study engaged in iterative experiential learning using the Automatic Review Tool (ART), an artificial intelligence tool to support software engineering as an Automatic Static Analysis Tool in the Continuous Integration pipeline alongside software testing harnesses and code style checkers. This pilot study was based on earlier results from a full computer science second semester (CS2) class (n=74)
 to develop an ART-generated code review intervention pilot study with a small group of students in CS2 / 3 and CS4. The project underway uses an experiential learning and iterative feedback process to answer research questions including “Does ART provide accurate and actionable code reviews for students” and “Which levels of students are best prepared to receive and use ART-based code reviews?” During this pilot study, the project used a mixed methods research approach with a series of surveys, code review interventions, and numerical analysis of the code reviews' accuracy. Results showed a reasonable degree of code review accuracy by ART and the students learned code review skills from interaction with the ART-based reviews they received. Ongoing work includes increasing the scale of data collection, using this work to refine and focus the ART-based reviews onto the categories of feedback that students find the most valuable, and building out a more modular tool for wider release in t...",https://doi.org/10.1109/FIE61694.2024.10893407,13-16 October 2024
https://ieeexplore.ieee.org/document/10669636/,Can ChatGPT Repair Non-Order-Dependent Flaky Tests?,"['Yang Chen', 'Reyhaneh Jabbarvand']",,"Regression testing helps developers check whether the latest code changes break software functionality. Flaky tests, which can non-deterministically pass or fail on the same code version, may mislead developers’ concerns, resulting in missing some bugs or spending time pinpointing bugs that do not exist. Existing flakiness detection and mitigation techniques have primarily focused on general order-dependent (OD) and implementation-dependent (ID) flaky tests. There is also a dearth of research on repairing test flakiness, out of which, mostly have focused on repairing OD flaky tests, and a few have explored repairing a subcategory of non-order-dependent (NOD) flaky tests that are caused by asynchronous waits. As a result, there is a demand for devising techniques to reproduce, detect, and repair NOD flaky tests. Large language models (LLMs) have shown great effectiveness in several programming tasks. To explore the potential of LLMs in addressing NOD flakiness, this paper investigates the possibility of using ChatGPT to repair different categories of NOD flaky tests. Our comprehensive study on 118 from the IDoFT dataset shows that ChatGPT, despite as a leading LLM with notable success in multiple code generation tasks, is ineffective in repairing NOD test flakiness, even by following the best practices for prompt crafting. We investigated the reasons behind the failure of using ChatGPT in repairing NOD tests, which provided us valuable insights about the next step to advance the field of NOD test flakiness repair.CCS CONCEPTS• Software and its engineering → Software testing and debugging.",,14-14 April 2024
https://ieeexplore.ieee.org/document/10765010/,HITS: High-coverage LLM-based Unit Test Generation via Method Slicing,"['Zejun Wang', 'Kaibo Liu', 'Ge Li', 'Zhi Jin']",,"Large language models (LLMs) have behaved well in generating unit tests for Java projects. However, the performance for covering the complex focal methods within the projects is poor. Complex methods comprise many conditions and loops, requiring the test cases to be various enough to cover all lines and branches. However, existing test generation methods with LLMs provide the whole method-to-test to the LLM without assistance on input analysis. The LLM has difficulty inferring the test inputs to cover all conditions, resulting in missing lines and branches. To tackle the problem, we propose decomposing the focal methods into slices and asking the LLM to generate test cases slice by slice. Our method simplifies the analysis scope, making it easier for the LLM to cover more lines and branches in each slice. We build a dataset comprising complex focal methods collected from the projects used by existing state-of-the-art approaches. Our experiment results show that our method significantly outperforms current test case generation methods with LLMs and the typical SBST method Evosuite regarding both line and branch coverage scores.CCS CONCEPTS• Software and its engineering → Software testing and debugging; • Computing methodologies → Natural language processing.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10764814/,MR-Adopt: Automatic Deduction of Input Transformation Function for Metamorphic Testing,"['Congying Xu', 'Songqiang Chen', 'Jiarong Wu', 'Shing-Chi Cheung', 'Valerio Terragni', 'Hengcheng Zhu']",,"While a recent study reveals that many developer-written test cases can encode a reusable Metamorphic Relation (MR), over 70% of them directly hard-code the source input and follow-up input in the encoded relation. Such encoded MRs, which do not contain an explicit input transformation to transform the source inputs to corresponding follow-up inputs, cannot be reused with new source inputs to enhance test adequacy.In this paper, we propose MR-Adopt (Automatic Deduction Of inPut Transformation) to automatically deduce the input transformation from the hard-coded source and follow-up inputs, aiming to enable the encoded MRs to be reused with new source inputs. With typically only one pair of source and follow-up inputs available in an MR-encoded test case as the example, we leveraged LLMs to understand the intention of the test case and generate additional examples of source-followup input pairs. This helps to guide the generation of input transformations generalizable to multiple source inputs. Besides, to mitigate the issue that LLMs generate erroneous code, we refine LLM-generated transformations by removing MR-irrelevant code elements with data-flow analysis. Finally, we assess candidate transformations based on encoded output relations and select the best transformation as the result. Evaluation results show that MR-Adopt can generate input transformations applicable to all experimental source inputs for 72.00% of encoded MRs, which is 33.33% more than using vanilla GPT-3.5. By incorporating MR-Adopt-generated input transformations, encoded MR-based test cases can effectively enhance the test adequacy, increasing the line coverage and mutation score by 10.62% and 18.91%, respectively.CCS CONCEPTS• Software and its engineering → Software testing and debugging.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11026897/,Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation,"['Benjamin Steenhoek', 'Michele Tufano', 'Neel Sundaresan', 'Alexey Svyatkovskiy']",,"Software testing is a crucial but time-consuming aspect of software development, and recently, Large Language Models (LLMs) have gained popularity for automated test case generation. However, because LLMs are trained on vast amounts of open-source code, they often generate test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose Reinforcement Learning from Static Quality Metrics (RLSQM), wherein we utilize Reinforcement Learning to generate high-quality unit tests based on static analysis-based quality metrics. First, we analyzed LLM-generated tests and show that LLMs frequently do generate undesirable test smells — up to 37% of the time. Then, we implemented lightweight static analysis-based reward model and trained LLMs using this reward model to optimize for five code quality metrics. Our experimental results demonstrate that the RL-optimized Codex model consistently generated higher-quality test cases than the base LLM, improving quality metrics by up to 23%, and generated nearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all code quality metrics, in spite of training a substantially cheaper Codex model. We provide insights into how reliably utilize RL to improve test generation quality and show that RLSQM is a significant step towards enhancing the overall efficiency and reliability of automated software testing. Our data are available at this link: https://doi.org/10.6084/m9.figshare.25983166.",https://doi.org/10.1109/DeepTest66595.2025.00011,03-03 May 2025
https://ieeexplore.ieee.org/document/10485640/,,[],,,,
https://ieeexplore.ieee.org/document/10172763/,,[],,,,
https://ieeexplore.ieee.org/document/10487209/,,[],,,,
https://ieeexplore.ieee.org/document/10605143/,,[],,,,
https://ieeexplore.ieee.org/document/10837857/,Understanding Defects in Generated Codes by Language Models,"['Ali Mohammadi Esfahani', 'Nafiseh Kahani', 'Samuel A. Ajila']",,"This study investigates the reliability of code generation by Large Language Models (LLMs), focusing on identifying and analyzing defects in the generated code. Despite the advanced capabilities of LLMs in automating code generation, ensuring the accuracy and functionality of the output remains a significant challenge. By using a structured defect classification method to understand their nature and origins this study categorizes and analyzes 367 identified defects from code snippets generated by LLMs, with a significant proportion being functionality and algorithm errors. These error categories indicate key areas where LLMs frequently fail, underscoring the need for targeted improvements. To enhance the accuracy of code generation, this paper implemented five prompt engineering techniques, including Scratchpad Prompting, Program of Thoughts Prompting, Chain-of-Thought Prompting, Chain of Code Prompting, and Structured Chain-of-Thought Prompting. These techniques were applied to refine the input prompts, aiming to reduce ambiguities and improve the models' accuracy rate. The research findings suggest that precise and structured prompting significantly miti-gates common defects, thereby increasing the reliability of LLM-aenerated code.",https://doi.org/10.1109/CASCON62161.2024.10837857,11-13 November 2024
https://ieeexplore.ieee.org/document/11029822/,LLM Based Input Space Partitioning Testing for Library APIs,"['Jiageng Li', 'Zhen Dong', 'Chong Wang', 'Haozhen You', 'Cen Zhang', 'Yang Liu']",,"Automated library APIs testing is difficult as it requires exploring a vast space of parameter inputs that may involve objects with complex data types. Existing search based approaches, with limited knowledge of relations between object states and program branches, often suffer from the low efficiency issue, i.e., tending to generate invalid inputs. Symbolic execution based approaches can effectively identify such relations, but fail to scale to large programs. In this work, we present an LLM-based input space partitioning testing approach, LISP, for library APIs. The approach lever-ages LLMs to understand the code of a library API under test and perform input space partitioning based on its understanding and rich common knowledge. Specifically, we provide the signature and code of the API under test to LLMs, with the expectation of obtaining a text description of each input space partition of the API under test. Then, we generate inputs through employing the generated text description to sample inputs from each partition, ultimately resulting in test suites that systematically explore the program behavior of the API. We evaluate LISP on more than 2,205 library API meth-ods taken from 10 popular open-source Java libraries (e.g., a
pa
che/commons-lang with 2.6k stars, guava with 48.8k stars on GitHub). Our experiment results show that LISP is effective in library API testing. It significantly outperforms state-of-the-art tool EvoSuite in terms of edge coverage. On average, LISP achieves 67.82 % branch coverage, surpassing EvoSuite by 1.21 times. In total, LISP triggers 404 exceptions or errors in the experiments, and discovers 13 previously unknown vulnerabilities during evaluation, which have been assigned CVE IDs.",https://doi.org/10.1109/ICSE55347.2025.00153,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10569684/,Guidelines for Effective Use of ChatGPT in Introductory Programming Education,"['Tlou RAMABU', 'Tumelo MALEBANE']",,"In the wake of highly capable Large Language Models (LLM) like ChatGPT, educational institutions have been navigating how to position themselves within this Artificial Intelligence (AI) era. There have been various suggestions and attempts to exclude ChatGPT in the education sector due to its AI abilities to give accurate responses to students, plagiarism and over reliance on the tool. However, there are also attempts to formally incorporate ChatGPT in education, such as in the field of economics, computer sciences or Mathematics. Without proper guidelines on the uses of ChatGPT in education, these AI technologies can be disruptive, uncontrollable and pose a risk to academic integrity. Based on the synthesis of ideas in the literature and ChatGPT experimental tests, this paper presents relevant guidelines for effective use of ChatGPT in the introductory programming education.",https://doi.org/10.23919/IST-Africa63983.2024.10569684,20-24 May 2024
https://ieeexplore.ieee.org/document/10962485/,Mutation Testing via Iterative Large Language Model-Driven Scientific Debugging,"['Philipp Straubinger', 'Marvin Kreis', 'Stephan Lukasczyk', 'Gordon Fraser']",,"Large Language Models (LLMs) can generate plausible test code. Intuitively they generate this by imitating tests seen in their training data, rather than reasoning about execution semantics. However, such reasoning is important when applying mutation testing, where individual tests need to demonstrate differences in program behavior between a program and specific artificial defects (mutants). In this paper, we evaluate whether Scientific Debugging, which has been shown to help LLMs when debugging, can also help them to generate tests for mutants. In the resulting approach, LLMs form hypotheses about how to kill specific mutants, and then iteratively generate and refine tests until they succeed, all with detailed explanations for each step. We compare this method to three baselines: (1) directly asking the LLM to generate tests, (2) repeatedly querying the LLM when tests fail, and (3) search-based test generation with Pynguin. Our experiments evaluate these methods based on several factors, including mutation score, code coverage, success rate, and the ability to identify equivalent mutants. The results demonstrate that LLMs, although requiring higher computational cost, consistently outperform Pynguin in generating tests with better fault detection and coverage. Importantly, we observe that the iterative refinement of test cases is important for achieving high-quality test suites.",https://doi.org/10.1109/ICSTW64639.2025.10962485,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10942512/,Challenges to Using Large Language Models in Code Generation and Repair,"['Liliana Pasquale', 'Antonino Sabetta', 'Marcelo d’Amorim', 'Péter Hegedűs', 'Mehdi Tarrit Mirakhorli', 'Hamed Okhravi']",,"Large language models (LLMs) hold great promise in solving many challenges arising from software complexity, including the possibility of automating code generation and repair. Although we cannot deny the groundbreaking nature of LLM-based code repair, we must be realistic in positioning current results.",https://doi.org/10.1109/MSEC.2025.3530488,
https://ieeexplore.ieee.org/document/10765018/,Enhancing Software Design and Developer Experience Via LLMs,['Simin Sun'],,"This research explores the transformative potential of generative AI in software development. Generative AI is revolutionizing the field by offering capabilities to automatically generate, refactor, and test code. Through the use of action research, new methods and tools based on generative AI models are studied and developed. The initial focus is on the models’ ability to comprehend high-level design concepts. Subsequently, the research moves into the augmented generation of software artifacts. Finally, organization-specific or task-specific methods are introduced to enhance software developers’ productivity and experience.CCS CONCEPTS• Software and its engineering → Software development process management; Software development methods;",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10449667/,Large Language Models for Software Engineering: Survey and Open Problems,"['Angela Fan', 'Beliz Gokkaya', 'Mark Harman', 'Mitya Lyubarskiy', 'Shubho Sengupta', 'Shin Yoo']",,"This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.",https://doi.org/10.1109/ICSE-FoSE59343.2023.00008,14-20 May 2023
https://ieeexplore.ieee.org/document/10988968/,Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities,"['Avishree Khare', 'Saikat Dutta', 'Ziyang Li', 'Alaia Solko-Breslin', 'Rajeev Alur', 'Mayur Naik']",,"Security vulnerabilities in modern software are prevalent and harmful. While automated vulnerability detection techniques have made promising progress, their scalability and applicability remain challenging. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted recent works to explore if LLMs can be used to detect security vulnerabilities. In this paper, we perform a more comprehensive study by examining a larger and more diverse set of datasets, languages, and LLMs, and qualitatively evaluating detection performance across prompts and vulnerability classes. Concretely, we evaluate the effectiveness of 16 pre-trained LLMs on 5,000 code samples-1,000 randomly selected each from five diverse security datasets. These balanced datasets encompass synthetic and real-world projects in Java and C/C++ and cover 25 distinct vulnerability classes. Our results show that LLMs across all scales and families show modest effectiveness in end-to-end reasoning about vul-nerabilities, obtaining an average accuracy of 62.8% and F1 score of 0.71 across all datasets. LLMs are significantly better at detecting vulnerabilities that typically only need intra-procedural reasoning, such as OS Command Injection and NULL Pointer Dereference. Moreover, LLMs report higher accuracies on these vulnerabilities than popular static analysis tools, such as CodeQL. We find that advanced prompting strategies that involve step-by-step analysis significantly improve performance of LLMs on real-world datasets in terms of F1 score (by up to 0.18 on average). Interestingly, we observe that LLMs show promising abilities at performing parts of the analysis correctly, such as identifying vulnerability-related specifications (e.g., sources and sinks) and leveraging natural language information to understand code behavior (e.g., to check if code is sanitized). We believe our insights can motivate future work on LLM-augmented vulnerability detect...",https://doi.org/10.1109/ICST62969.2025.10988968,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10705238/,A Framework for Security Testing of Large Language Models,['Kiril Traykov'],,"The purpose of this paper is to present a framework for testing of large language models (LLMs) for security vulnerabilities before their implementation to production environment. The paper discusses the latest developments in the Artificial Intelligence (AI) and Generative Artificial Intelligence (Generative AI) adoption in the industry, the expectations for further accelerated adoption and evolving regulatory landscape. An overview of the most significant risks and vulnerabilities of the LLMs such as prompt injection and denial of service have been presented with their mitigation strategies. A testing approach and testing framework have been developed and implemented with simple chatbot app. The test scenarios have been executed and results have been obtained for three open-source LLMs from which two pass the test and one failed and demonstrated the application of the proposed testing framework. Source code of the application and test script are published open source for reproducibility and reuse. In conclusion the with the confirmation of the results the limitation of the reliance on semantic similarity for the responses of the models was discussed together with three areas for further development: expanding the test scenarios to significant risks, integration with popular cloud continuous development platforms and integrating blockchain for transparent publication of the final test results.",https://doi.org/10.1109/IS61756.2024.10705238,29-31 August 2024
https://ieeexplore.ieee.org/document/11024524/,Predicting the Root Cause of Flaky Tests Based on Test Smells,"['Jing Wang', 'Weixi Zhang', 'Weiwei Wang', 'Ruilian Zhao', 'Ying Shang']",,"Flaky tests refer to test cases that exhibit inconsistent behaviors across multiple executions, potentially passing or failing unpredictably. They are frequently associated with suboptimal design practices that testers may utilize when crafting test cases, which undermine the quality of software testing. So, identifying the root causes of flaky tests is crucial for fixing them. Currently, inspired by the success of the Large Language Models (LLMs), researchers leverage the pre-trained language model to embed flaky test code as vectors and predict its root cause category based on vector similarity measures. However, such code embeddings generated by LLM mainly focus on capturing general semantic features but lack sufficient comprehension of the behavioral patterns involved in test scenarios, resulting in poor root cause identification. Test smells, which reflect poor coding practices or habits when writing test cases, provide complementary information in the root cause identification of test flakiness. Therefore, this paper proposes a root cause identification method for flaky tests based on test smells. Test smells are used to abstract and express behavioral patterns of test codes, and general semantic features extracted by vector embeddings to enhance the feature representation of flaky tests. Furthermore, to capture the complex nonlinear relationships between test smell features and code embeddings, a Feedforward Neural Network is constructed to categorize the root cause of test flakiness. To validate the effectiveness of our method, we performed evaluations on a dataset consisting of 451 Java flaky test cases. The experimental results indicate that our method achieves an F1-score of 80%, which is 7% higher than that of the baseline model that does not incorporate test smells.",https://doi.org/10.1109/ICSR66718.2025.00015,27-28 April 2025
https://ieeexplore.ieee.org/document/10731701/,Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation,"['Gavin Black', 'Varghese Mathew Vaidyan', 'Gurcan Comert']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10731701,"Fuzzing is a crucial technique for detecting software defects by dynamically generating and testing program inputs. This study introduces a framework designed to assess the application of Large Language Models (LLMs) to automate the generation of effective seed inputs for fuzzing, particularly in the Python programming environment where traditional approaches are less effective. Utilizing the Atheris fuzzing framework, we created over 38,000 seed inputs from LLMs targeted at 50 Python functions from widely-used libraries. Our findings underscore the critical role of LLM selection in seed effectiveness. In certain cases, seeds generated by LLMs rivaled or surpassed traditional fuzzing campaigns, with a corpus of fewer than 100 LLM-generated entries outperforming over 100,000 conventionally produced inputs. These seeds significantly improved code coverage and instruction count during fuzzing sessions, illustrating the efficacy of our framework in facilitating an automated, scalable approach to evaluating LLM effectiveness. The results, validated through linear regression analysis, demonstrate that selecting the appropriate LLM based on its training and capabilities is essential for optimizing fuzzing efficiency and facilitates the testing of future LLM versions.",https://doi.org/10.1109/ACCESS.2024.3484947,
https://ieeexplore.ieee.org/document/10638554/,On the Coupling between Vulnerabilities and LLM-Generated Mutants: A Study on Vul4J Dataset,"['Aayush Garg', 'Renzo Degiovanni', 'Mike Papadakis', 'Yves Le Traon']",,"With the release of powerful language models trained on large code corpus (e.g., CodeBERT, trained on 6.4 million programs), a new family of mutation testing tools has arisen that promises to generate more “natural” mutants, where the mutated code aims at following the implicit rules and coding conventions produced by programmers. In this paper, we empirically study the observable behavior of CodeBERT-generated mutants and to what extent are these coupled with software vulnerabilities. To do so, we carefully analyze 45 reproducible vulnerabilities from the Vul4J dataset to determine whether the mutants and vulnerabilities fail the same tests and whether the failures are for the same reasons or not. Hence, we define different degrees of vulnerability-coupling classes. Strongly coupled mutants fail the same tests for the same reasons as the vulnerabilities, while test coupled mutants fail the same tests but for some different reason as the vulnerabilities. Partial coupling classes are also considered. Overall, CodeBERT-generated mutants strongly coupled with 32 out of these 45 vulnerabilities (i.e. The mutants fail on the same tests for the same reasons), while another 7 vulnerabilities are test-coupled by CodeBERT mutants (i.e. The mutants fail on the same tests but not for the same reasons). Interestingly, CodeBERT mutants are diverse enough to couple vulnerabilities from 14 out of the 15 types of vulnerabilities explored, i.e., CWEs (Common Weakness Enumeration). Finally, we observe that strongly coupled mutants are scarce (1.17 % of the killable mutants), test coupled mutants represent 7.2 %, and 64.9 % of the killable mutants are not coupled with the vulnerabilities.",https://doi.org/10.1109/ICST60714.2024.00035,27-31 May 2024
https://ieeexplore.ieee.org/document/10223873/,Object Oriented BDD and Executable Human-Language Module Specification,"['Eric Lee', 'Jiayu Gong', 'Qinghong Cao']",,"This paper presents an approach to software development which uses a generative AI Model as compiler to translate human language requirements into high-level programming language. We propose an executable human-language module specification and a tool to support it, which has been used successfully for human-language UI test automation. We anticipate further development of this approach to enable complex software to be programmed in human language, allowing for more intuitive and efficient software development.",https://doi.org/10.1109/SNPD-Winter57765.2023.10223873,05-07 July 2023
https://ieeexplore.ieee.org/document/10989022/,Addressing Data Leakage in HumanEval Using Combinatorial Test Design,"['Jeremy S. Bradbury', 'Riddhi More']",,"The use of large language models (LLMs) is widespread across many domains, including Software Engineering, where they have been used to automate tasks such as program generation and test classification. As LLM-based methods continue to evolve, it is important that we define clear and robust methods that fairly evaluate performance. Benchmarks are a common approach to assess LLMs with respect to their ability to solve problem-specific tasks as well as assess different versions of an LLM to solve tasks over time. For example, the HumanEval benchmark is composed of 164 hand-crafted tasks and has become an important tool in assessing LLM-based program generation. However, a major barrier to a fair evaluation of LLMs using benchmarks like HumanEval is data contamination resulting from data leakage of benchmark tasks and solutions into the training data set. This barrier is compounded by the black-box nature of LLM training data which makes it difficult to even know if data leakage has occurred. To address the data leakage problem, we propose a new benchmark construction method where a benchmark is composed of template tasks that can be instantiated into new concrete tasks using combinatorial test design. Concrete tasks for the same template task must be different enough that data leakage has minimal impact and similar enough that the tasks are interchangeable with respect to performance evaluation. To assess our benchmark construction method, we propose HumanEval_T, an alternative benchmark to HumanEval that was constructed using template tasks and combinatorial test design.",https://doi.org/10.1109/ICST62969.2025.10989022,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10172800/,CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models,"['Caroline Lemieux', 'Jeevana Priya Inala', 'Shuvendu K. Lahiri', 'Siddhartha Sen']",,"Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.",https://doi.org/10.1109/ICSE48619.2023.00085,14-20 May 2023
https://ieeexplore.ieee.org/document/11029729/,Clozemaster: Fuzzing Rust Compiler by Harnessing Llms for Infilling Masked Real Programs,"['Hongyan Gao', 'Yibiao Yang', 'Maolin Sun', 'Jiangchang Wu', 'Yuming Zhou', 'Baowen Xu']",,"Ensuring the reliability of the Rust compiler is of paramount importance, given increasing adoption of Rust for critical systems development, due to its emphasis on memory and thread safety. However, generating valid test programs for the Rust compiler poses significant challenges, given Rust's complex syntax and strict requirements. With the growing popularity of large language models (LLMs), much research in software testing has explored using LLMs to generate test cases. Still, directly using LLMs to generate Rust programs often results in a large number of invalid test cases. Existing studies have indicated that test cases triggering historical compiler bugs can assist in software testing. Our investigation into Rust compiler bug issues supports this observation. Inspired by existing work and our empirical research, we introduce a bracket-based masking and filling strategy called clozeMask. The clozeMask strategy involves extracting test code from historical issue reports, identifying and masking code snippets with specific structures, and using an LLM to fill in the masked portions for synthesizing new test programs. This approach harnesses the generative capabilities of LLMs while retaining the ability to trigger Rust compiler bugs. It enables comprehensive testing of the compiler's behavior, particularly exploring edge cases. We implemented our approach as a prototype ClozeMaster. ClozeMaster has identified 27 confirmed bugs for rustc and mrustc, of which 10 have been fixed by developers. Furthermore, our experimental results indicate that ClozeMaster outperforms existing fuzzers in terms of code coverage and effectiveness.",https://doi.org/10.1109/ICSE55347.2025.00175,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10429996/,Exploring the Capability of ChatGPT in Test Generation,"['Gaolei Yi', 'Zizhao Chen', 'Zhenyu Chen', 'W. Eric Wong', 'Nicholas Chau']",,"The design of test is a crucial step in the field of software testing. The quality of test significantly impacts the effectiveness of software testing, with well-designed test cases improving the efficiency of bug detection. However, manual test case design and writing are often considered time-consuming and labor-intensive. With the emergence of large language models (LLMs), especially ChatGPT, the potential of LLMs in the field of test generation has become evident. Pretrained LLMs can learn and understand code in various programming languages and design test cases using multiple testing frameworks. In this paper, we used ChatGPT to generate tests for some tested projects. Through experiments, we found that ChatGPT has some gaps compared to traditional test generation tools, but its performance is closer to manual testing. However, the tests generated by ChatGPT exhibit higher readability. We believe that ChatGPT is better suited to serve as a manual testing assistant, helping understand the tested code and providing testing ideas.",https://doi.org/10.1109/QRS-C60940.2023.00013,22-26 October 2023
https://ieeexplore.ieee.org/document/10989026/,Accessible Smart Contracts Verification: Synthesizing Formal Models with Tamed LLMs,"['Jan Corazza', 'Ivan Gavran', 'Gabriela Moreira', 'Daniel Neider']",,"When blockchain systems are said to be trustless, what this really means is that all the trust is put into software. Thus, there are strong incentives to ensure blockchain software is correct–vulnerabilities here cost millions and break businesses. One of the most powerful ways of establishing software correctness is by using formal methods. Approaches based on formal methods, however, induce a significant overhead in terms of time and expertise required to successfully employ them. Our work addresses this critical disadvantage by automating the creation of a formal model–a mathematical abstraction of the software system–which is often a core task when employing formal methods. We perform model synthesis in three phases: we first transpile the code into model stubs; then we “fill in the blanks” using a large language model (LLM); finally, we iteratively repair the generated model, on both syntactical and semantical level. In this way, we significantly reduce the amount of time necessary to create formal models and increase accessibility of valuable software verification methods that rely on them. The practical context of our work was reducing the time-to-value of using formal models for correctness audits of smart contracts.",https://doi.org/10.1109/ICST62969.2025.10989026,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10992399/,Can Large Language Models Discover Metamorphic Relations? A Large-Scale Empirical Study,"['Jiaming Zhang', 'Chang-ai Sun', 'Huai Liu', 'Sijin Dong']",,"Software testing is a mainstream approach for software quality assurance. One fundamental challenge for testing is that in many practical situations, it is very difficult to verify the correctness of test results given inputs for Software Under Test (SUT), which is known as the oracle problem. Metamorphic Testing (MT) is a software testing technique that can effectively alleviate the oracle problem. The core component of MT is a set of Metamorphic Relations (MRs), which are basically the necessary properties of SUT, represented in the form of relationship among multiple inputs and their corresponding expected outputs. Different methods have been proposed to support the systematic MR identification. However, most of them still rely heavily on test engineers' understanding of the SUT and involve massive manual work. Although a few preliminary studies have shown LLMs' viability in generating MRs, there does not exist a thorough and in-depth investigation on their capability in MR identification. We are thus motivated to conduct a comprehensive and large-scale empirical study to systematically evaluate the performance of LLMs in identifying appropriate MRs for a wide variety of software systems. This study makes use of 37 SUTs collected from previous MT studies. Prompts are constructed for two LLMs, gpt-3.5-turbo-1106 and gpt-4-1106-preview, to perform the MR identification for each SUT. The empirical results demonstrate that both LLMs can generate a large amount of MR candidates (MRCs). Among them, 29.86% and 43.79% of all MRCs are identified as the MRs valid for the corresponding SUT, respectively. In addition, 24.59% and 38.63% of all MRCs are MRs that had never been identified in previous studies. Our study not only reinforces LLM-based MR identification as a promising research direction for MT, but also provides some practical guidelines for how to further improve LLMs' performance in generating good MRs.",https://doi.org/10.1109/SANER64311.2025.00011,04-07 March 2025
https://ieeexplore.ieee.org/document/10765033/,On the Evaluation of Large Language Models in Unit Test Generation,"['Lin Yang', 'Chen Yang', 'Shutao Gao', 'Weijing Wang', 'Bo Wang', 'Qihao Zhu']",,"Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs’ capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.CCS CONCEPTS • Software and its engineering → Software testing and debugging.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10638609/,Quantizing Large-Language Models for Predicting Flaky Tests,"['Shanto Rahman', 'Abdelrahman Baz', 'Sasa Misailovic', 'August Shi']",,"A major challenge in regression testing practice is the presence of flaky tests, which non-deterministically pass or fail when run on the same code. Previous research identified multiple categories of flaky tests. Prior research has also de-veloped techniques for automatically detecting which tests are flaky or categorizing flaky tests, but these techniques generally involve repeatedly rerunning tests in various ways, making them costly to use. Although several recent approaches have utilized large-language models (LLMs) to predict which tests are flaky or predict flaky-test categories without needing to rerun tests, they are costly to use due to relying on a large neural network to perform feature extraction and prediction. We propose FlakyQ to improve the effectiveness of LLM-based flaky-test prediction by quantizing LLM's weights. The quantized LLM can extract features from test code more efficiently. To make up for loss in prediction performance due to quantization, we further train a traditional ML classifier (e.g., a random forest) to learn from the quantized LLM-extracted features and do the same prediction. The final model has similar prediction performance while running faster than the non-quantized LLM. Our evaluation finds that FlakyQ classifiers consistently improves prediction time over the non-quantized LLM classifier, saving 25.4% in prediction time over all tests, along with a 48.4 % reduction in memory usage. Furthermore, prediction performance is equal or better than the non-quantized LLM classifier.",https://doi.org/10.1109/ICST60714.2024.00018,27-31 May 2024
https://ieeexplore.ieee.org/document/11031392/,Defining a New Metric for Detecting Bias in Software Systems: Towards Ethical Software Engineering,"['Ahmed Abdelraheem', 'Malak Elbanna', 'Mohamed Elnaggar', 'Doaa Shawky']",,"Bias in software systems poses ethical concerns that may lead to unintended discrimination, particularly when sensitive variables (e.g., gender, ethnicity) influence decision-making processes. While bias detection in machine learning models has been extensively studied, traditional software systems remain largely unexplored. However, implicit bias can manifest in conditional logic, user role definitions, or static decision trees, which directly influences user experience and access equity in real-world applications (e.g., government services or healthcare platforms). This paper presents a novel static analysis methodology for detecting and quantifying bias in general-purpose software systems. The proposed approach leverages static backward slicing to isolate relevant code, builds a Control Flow Graph (CFG) to trace sensitive variables in conditional branches, and utilizes a Control Dependency Graph (CDG) to assess bias propagation in a weighted-analysis format. Two bias metrics are inferred from the process: Bias Impact Score (BIS), which quantifies how the detected bias influences code execution, and the Bias Severity Score (BSS), which measures the broader implications of the impact. A final composite metric is introduced combining static code structure and ML-based sensitivity analysis. The proposed methodology is evaluated using an LLM-generated dataset of 3920 code snippets from prior research, covering different demographic bias directions such as ethnicity, gender, religion, and occupation. Results, achieving 94.6% accuracy in bias detection, show that the proposed methodology effectively identifies and quantifies bias, allowing developers to mitigate ethical risks early in the software development lifecycle. This research paper provides a foundation for ethical software engineering by offering a systematic and scalable approach to bias detection not only limited to AI-driven models. The methodology currently focuses on Python code and may require adaptation ...",https://doi.org/10.1109/ICEENG64546.2025.11031392,12-15 May 2025
https://ieeexplore.ieee.org/document/10647083/,Neural Fault Injection: Generating Software Faults from Natural Language,"['Domenico Cotroneo', 'Pietro Liguori']",,"Traditional software fault injection methods, while foundational, face limitations in adequately representing real-world faults, offering customization, and requiring significant manual effort and expertise. This paper introduces a novel methodology that harnesses the capabilities of Large Language Models (LLMs) augmented with Reinforcement Learning from Human Feedback (RLHF) to overcome these challenges. The usage of RLHF emphasizes an iterative refinement process, allowing testers to provide feedback on generated faults, which is then used to enhance the LLM’s fault generation capabilities, ensuring the generation of fault scenarios that closely mirror actual operational risks. This innovative methodology aims to significantly reduce the manual effort involved in crafting fault scenarios as it allows testers to focus on higher-level testing strategies, hence paving the way to new possibilities for enhancing the dependability of software systems.",https://doi.org/10.1109/DSN-S60304.2024.00016,24-27 June 2024
https://ieeexplore.ieee.org/document/10962528/,Combinatorial Test Design Model Creation using Large Language Models,"['Deborah Ann Furman', 'Eitan Farchi', 'Michael Edward Gildein', 'Andrew C. M. Hicks', 'Ryan Thomas Rawlins']",,"In this paper, we report on our initial experience in using Large Language Models (LLMs), which continue to impact a growing multitude of domains, further expanding machine learning applicability. One possible use case is to apply LLMs as a tool to help drive more optimized test coverage via assisting to generate Combinatorial Test Design (CTD) models. This can lower the entry barrier for new CTD practitioners by requiring less subject matter expertise to generate a basic CTD model. In this paper we report on our initial experience in using LLMs to generate a base CTD model and analyze the usefulness of the approach. In common testing scenarios, the LLMs easily provide the necessary attributes and values that are needed to define the CTD model. Prompting the LLM for additional use cases is useful in highlighting possible interactions and determining constraints of the attributes identified in the first stage. Combining the two stages together facilitates the creation of base CTD models.",https://doi.org/10.1109/ICSTW64639.2025.10962528,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/11029962/,SpecGen: Automated Generation of Formal Program Specifications via Large Language Models,"['Lezhi Ma', 'Shangqing Liu', 'Yi Li', 'Xiaofei Xie', 'Lei Bu']",,"In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on...",https://doi.org/10.1109/ICSE55347.2025.00129,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/11029748/,TOGLL: Correct and Strong Test Oracle Generation with LLMS,"['Soneya Binta Hossain', 'Matthew B. Dwyer']",,"Test oracles play a crucial role in software testing, enabling effective bug detection. Despite initial promise, neural methods for automated test oracle generation often result in a large number of false positives and weaker test oracles. While LLMs have shown impressive effectiveness in various software engineering tasks, including code generation, test case creation, and bug fixing, there remains a notable absence of large-scale studies exploring their effectiveness in test oracle generation. The question of whether LLMs can address the challenges in effective oracle generation is both compelling and requires thorough investigation. In this research, we present the first comprehensive study to investigate the capabilities of LLMs in generating correct, diverse, and strong test oracles capable of effectively identifying a large number of unique bugs. To this end, we fine-tuned seven code LLMs using six distinct prompts on a large dataset consisting of 110 Java projects. Utilizing the most effective finetuned LLM and prompt pair, we introduce TOGLL, a novel LLM-based method for test oracle generation. To investigate the generalizability of TOGLL, we conduct studies on 25 unseen large-scale Java projects. Besides assessing the correctness, we also assess the diversity and strength of the generated oracles. We compare the results against EvoSuite and the state-of-the-art neural method, TOGA. Our findings reveal that TOGLL can produce 3.8 times more correct assertion oracles and 4.9 times more exception oracles than TOGA. Regarding bug detection effectiveness, TOGLL can detect 1,023 unique mutants that EvoSuite cannot, which is ten times more than what TOGA can detect. Additionally, TOGLL significantly outperforms TOGA in detecting real bugs from the Defects4J dataset.",https://doi.org/10.1109/ICSE55347.2025.00098,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10556390/,Using GitHub Copilot for Test Generation in Python: An Empirical Study,"['Khalid El Haji', 'Carolin Brandt', 'Andy Zaidman']",,"Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot’s test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations. Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28%
 of the tests generated by Copilot are passing tests; 54.72%
 of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45%
 of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.",,15-16 April 2024
https://ieeexplore.ieee.org/document/10647136/,What’s in a Display Name? An Empirical Study on the Use of Display Names in Open-Source JUnit Tests,"['Yining Qiao', 'José Miguel Rojas']",,"Readability is an important aspect of any production or test code artefact. During development, testing and maintenance, the readability of a unit test can be a key contributor to its usefulness for a range of tasks, including refactoring, debugging, and test augmentation. Several strategies have been proposed both in academia and in industry to build readability into unit tests, e.g., test code summarisation and test naming conventions. Display names constitute an industry-led effort to incorporate natural language descriptions into unit tests. In this paper, we investigate the use of display names in a large dataset of open-source Java projects. Our study reveals that despite being a stable feature of the JUnit framework for at least five years, display names are not widely used in open-source projects yet. We analyse existing display names in terms of length, language and grammatical structure, explore the use of a large language model to generate display names similar to those open-source developers write, and develop a taxonomy of display name smells aimed at fostering a more cohesive and coherent use of the feature by developers.",,20-20 April 2024
https://ieeexplore.ieee.org/document/11006641/,From LLMs to Randomness: Analyzing Program Input Efficacy With Resource and Language Metrics,"['Gavin Black', 'Eric Yocam', 'Varghese Mathew Vaidyan', 'Gurcan Comert', 'Yong Wang']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11006641,"Security-focused program testing typically focuses on crash detection and code coverage while overlooking additional system behaviors that can impact program confidentiality and availability. To address this gap, we propose a statistical framework that combines embedding-based anomaly detection, resource usage metrics, and resource-state distance measures to systematically profile software behaviors beyond traditional coverage-based methods. Leveraging over 5 million labeled samples from 50 Python programs, we evaluate how these independent scoring terms distinguish among different sources of input, including Large Language Model (LLM)-generated inputs, and demonstrate how standard statistical tests (e.g., Kolmogorov—Smirnov and Kendall’s τ
 ) confirm their effectiveness. Our findings show that LLM-generated samples can trigger diverse behaviors but are often less effective at exploring resource usage dynamics (CPU, memory) compared with conventional fuzzing. However, combining LLM outputs with existing techniques broadens behavior coverage and reveals commonalities between commercial LLM outputs. We provide open-source tools for this evaluation framework, demonstrating the potential to refine software testing by integrating behavior metrics into security-testing workflows.",https://doi.org/10.1109/ACCESS.2025.3571205,
https://ieeexplore.ieee.org/document/10753556/,Large Language Model-Based Optimization for System-Level Test Program Generation,"['Denis Schwachhofer', 'Peter Domanski', 'Steffen Becker', 'Stefan Wagner', 'Matthias Sauer', 'Dirk Pflüger']",,"System-Level Test (SLT) is essential for testing integrated circuits, focusing on functional and non-functional properties of the Device under Test (DUT). Traditionally, test engineers manually create tests with commercial software to simulate the DUT's end-user environment. This process is both time-consuming and offers limited control over non-functional properties. This paper proposes Large Language Models (LLMs) enhanced by Structural Chain of Thought (SCoT) prompting, a temperature schedule, and a pool of previously generated snippets to generate high-quality code snippets for SLT. We repeatedly query the LLM for a better snippet using previously generated snippets as examples, thus creating an iterative optimization loop. This approach can automatically generate snippets for SLT that target specific non-functional properties, reducing time and effort. Our findings show that this approach improves the quality of the generated snippets compared to unstructured prompts containing only a task description.",https://doi.org/10.1109/DFT63277.2024.10753556,08-10 October 2024
https://ieeexplore.ieee.org/document/10962997/,,[],,,,
https://ieeexplore.ieee.org/document/10599585/,,[],,,,
https://ieeexplore.ieee.org/document/10765050/,,[],,,,
https://ieeexplore.ieee.org/document/10664350/,,[],,,,
https://ieeexplore.ieee.org/document/11029879/,A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs,"['Myeongsoo Kim', 'Tyler Stennett', 'Saurabh Sinha', 'Alessandro Orso']",,"As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API documentation languages, such as the OpenAPI Specification, has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in fault detection. To address these limitations, we present AutoRestTest, the first black-box tool to adopt a dependency-embedded multi-agent approach for REST API testing that integrates multi-agent reinforcement learning (MARL) with a semantic property dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents-API, dependency, parameter, and value agents-collaborate to optimize API exploration. LLMs handle domain-specific value generation, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest on 12 real-world REST services shows that it outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which generates realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to trigger an internal server error in the Spotify service. Our ablation study illustrates that each component of AutoRestTest-the SPDG, the LLM, and the agent-learning mechanism-contributes to its overall effectiveness.",https://doi.org/10.1109/ICSE55347.2025.00179,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10628487/,Requirements are All You Need: From Requirements to Code with LLMs,['Bingyang Wei'],,"The pervasive use of textual formats in the documentation of software requirements presents a great opportunity for applying large language models (LLMs) to software engineering tasks. High-quality software requirements not only enhance the manual software development process but also position organizations to fully harness the potential of the emerging LLMs technology. This paper introduces a tailored LLM for automating the generation of code snippets from well-structured requirements documents. This LLM is augmented with knowledge, heuristics, and instructions that are pertinent to the software development process, requirements analysis, object-oriented design, and test-driven development, effectively emulating the expertise of a seasoned software engineer. We introduce a “Progressive Prompting” method that allows software engineers to engage with this LLM in a stepwise manner. Through this approach, the LLM incrementally tackles software development tasks by interpreting the provided requirements to extract functional requirements, using these to create object-oriented models, and subsequently generating unit tests and code based on the object-oriented designs. We demonstrate the LLM's proficiency in comprehending intricate user requirements and producing robust design and code solutions through a case study focused on the development of a web project. This study underscores the potential of integrating LLMs into the software development workflow to significantly enhance both efficiency and quality. The tailored LLM is available at https://chat.openai.com/g/g-bahoiKzkB-software-engineer-gpt.",https://doi.org/10.1109/RE59067.2024.00049,24-28 June 2024
https://ieeexplore.ieee.org/document/10923987/,AI-Powered Multi-Agent Framework for Automated Unit Test Case Generation: Enhancing Software Quality through LLM’s,"['Anusha Garlapati', 'M N V Satya Sai Muni Parmesh', 'Savitha', 'Jaisri S']",,"Recent years have witnessed an enormous rise in the design, repair and the enhancement of software automation tests. The reliability of program’s unit testing has major impact on its overall performance. The anticipated influence of Artificial Intelligence advancements on test automation methodologies are significant. Many studies on automated testing implicitly assume that the test results are deterministic, means that similar tests faults remain same. The precision of software is largely ensured by unit testing. But writing unit tests manually is a time-consuming process, which leads us to drive into ""Automation Analysis"". Recent years comprised the application of Large Language Models (LLM’s) in numerous fields related to software development, especially the automated creation of unit testing.However, these frameworks require more instructions, or few shot learnings on sample tests that already exist. This research provides a comprehensive empirical assessment of the efficiency of LLM’s for automating unit testing production, with no need for further manual analysis. The method we employ is put into practice for test cases, an adaptable Agents and LLM-based testing framework that evaluates test cases generated, by reviewing and re-writing them in different phases. Evaluation of this test cases was done by using mistral-large LLM Model. The analysis results that developed acquired an overall coverage of 100% for code given. Finally, to enhance the typical evaluation, this research suggests and concludes that LLMs, can be successfully incorporated into present practices, through adaptative instructions and improvements.",https://doi.org/10.1109/GCAT62922.2024.10923987,04-06 October 2024
https://ieeexplore.ieee.org/document/10817956/,LLM - TG: Towards Automated Test Case Generation for Processors Using Large Language Models,"['Yifei Deng', 'Renzhi Chen', 'Chao Xiao', 'Zhijie Yang', 'Yuanfeng Luo', 'Jingyue Zhao']",,"Design verification (DV) has existed for decades and is crucial for identifying potential bugs before chip tape- out. Hand-crafting test cases is time-consuming and error-prone, even for experienced verification engineers. Prior work has attempted to lighten this burden by rule-guided random test case generation. However, this approach does not eliminate the manual effort required to write rules that describe detailed hardware behavior. Motivated by advances in large language models (LLMs), we explore their potential to capture register transfer level (RTL) behavior and construct prompts for test case generation based on RTL behavior. First, we introduce a prompt framework, LLM - Driven Test Generation (LLM - TG), to generate test cases, thereby enhancing LLMs' test generation capabilities. Additionally, we provide an open-source prompt library that offers a set of standardized prompts for processor verification, aiming to improve test generation efficiency. Lastly, we use an LLM to verify a 12-stage, multi-issue, out-of-order RV64GC processor, achieving at least an 8.34 % increase in block coverage and at least a 5.8 % increase in expression coverage compared to the state-of-the-art (SOTA) methods, LLM4DV and RISCV- DV. The prompt library is available at https://github.com/LLM-TGIPrompt_Library.",https://doi.org/10.1109/ICCD63220.2024.00066,18-20 November 2024
https://ieeexplore.ieee.org/document/11024370/,Trustworthiness of Large Language Models for Code,['Dipin Khati'],,"In recent years, Large Language Models for code (LLMc) have transformed the landscape of software engineering (SE), demonstrating significant efficacy in tasks such as code completion, summarization, review, tracing, translation, test case generation, clone detection, and bug fixing. Notably, GitHub Copilot and Google's CodeBot exemplify how LLMc contributes to substantial time and effort savings in software development. However, the widespread application of these models has raised critical concerns regarding their trustworthiness. The lack of well-defined trust metrics beyond mere accuracy poses significant risks, including potential security vulnerabilities and compromised data integrity. This dissertation proposes solving this pressing need by developing a comprehensive framework to evaluate LLMc's trustworthiness. We aim to establish contextualized definitions of trust, distrust, and trustworthiness specific to LLMc, identify key influencing factors, and create a standardized evaluation framework encompassing both model-based attributes and human-centric considerations. We will validate the framework's effectiveness through rigorous empirical studies and user evaluations and provide insights for targeted improvements in LLMc development. This dissertation seeks to enhance the reliability and transparency of LLMc, fostering their responsible integration into software engineering practices and paving the way for more trustworthy AI-assisted code generation.",https://doi.org/10.1109/ICSE-Companion66252.2025.00063,27 April 2025 - 03 May 2025
https://ieeexplore.ieee.org/document/10764936/,Test-Driven Development and LLM-based Code Generation,"['Noble Saji Mathews', 'Meiyappan Nagappan']",,"Recent Large Language Models (LLMs) have demonstrated significant capabilities in generating code snippets directly from problem statements. This increasingly automated process mirrors traditional human-led software development, where code is often written in response to a requirement. Historically, Test-Driven Development (TDD) has proven its merit, requiring developers to write tests before the functional code, ensuring alignment with the initial problem statements. Applying TDD principles to LLM-based code generation offers one distinct benefit: it enables developers to verify the correctness of generated code against predefined tests. This paper investigates if and how TDD can be incorporated into AI-assisted code-generation processes. We experimentally evaluate our hypothesis that providing LLMs like GPT-4 and Llama 3 with tests in addition to the problem statements enhances code generation outcomes. We experimented with established function-level code generation benchmarks such as MBPP and HumanEval. Our results consistently demonstrate that including test cases leads to higher success in solving programming challenges. We assert that TDD is a promising paradigm for helping ensure that the code generated by LLMs effectively captures the requirements.CCS CONCEPTS• Software and its engineering → Software development techniques; • Computing methodologies → Artificial intelligence.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11028438/,Metamon: Finding Inconsistencies between Program Documentation and Behavior using Metamorphic LLM Queries,"['Hyeonseok Lee', 'Gabin An', 'Shin Yoo']",,"Code documentation can, if written precisely, help developers better understand the code they accompany. However, unlike code, code documentation cannot be automatically verified via execution, potentially leading to inconsistencies between documentation and the actual behavior. While such inconsistencies can be harmful for the developer’s understanding of the code, checking and finding them remains a costly task due to the involvement of human engineers. This paper proposes Metamon, which uses an existing search-based test generation technique to capture the current program behavior in the form of test cases, and subsequently uses LLM-based code reasoning to identify the generated regression test oracles that are not consistent with the program specifications in the documentation. Metamon is supported in this task by metamorphic testing and self-consistency. An empirical evaluation against 9,482 pairs of code documentation and code snippets, generated using five open-source projects from Defects4J v2.0.1, shows that Metamon can classify the code-and-documentation inconsistencies with a precision of 0.72 and a recall of 0.48.",https://doi.org/10.1109/LLM4Code66737.2025.00020,03-03 May 2025
https://ieeexplore.ieee.org/document/11052822/,Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements,"['Seyed Moein Abtahi', 'Akramul Azim']",,"This study examined code issue detection and revision automation by integrating Large Language Models (LLMs) such as OpenAI’s GPT-3.5 Turbo and GPT-4o into software development workflows. A static code analysis framework detects issues such as bugs, vulnerabilities, and code smells within a large-scale software project. Detailed information on each issue was extracted and organized to facilitate automated code revision using LLMs. An iterative prompt engineering process is applied to ensure that prompts are structured to produce accurate and organized outputs aligned with the project requirements. Retrieval-augmented generation (RAG) is implemented to enhance the relevance and precision of the revisions, enabling LLM to access and integrate real-time external knowledge. The issue of LLM hallucinations—where the model generates plausible but incorrect outputs—is addressed by a custom-built ""Code Comparison App,"" which identifies and corrects erroneous changes before applying them to the codebase. Subsequent scans using the static code analysis framework revealed a significant reduction in code issues, demonstrating the effectiveness of combining LLMs, static analysis, and RAG to improve code quality, streamline the software development process, and reduce time and resource expenditure.",https://doi.org/10.1109/Forge66646.2025.00017,27-28 April 2025
https://ieeexplore.ieee.org/document/10606356/,LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation,"['Sarah Fakhoury', 'Aaditya Naik', 'Georgios Sakkas', 'Saikat Chakraborty', 'Shuvendu K. Lahiri']",,"Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, given NL is informal, it does not lend easily to checking that the generated code correctly satisfies the user intent. In this paper, we propose a novel interactive workflow TiCoder for guided intent clarification (i.e., partial formalization) through tests to support the generation of more accurate code suggestions. Through a mixed methods user study with 15 programmers, we present an empirical evaluation of the effectiveness of the workflow to improve code generation accuracy. We find that participants using the proposed workflow are significantly more likely to correctly evaluate AI generated code, and report significantly less task-induced cognitive load. Furthermore, we test the potential of the workflow at scale with four different state-of-the-art LLMs on two python datasets, using an idealized proxy for a user feedback. We observe an average absolute improvement of 45.97% in the pass@1 code generation accuracy for both datasets and across all LLMs within 5 user interactions, in addition to the automatic generation of accompanying unit tests.",https://doi.org/10.1109/TSE.2024.3428972,
https://ieeexplore.ieee.org/document/10764938/,Bridging Gaps in LLM Code Translation: Reducing Errors with Call Graphs and Bridged Debuggers,"['Yang Luo', 'Richard Yu', 'Fajun Zhang', 'Ling Liang', 'Yongqiang Xiong']",,"When using large language models (LLMs) for code translation of complex software, numerous compilation and runtime errors can occur due to insufficient context awareness. To address this issue, this paper presents a code translation method based on call graphs and bridged debuggers: TransGraph. TransGraph first obtains the call graph of the entire code project using the Language Server Protocol, which provides a detailed description of the function call relationships in the program. Through this structured view of the code, LLMs can more effectively handle large-scale and complex codebases, significantly reducing compilation errors. Furthermore, TransGraph, combined with bridged debuggers and dynamic test case generation, significantly reduces runtime errors, overcoming the limitations of insufficient test case coverage in traditional methods. In experiments on six datasets including CodeNet and Avatar, TransGraph outperformed existing code translation methods and LLMs in terms of translation accuracy, with improvements of up to 10.2%.CCS CONCEPTS• Software and its engineering → Source code generation; Translator writing systems and compiler generators; Parsers.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11052811/,Vulnerability-Triggering Test Case Generation from Third-Party Libraries,"['Yi Gao', 'Xing Hu', 'Zirui Chen', 'Tongtong Xu', 'Xiaohu Yang']",,"Open-source third-party libraries are widely used in software development. These libraries offer substantial advantages in terms of time and resource savings. However, a significant concern arises due to the publicly disclosed vulnerabilities within these libraries. Existing automated vulnerability detection tools often suffer from false positives and fail to accurately assess the propagation of inputs capable of triggering vulnerabilities from client projects to vulnerable code in libraries. In this paper, we propose a novel approach called VulEUT (Vulnerability Exploit Unit Test Generation), which combines vulnerability exploitation reachability analysis and LLM-based unit test generation. VulEUT is designed to automatically verify the exploitability of vulnerabilities in third-party libraries commonly used in Java client software projects. VulEUT first analyzes the client projects to determine the reachability of vulnerability conditions. And then, it leverages the Large Language Model (LLM) to generate unit tests for vulnerability confirmation. To evaluate the effectiveness of VulEUT, we collect 32 vulnerabilities from various third-party libraries and conduct experiments on 70 real client projects. Besides, we also compare our approach with two representative tools, i.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VulEUT, with 229 out of 292 generated unit tests successfully confirming vulnerability exploitation across 70 client projects, which outperforms baselines by 24%.",https://doi.org/10.1109/Forge66646.2025.00021,27-28 April 2025
https://ieeexplore.ieee.org/document/10711016/,Automated Control Logic Test Case Generation using Large Language Models,"['Heiko Koziolek', 'Virendra Ashiwal', 'Soumyadip Bandyopadhyay', 'Chandrika K R']",,"Testing PLC and DCS control logic in industrial automation is laborious and challenging since appropriate test cases are often complex and difficult to formulate. Researchers have previously proposed several automated test case generation approaches for PLC software applying symbolic execution and search-based techniques. Often requiring formal specifications and performing a mechanical analysis of programs, these approaches may uncover specific programming errors but some-times suffer from state space explosion and cannot process rather informal specifications. We proposed a novel approach for the automatic generation of PLC test cases that queries a Large Language Model (LLM) to synthesize test cases for code provided in a prompt. Experiments with ten open-source function blocks from the OSCAT automation library showed that the approach is fast, easy to use, and can yield test cases with high statement coverage for low-to-medium complex programs. However, we also found that LLM-generated test cases suffer from erroneous assertions in many cases, which still require manual adaption.",https://doi.org/10.1109/ETFA61755.2024.10711016,10-13 September 2024
https://ieeexplore.ieee.org/document/10538871/,LLMs for Hardware Security: Boon or Bane?,"['Rahul Kande', 'Vasudev Gohil', 'Matthew DeLorenzo', 'Chen Chen', 'Jeyavijayan Rajendran']",,"Large language models (LLMs) have emerged as transformative tools within the hardware design and verification lifecycle, offering numerous capabilities in accelerating design processes. Recent research has showcased the efficacy of LLMs in translating design specifications into source code through hardware description languages. Researchers are also using LLMs to generate test cases and write assertion rules to bolster the detection of hardware vulnerabilities. Thus, the semiconductor industry is swiftly integrating LLMs into its design workflows. However, this adoption is not without its challenges.While LLMs offer remarkable benefits, they concurrently introduce security concerns that demand a thorough examination. These concerns manifest as potential vulnerabilities indirectly introduced into the designs while generating the design code, or by directly equipping the attackers with novel avenues for exploitation. In this paper, we discuss the emerging security implications due to the capabilities introduced by LLMs in the context of hardware design verification, evaluate the capabilities of existing security detection and mitigation techniques, and highlight the possible future security attacks that use LLMs.",https://doi.org/10.1109/VTS60656.2024.10538871,22-24 April 2024
https://ieeexplore.ieee.org/document/10967340/,Unraveling the Potential of Large Language Models in Code Translation: How Far are We?,"['Qingxiao Tao', 'Tingrui Yu', 'Xiaodong Gu', 'Beijun Shen']",,"While large language models (LLMs) exhibit state-of-the-art performance in various tasks, recent studies have revealed their struggle for code translation. This is because they haven't been extensively pre-trained with parallel multilingual code, which code translation heavily depends on. Moreover, existing benchmarks only cover a limited subset of common programming languages, and thus cannot reflect the full potential of LLMs in code translation. In this paper, we conduct a large-scale empirical study to exploit the capabilities and incapabilities of LLMs in code translation tasks. We first craft a novel benchmark called PolyHumanEval by extending HumanEval to a multilingual benchmark of 14 languages. With PolyHumanEval, we then perform over 110,000 translations with bleeding-edge code LLMs. The result shows LLMs' suboptimal performance on Python to other languages and the negligible impact of widely adopted LLM optimization techniques such as conventional pre-training and instruction tuning on code translation. To further uncover the potential of LLMs in code translation, we propose two methods: (1) intermediary translation which selects an intermediary language between the source and target ones; and (2) self-training which fine-tunes LLMs on self-generated parallel data. Evaluated with CodeLlama-13B, our approach yields an average improvement of 11.7% computation accuracy on Python-to-other translations. Notably, we interestingly find that Go can serve as a lingua franca for translating between any two studied languages.",https://doi.org/10.1109/APSEC65559.2024.00046,03-06 December 2024
https://ieeexplore.ieee.org/document/10554734/,CodeFuse-13B: A Pretrained Multi-Lingual Code Large Language Model,"['Peng Di', 'Jianguo Li', 'Hang Yu', 'Wei Jiang', 'Wenting Cai', 'Yang Cao']",,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectivness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CODEFuSE-13B, an open-sourced pre-trained code LLM 2. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CODEFUSE achieves its effectiveness by utilizing a high-quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HUMANEvAL-x, and the specially designed CODEFUSEEvAL for Chinese prompts. To assess the effectiveness of CODEFUSE, we actively collected valuable human feed-back from the AntGroup's software development process where CODEFUSE has been successfully deployed. The results demonstrate that CODEFUSE-13B achieves a HUMANEvAL pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CODEFUSE performs better than other models when confronted with Chinese prompts.",https://doi.org/10.1145/3639477.3639719,14-20 April 2024
https://ieeexplore.ieee.org/document/11066171/,TransferFuzz-Pro: Large Language Model Driven Code Debugging Technology for Verifying Propagated Vulnerability,"['Siyuan Li', 'Kaiyu Xie', 'Yuekang Li', 'Hong Li', 'Yimo Ren', 'Limin Sun']",,"Code reuse in software development frequently facilitates the spread of vulnerabilities, leading to imprecise scopes of affected software in CVE reports. Traditional methods focus primarily on detecting reused vulnerability code in target software but lack the ability to confirm whether these vulnerabilities can be triggered in new software contexts. In previous work, we introduced the TransferFuzz framework to address this gap by using historical trace-based fuzzing. However, its effectiveness is constrained by the need for manual intervention and reliance on source code instrumentation. To overcome these limitations, we propose TransferFuzz-Pro, a novel framework that integrates Large Language Model (LLM)-driven code debugging technology. By leveraging LLM for automated, human-like debugging and Proof-of-Concept (PoC) generation, combined with binary-level instrumentation, TransferFuzz-Pro extends verification capabilities to a wider range of targets. Our evaluation shows that TransferFuzz-Pro is significantly faster and can automatically validate vulnerabilities that were previously unverifiable using conventional methods. Notably, it expands the number of affected software instances for 15 CVE-listed vulnerabilities from 15 to 53 and successfully generates PoCs for various Linux distributions. These results demonstrate that TransferFuzz-Pro effectively verifies vulnerabilities introduced by code reuse in target software and automatically generation PoCs.",https://doi.org/10.1109/TSE.2025.3584774,
https://ieeexplore.ieee.org/document/10795597/,Transforming Software Development: A Study on the Integration of Multi-Agent Systems and Large Language Models for Automatic Code Generation,"['Rolando Ramírez-Rueda', 'Edgard Benítez-Guerrero', 'Carmen Mezura-Godoy', 'Everardo Bárcenas']",,"This paper explores the integration of Multi-Agent Systems (MAS) and Large Language Models (LLMs) for auto-matic code generation, addressing the limitations of traditional manual coding. By conducting a comprehensive review of existing literature and analyzing a practical case study, we demonstrate how MAS and LLMs can collaboratively enhance software development processes. The research focuses on the technical and theoretical challenges of this integration, highlighting the potential for improved productivity, adaptability, and quality in code generation. The findings contribute to AI-based software engineering by revealing new research directions in collective intelligence and automated programming.",https://doi.org/10.1109/CONISOFT63288.2024.00013,28 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10765071/,ℬ4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests,"['Mouxiang Chen', 'Zhongxin Liu', 'He Tao', 'Yusu Hong', 'David Lo', 'Xin Xia']",,"Selecting the best code solution from multiple generated ones is an essential task in code generation, which can be achieved by using some reliable validators (e.g., developer-written test cases) for assistance. Since reliable test cases are not always available and can be expensive to build in practice, researchers propose to automatically generate test cases to assess code solutions. However, when both code solutions and test cases are plausible and not reliable, selecting the best solution becomes challenging. Although some heuristic strategies have been proposed to tackle this problem, they lack a strong theoretical guarantee and it is still an open question whether an optimal selection strategy exists. Our work contributes in two ways. First, we show that within a Bayesian framework, the optimal selection strategy can be defined based on the posterior probability of the observed passing states between solutions and tests. The problem of identifying the best solution is then framed as an integer programming problem. Second, we propose an efficient approach for approximating this optimal (yet uncomputable) strategy, where the approximation error is bounded by the correctness of prior knowledge. We then incorporate effective prior knowledge to tailor code generation tasks. Both theoretical and empirical studies confirm that existing heuristics are limited in selecting the best solutions with plausible test cases. Our proposed approximated optimal strategy ℬ4 significantly surpasses existing heuristics in selecting code solutions generated by large language models (LLMs) with LLM-generated tests, achieving a relative performance improvement by up to 50% over the strongest heuristic and 246% over the random selection in the most challenging scenarios. Our code is publicly available at https://github.com/ZJU-CTAG/B4.CCS CONCEPTS• Computing methodologies → Artificial intelligence; • Software and its engineering → Software design engineering.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10745813/,ECG: Augmenting Embedded Operating System Fuzzing via LLM-Based Corpus Generation,"['Qiang Zhang', 'Yuheng Shen', 'Jianzhong Liu', 'Yiru Xu', 'Heyuan Shi', 'Yu Jiang']",,"Embedded operating systems (Embedded OSs) power much of our critical infrastructure but are, in general, much less tested for bugs than general-purpose operating systems. Fuzzing Embedded OSs encounter significant roadblocks due to much less documented specifications, an inherent ineffectiveness in generating high-quality payloads. In this article, we propose ECG, an Embedded OS fuzzer empowered by large language models (LLMs) to sufficiently mitigate the aforementioned issues. ECG approaches fuzzing Embedded OS by automatically generating input specifications based on readily available source code and documentation, instrumenting and intercepting execution behavior for directional guidance information, and generating inputs with payloads according to the pregenerated input specifications and directional hints provided from previous runs. These methods are empowered by using an interactive refinement method to extract the most from LLMs while using established parsing checkers to validate the outputs. Our evaluation results demonstrate that ECG uncovered 32 new vulnerabilities across three popular open-source Embedded OS (RT-Linux, RaspiOS, and OpenWrt) and detected ten bugs in a commercial Embedded OS running on an actual device. Moreover, compared to Syzkaller, Moonshine, KernelGPT, Rtkaller, and DRLF, ECG has achieved additional kernel code coverage improvements of 23.20%, 19.46%, 10.96%, 15.47%, and 11.05%, respectively, with an overall average improvement of 16.02%. These results underscore ECG’s enhanced capability in uncovering vulnerabilities, thus contributing to the overall robustness and security of the Embedded OS.",https://doi.org/10.1109/TCAD.2024.3447220,
https://ieeexplore.ieee.org/document/11028927/,FuzzCoder: Code Large Language Model-Based Fuzz Testing for Industrial IoT Programs,"['Liqun Yang', 'Chaoren Wei', 'Jian Yang', 'Wanxu Xia', 'Yuze Yang', 'Yang Luo']",,"Fuzz testing is an dynamic program analysis technique designed for discovering vulnerabilities in IoT systems. The core goal is to deliberately feed maliciously crafted inputs into an IoT device or service, triggering vulnerabilities such as system crashes, buffer overflow exploits, and memory corruption, etc. Efficiently generating malicious inputs remains challenging, with leading methods often relying on randomly mutating existing valid inputs. In this work, we propose to adopt fine-tuned large language models (FuzzCoder) to learn patterns in the input files from successful attacks to guide future fuzzing explorations. Specifically, we develop a framework that leverages code LLMs to guide the mutation process to perform meaningful input mutations. We formulate the mutation process as the sequenceto-sequence modeling, where LLM receives a sequence of bytes and outputs the mutated byte sequence. FuzzCoder is fine-tuned on our created instruction dataset (FuzzInstruct), where the successful fuzzing history is collected from the heuristic fuzzing tool. FuzzCoder can predict mutation positions and strategies for input files to trigger abnormal behaviors of the program. Most importantly, the experiment reveals results that FuzzCoder achieves better fuzzing performance compared to traditional and other AFL-based fuzzers, such as AFL, AFL++, AFLSmart, etc. On average, FuzzCoder achieves an improvement in code coverage of more than 20%, along with a significant increase in the number of crashes. 1",https://doi.org/10.1109/JIOT.2025.3577602,
https://ieeexplore.ieee.org/document/10463245/,"Generative Artificial Intelligence for Industry: Opportunities, Challenges, and Impact",['Barun Kumar Saha'],,"The recent advances in Generative Artificial In-telligence (GenAI) and Large Language Models (LLMs) have generated significant interest across the world. For a successful adoption of GenAI and LLMs by industry, it is critical to identify their potential benefits, impact, and challenges. Accordingly, in this work, we investigate a few use cases of LLMs, which are relevant across most industry segments. In order to empirically evaluate the impact of GenAI on the code generation use case, we build CodePrompt, a handcrafted dataset of sequential prompts used by a human user to generate code. We approximate efficiency by considering the ratio of the number of tokens of code generated by an LLM to the number of tokens in the user's prompt. Experimental results reveal that a sequential trial of prompts for code generation may lead to an efficiency factor of about 6.33, on average, which means that a user's effort is reduced to about one-sixth.",https://doi.org/10.1109/ICAIIC60209.2024.10463245,19-22 February 2024
https://ieeexplore.ieee.org/document/10930847/,Evaluating Spectrum-Based Fault Localization on Deep Learning Libraries,"['Ming Yan', 'Junjie Chen', 'Tianjie Jiang', 'Jiajun Jiang', 'Zan Wang']",,"Deep learning (DL) libraries have become increasingly popular and their quality assurance is also gaining significant attention. Although many fault detection techniques have been proposed, effective fault localization techniques tailored to DL libraries are scarce. Due to the unique characteristics of DL libraries (e.g., complicated code architecture supporting DL model training and inference with extensive multidimensional tensor calculations), the effectiveness of existing fault localization techniques for traditional software is also unknown on DL library faults. To bridge this gap, we conducted the first empirical study to investigate the effectiveness of fault localization on DL libraries. Specifically, we evaluated spectrum-based fault localization (SBFL) due to its high generalizability and affordable overhead on such complicated libraries. Based on the key aspects in SBFL, our study investigated the effectiveness of SBFL with different sources of passing test cases (including human-written, fuzzer-generated, and mutation-based test cases) and various suspicious value calculation methods. In particular, mutation-based test cases are produced by our designed rule-based mutation technique and LLM-based mutation technique tailored to DL library faults. To enable our extensive study, we built the first benchmark (Defects4DLL), which contains 120 real-world faults in PyTorch and TensorFlow with easy-to-use experimental environments. Our study delivered a series of useful findings. For example, the rule-based approach is effective in localizing crash faults in DL libraries, successfully localizing 44.44% of crash faults within Top-10 functions and 74.07% of crash faults within Top-10 files, while the passing test cases from DL library fuzzers perform poorly on this task. Furthermore, based on our findings on the complementarity of different sources, we designed a hybrid technique by effectively integrating human-written, LLM-mutated, rule-based mutated test cases,...",https://doi.org/10.1109/TSE.2025.3552622,
https://ieeexplore.ieee.org/document/11024336/,Students' Perception of ChatGPT in Software Engineering: Lessons Learned from Five Courses,"['Luciano Baresi', 'Andrea De Lucia', 'Antinisca Di Marco', 'Massimiliano Di Penta', 'Davide Di Ruscio', 'Leonardo Mariani']",,"A few years after their release, Large Language Models (LLMs)-based tools are becoming an essential component of software education, as calculators are used in math courses. When learning software engineering (SE), the challenge is the extent to which LLMs are suitable and easy to use for different software development tasks. In this paper, we report the findings and lessons learned from using LLM-based tools-ChatGPT in particular-in five SE courses from four universities. After instructing students on the LLM potentials in SE and about prompting strategies, we ask participants to complete a survey and be involved in semi-structured interviews. The collected results report (i) indications about the usefulness of the LLM for different tasks, (ii) challenges to prompt the LLM, i.e., interact with it, (iii) challenges to adapt the generated artifacts to their own needs, and (iv) wishes about some valuable features students would like to see in LLM-based tools. Although results vary among different courses, also because of students' seniority and course goals, the perceived usefulness is greater for lowlevel phases (e.g., coding or debugging/fault localization) than for analysis and design phases. Interaction and code adaptation challenges vary among tasks and are mostly related to the need for task-specific prompts, as well as better specification of the development context.",https://doi.org/10.1109/CSEET66350.2025.00023,27 April 2025 - 03 May 2025
https://ieeexplore.ieee.org/document/11029828/,Treefix: Enabling Execution with a Tree of Prefixes,"['Beatriz Souza', 'Michael Pradel']",,"The ability to execute code is a prerequisite for various dynamic program analyses. Learning-guided execution has been proposed as an approach to enable the execution of arbitrary code snippets by letting a neural model predict likely values for any missing variables. Although state-of-the-art learning-guided execution approaches, such as LExecutor, can enable the execution of a relative high amount of code, they are limited to predicting a restricted set of possible values and do not use any feedback from previous executions to execute even more code. This paper presents Treefix, a novel learning-guided execution approach that leverages LLMs to iteratively create code prefixes that enable the execution of a given code snippet. The approach addresses the problem in a multi-step fashion, where each step uses feedback about the code snippet and its execution to instruct an LLM to improve a previously generated prefix. This process iteratively creates a tree of prefixes, a subset of which is returned to the user as prefixes that maximize the number of executed lines in the code snippet. In our experiments with two datasets of Python code snippets, Treefix achieves 25% and 7% more coverage relative to the current state of the art in learning-guided execution, covering a total of 84% and 82% of all lines in the code snippets.",https://doi.org/10.1109/ICSE55347.2025.00215,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/11029747/,,[],,,,
https://ieeexplore.ieee.org/document/10329992/,,[],,,,
https://ieeexplore.ieee.org/document/10684637/,,[],,,,
https://ieeexplore.ieee.org/document/10663045/,,[],,,,
https://ieeexplore.ieee.org/document/10628428/,,[],,,,
https://ieeexplore.ieee.org/document/10727145/,,[],,,,
https://ieeexplore.ieee.org/document/11052849/,,[],,,,
https://ieeexplore.ieee.org/document/10548854/,Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer,"['Jingzhou Fu', 'Jie Liang', 'Zhiyong Wu', 'Yu Jiang']",,"Effective DBMS fuzzing relies on high-quality initial seeds, which serve as the starting point for mutation. These initial seeds should incorporate various DBMS features to explore the state space thoroughly. While built-in test cases are typically used as initial seeds, many DBMSs lack comprehensive test cases, making it difficult to apply state-of-the-art fuzzing techniques directly. To address this, we propose Sedar which produces initial seeds for a target DBMS by transferring test cases from other DBMSs. The underlying insight is that many DBMSs share similar functionalities, allowing seeds that cover deep execution paths in one DBMS to be adapted for other DBMSs. The challenge lies in converting these seeds to a format supported by the grammar of the target database. Sedar follows a three-step process to generate seeds. First, it executes existing SQL test cases within the DBMS they were designed for and captures the schema information during execution. Second, it utilizes large language models (LLMs) along with the captured schema information to guide the generation of new test cases based on the responses from the LLM. Lastly, to ensure that the test cases can be properly parsed and mutated by fuzzers, Sedar temporarily comments out unparsable sections for the fuzzers and uncomments them after mutation. We integrate Sedar into the DBMS fuzzers SQUIRREL and Griffin, targeting DBMSs such as Virtuoso, Mon-etDB, DuckDB, and ClickHouse. Evaluation results demonstrate significant improvements in both fuzzers. Specifically, compared to SQUIRREL and Griffin with non-transferred seeds, Sedar enhances code coverage by 72.46%-214.84% and 21.40%-194.46%; compared to SQUIRREL and Griffin with native test cases of these DBMSs as initial seeds, incorporating the transferred seeds of Sedar results in an improvement in code coverage by 4.90%-16.20% and 9.73%-28.41 %. Moreover, Sedar discovered 70 new vulnerabilities, with 60 out of them being uniquely found by Sedar with tra...",https://doi.org/10.1145/3597503.3639210,14-20 April 2024
https://ieeexplore.ieee.org/document/10589904/,Navigating Confidentiality in Test Automation: A Case Study in LLM Driven Test Data Generation,"['Hrishikesh Karmarkar', 'Supriya Agrawal', 'Avriti Chauhan', 'Pranav Shete']",,"In out sourced industrial projects for testing of web applications, often neither the application to be tested, nor its source code are provided to the testing team, due to confidentiality reasons, making systematic testing of these applications very challenging. However, textual descriptions of such systems are often available. So, one can consider leveraging a Large Language Model (LLM) to parse these descriptions and synthesize test generators (programs that produce test data). In our experience, LLM synthesized test generators suffer from two problems:- (1) unsound: the generators might produce invalid data and (2) incomplete: the generators typically fail to generate all expected valid inputs. To mitigate these problems, we introduce TestRefineGen a method for autonomously generating test data from textual descriptions. TestRe-fineGen begins by invoking an LLM to parse a given corpus of documents and produce multiple test gener-ators. It then uses a novel ranking approach to identify generators that can produce invalid test data, and then automatically repairs them using a counterexample-guided refinement process. Lastly, TestRefineGen per-forms a generalization procedure that offsets synthesis or refinements that leads to incompleteness, to obtain generators that produce more comprehensive valid in-puts. We evaluated the effectiveness of TestRefineGen on a manually curated set of 256 textual descriptions of test data. TestRefineGen synthesized generators that produce valid test data for 66.01 % of the descriptions. Using a combination of post-processing sanitisation and refinement it was able to successfully repair synthesized generators, which improved the success rate to 76.95 %. Further, our statistical analysis on a small subset of synthesized generators shows that TestRefineGen is able to generate test data that is well distributed across the input space. Thus, TestRefineGen can be an effective technique for autonomous test data generation for web testing...",https://doi.org/10.1109/SANER60148.2024.00041,12-15 March 2024
https://ieeexplore.ieee.org/document/10554852/,Domain Knowledge is All You Need: A Field Deployment of LLM-Powered Test Case Generation in FinTech Domain,"['Zhiyi Xue', 'Liangguo Li', 'Senyue Tian', 'Xiaohong Chen', 'Liangyu Chen', 'Min Zhang']",,"Despite the promise of automation, general-purpose Large Language Models (LLMs) face difficulties in generating complete and accurate test cases from informal software requirements, primarily due to challenges in interpreting unstructured text and producing diverse, relevant scenarios. This paper argues that incorporating domain knowledge significantly improves LLM performance in test case generation. We report on the successful deployment of our LLM-powered tool, LLM4Fin, in the FinTech domain, showcasing the crucial role of domain knowledge in addressing the aforementioned challenges. We demonstrate two methods for integrating domain knowledge: implicit incorporation through model fine-tuning, and explicit incorporation with algorithm design. This combined approach delivers remarkable results, achieving up to 98.18% improvement in test scenario coverage and reducing generation time from 20 minutes to 7 seconds.",https://doi.org/10.1145/3639478.3643087,14-20 April 2024
https://ieeexplore.ieee.org/document/10844968/,Quality Assurance for LLM-Generated Test Cases: A Systematic Literature Review,"['Hasali Edirisinghe', 'Dilani Wickramaarachchi']",,"The rapid advancements in artificial intelligence have transformed software testing, with Large Language Models (LLMs) emerging as powerful tools for automating test case generation. This paper explores Quality Assurance (QA) for LLM-generated test cases in black-box testing through a systematic literature review. Though LLMs are increasingly used for test case generation, challenges in ensuring their quality remain. Following PRISMA guidelines, relevant studies were selected from databases focusing on critical quality attributes, QA frameworks, metrics, and challenges. LLMs demonstrate high efficiency but face numerous issues. A recommendation for future research is given on addressing standardized metrics and improving human-AI collaboration for enhanced testing outcomes.",https://doi.org/10.1109/SLAAI-ICAI63667.2024.10844968,18-19 December 2024
https://ieeexplore.ieee.org/document/10818233/,Automated Structural Test Case Generation for Human-Computer Interaction Software Based on Large Language Model,"['Long Kang', 'Jun Ai', 'Minyan Lu']",,"As software systems expand in complexity, managing the vast and varied collection of test cases becomes increasingly difficult with traditional manual testing methods. This paper presents a new approach for automating the generation of structured test cases, named Test Element Extraction and Restructuring (TEER), which leverages the advanced natural language processing capabilities of large language models (LLMs). Specifically targeting human-computer interaction (HCI) software, TEER employs prompt tuning techniques to extract critical elements from natural language test cases and systematically reassemble them into structured formats. The study evaluates the effectiveness of TEER by applying it to common test cases from desktop HCI applications. The experimental results demonstrate that this method successfully produces structured test cases that meet predefined requirements.",https://doi.org/10.1109/DSA63982.2024.00027,02-03 November 2024
https://ieeexplore.ieee.org/document/10930297/,LLM-Based Video Analytics Test Scenario Generation in Smart Cities,"['Merve Yilmazer', 'Mehmet Karakose']",,"Rapid advances in the field of artificial intelligence have made significant contributions to the automation of software development and testing stages. Software created for use in various fields is tested with test scenarios created manually by software test experts or using test automation. Testing large-scale software with these methods complicates the testing phases because it requires increased human intervention and includes complex applications. In this study, an LLM-based scenario generation framework enhanced with prompt engineering is proposed for testing software to be used for video analysis in smart cities and smart campus areas. Thus, software test scenarios are created by strengthening large language models that are fast, flexible and have high learning ability using prompt engineering techniques. Test scenarios produced through LLM reinforced with prompt engineering techniques were evaluated with rarity and reality metrics and it was determined that more robust scenarios were produced compared to randomly generated test scenarios in the relevant field.",https://doi.org/10.1109/IT64745.2025.10930297,19-22 February 2025
https://ieeexplore.ieee.org/document/11029423/,LLM-AQuA-DiVeR: LLM-Assisted Quality Assurance Through Dialogues on Verifiable Specification with Requirement Owners,"['Shohei Mitani', 'Salonee Moona', ""Shin'ichiro Matsuo"", 'Eric Burger']",,"Quality Assurance (QA) is important for verifying software compliance with stakeholder requirements. QA faces a fundamental challenge of requirement interpretation ambiguity, which can result in insufficient software verification and failure in achieving the stakeholders' intended quality. The interpre-tation challenge intensifies in software development driven by Large Language Models (LLMs), where over-reliance can lead to missed quality-critical alternatives. However, existing works have paid limited attention to stakeholder involvement. We propose an LLM-assisted QA framework extending conventional LLM-driven development to enable stakeholder engagement in software verification. Our framework employs formal methods and rigorous testing to meet diverse quality demands, though this comprehensive verification introduces technical complexity affecting stakeholder engagement and verification costs. Our framework addresses these challenges through two key LLM roles: 1) an explanation assistant for stakeholder understanding, 2) a refinement assistant for incorporating stakeholder feedback while maintaining feasible verification costs. Our initial evaluation empirically demonstrates the framework's effectiveness through participant assessment scores, showing improved quality risk comprehension and efficient feedback incorporation in the verification process.",https://doi.org/10.1109/RAIE66699.2025.00008,29-29 April 2025
https://ieeexplore.ieee.org/document/10988960/,Poster: Unit Testing Past vs. Present: Examining LLMs' Impact on Defect Detection and Efficiency,"['Rudolf Ramler', 'Philipp Straubinger', 'Reinhold Plösch', 'Dietmar Winkler']",,"The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into software engineering workflows has shown potential to enhance productivity, particularly in software testing. This paper investigates whether LLM support improves defect detection effectiveness during unit testing. Building on prior studies comparing manual and tool-supported testing, we replicated and extended an experiment where participants wrote unit tests for a Java-based system with seeded defects within a time-boxed session, supported by LLMs. Comparing LLM supported and manual testing, results show that LLM support significantly increases the number of unit tests generated, defect detection rates, and overall testing efficiency. These findings highlight the potential of LLMs to improve testing and defect detection outcomes, providing empirical insights into their practical application in software testing.",https://doi.org/10.1109/ICST62969.2025.10988960,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10795004/,Take Loads Off Your Developers: Automated User Story Generation using Large Language Model,"['Tajmilur Rahman', 'Yuecai Zhu', 'Lamyea Maha', 'Chanchal Roy', 'Banani Roy', 'Kevin Schneider']",,"Software Maintenance and Evolution (SME) is moving fast with the assistance of artificial intelligence (AI), especially Large Language Models (LLM). Researchers have already started automating various activities of the SME workflow. Un-derstanding the requirements for maintenance and development work i.e. Requirements Engineering (RE) is a crucial phase that kicks off the SME workflow through multiple discussions on a proposed scope of work documented in different forms. The RE phase ends with a list of user stories for each unit task and usually created and tracked on a project management tool such as GitHub, Jira, AzurDev, etc. In this research, we collaborated with Bell Mobility to develop a tool “Geneus” (Generate UserSory) using GPT-4-turbo to automatically create user stories from software requirements documents. Requirements documents are usually long and contain complex information. Since LLMs typically suffer from hallucination when the input is too complex, this paper proposes a new prompting strategy, “Refine and Thought” (RaT), to mitigate that issue and improve the performance of the LLM in prompts with large and noisy contexts. Along with manual evaluation using RUST (Readability, Understandability, Specificity, Technical-aspects) survey questionnaire, automatic evaluation with BERTScore, and AlignScore evaluation metrics are used to evaluate the results of the “Geneus” tool. Results show that our method with RaT performs consistently better in most of the cases of interactions compared to the single-shot baseline method. However, the BERTScore and AlignScore test results are not consistent. In the median case, Geneus performs significantly better in all three interactions (requirements specifi-cation, user story details, and test case specifications) according to AlignScorebut it shows slightly low performance in requirements specifications according to BERTScore. Distilling RE documents requires significant time & effort from the senior members of th...",https://doi.org/10.1109/ICSME58944.2024.00082,06-11 October 2024
https://ieeexplore.ieee.org/document/11029762/,Test Intention Guided LLM-Based Unit Test Generation,"['Zifan Nan', 'Zhaoqiang Guo', 'Kui Liu', 'Xin Xia']",,"The emergence of Large Language Models (LLMs) has accelerated the progress of intelligent software engineering technologies, which brings promising possibilities for unit test generation. However, existing approaches for unit tests directly generated from Large Language Models (LLMs) often prove impractical due to their low coverage and insufficient mocking capabilities. This paper proposes IntUT, a novel approach that utilizes explicit test intentions (e.g., test inputs, mock behaviors, and expected results) to effectively guide the LLM to generate high-quality test cases. Our experimental results on three industry Java projects and live study demonstrate that prompting LLM with test intention can generate high-quality test cases for developers. Specifically, it achieves the improvements on branch coverage by 94 % and line coverage by 49 %. Finally, we obtain developers' feedback on using IntUT to generate cases for three new Java projects, achieving over 80 % line coverage and 30 % efficiency improvement on writing unit test cases.",https://doi.org/10.1109/ICSE55347.2025.00243,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10554970/,Understandable Test Generation Through Capture/Replay and LLMs,['Amirhossein Deljouyi'],https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10554970,"Automatic unit test generators, particularly search-based software testing (SBST) tools such as EvoSuite, efficiently generate unit test suites with acceptable coverage. Although this removes the burden of writing unit tests from developers, these generated tests often pose challenges in terms of comprehension for developers. In my doctoral research, I aim to investigate strategies to address the issue of comprehensibility in generated test cases and improve the test suite in terms of effectiveness. To achieve this, I introduce four projects leveraging Capture/Replay and Large Language Model (LLM) techniques. Capture/Replay carves information from End-to-End (E2E) tests, enabling the generation of unit tests containing meaningful test scenarios and actual test data. Moreover, the growing capabilities of large language models (LLMs) in language analysis and transformation play a significant role in improving readability in general. Our proposed approach involves leveraging E2E test scenario extraction alongside an LLM-guided approach to enhance test case understandability, augment coverage, and establish comprehensive mock and test oracles. In this research, we endeavor to conduct both a quantitative analysis and a user evaluation of the quality of the generated tests in terms of executability, coverage, and understandability.",https://doi.org/10.1145/3639478.3639789,14-20 April 2024
https://ieeexplore.ieee.org/document/10962454/,A System for Automated Unit Test Generation using Large Language Models and Assessment of Generated Test Suites,"['Andrea Lops', 'Fedelucio Narducci', 'Azzurra Ragone', 'Michelantonio Trizio', 'Claudio Bartolini']",,"Unit tests are fundamental for ensuring software correctness but are costly and time-intensive to design and create. Recent advances in Large Language Models (LLMs) have shown potential for automating test generation, though existing evaluations often focus on simple scenarios and lack scalability for real-world applications. To address these limitations, we present AgoneTest, an automated system for generating and assessing complex, class-level test suites for Java projects. Leveraging the Methods2Test dataset, we developed Classes2Test, a new dataset enabling the evaluation of LLM-generated tests against human-written tests. Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.",https://doi.org/10.1109/ICSTW64639.2025.10962454,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10771447/,Enhancing Black-box Compiler Option Fuzzing with LLM through Command Feedback,"['Taiyan Wang', 'Ruipeng Wang', 'Yu Chen', 'Lu Yu', 'Zulie Pan', 'Min Zhang']",,"Since the compiler acts as a core component in software building, it is essential to ensure its availability and reliability through software testing and security analysis. Most research has focused on compiler robustness when compiling various test cases, while the reliability of compiler options lacks attention, especially since each option can activate a specific compiler function. Although some researchers have made efforts in testing it, the insufficient utilization of compiler command feedback messages leads to the poor efficiency, which hinders more diverse and in-depth testing.In this paper, we propose a novel solution to enhance black-box compiler option fuzzing by utilizing command feedback, such as error messages, standard output and compiled files, to guide the error fixing and option pruning via prompting large language models for suggestions. We have implemented the prototype and evaluated it on 4 versions of LLVM. Experiments show that our method significantly improves the detection of crashes, reduces false negatives, and even increase the success rate of compilation when compared to the baseline. To date, our method has identified hundreds of unique bugs, and 9 of them are previously unknown. Among these, 8 have been assigned CVE numbers, and 1 has been fixed following our report.",https://doi.org/10.1109/ISSRE62328.2024.00039,28-31 October 2024
https://ieeexplore.ieee.org/document/10196883/,Automated Metamorphic-Relation Generation with ChatGPT: An Experience Report,"['Yifan Zhang', 'Dave Towey', 'Matthew Pike']",,"This paper reports on a pilot study of using ChatGPT, a language model based on GPT-3.5 architecture, for automatic generation of metamorphic relations (MRs), in the context of testing of autonomous driving systems (ADSs). The oracle problem is a major challenge in testing such systems, where it is difficult to determine whether or not the output of a system is correct. Metamorphic testing (MT) can alleviate this problem by checking the consistency of the system’s outputs under various transformations. However, manual generation of MRs is often a time-consuming and error-prone process. Automated MR generation can yield several benefits, including enhanced efficiency, quality, coverage, scalability, and reusability in software testing, thereby facilitating a more comprehensive and effective testing process. In this paper, we investigate the effectiveness of using ChatGPT for automatic generation of MRs for ADSs. We provide a detailed methodology for generating MRs using ChatGPT and evaluate the generated MRs using our domain knowledge and existing MRs. The results of our study indicate that our proposed approach is effective at generating high-quality MRs, and can significantly reduce the manual effort required for MR generation. Furthermore, we discuss the practical implications and limitations of using ChatGPT for MR generation and provide recommendations for future research. Our study contributes to the advancement of automated testing of ADSs, which is crucial for ensuring their safety and reliability in real-world scenarios.",https://doi.org/10.1109/COMPSAC57700.2023.00275,26-30 June 2023
https://ieeexplore.ieee.org/document/10962520/,Evaluating Large Language Model Robustness using Combinatorial Testing,"['Jaganmohan Chandrasekaran', 'Ankita Ramjibhai Patel', 'Erin Lanus', 'Laura J. Freeman']",,"Recent advancements in large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like text, leading to widespread adoption across domains. Given LLM’s versatile capabilities, current evaluation practices assess LLMs across a wide variety of tasks, including answer generation, sentiment analysis, text completion, and question and answers, to name a few. Multiple choice questions (MCQ) have emerged as a widely used evaluation task to assess LLM’s understanding and reasoning across various subject areas. However, studies from the literature have revealed that LLMs exhibit sensitivity to the ordering of options in MCQ tasks, with performance variations based on option sequence, thus underscoring the robustness concerns in LLM performance.This work presents a combinatorial testing-based framework for systematic and comprehensive robustness assessment of pre-trained LLMs. By leveraging the sequence covering array, the framework constructs test sets by systematically swapping the order of options, which are then used in ascertaining the robustness of LLMs. We performed an experimental evaluation using the Measuring Massive Multitask Language Understanding (MMLU) dataset, a widely used MCQ dataset and evaluated the robustness of GPT 3.5 Turbo, a pre-trained LLM. Results suggest the framework can effectively identify numerous robustness issues with a relatively minimal number of tests.",https://doi.org/10.1109/ICSTW64639.2025.10962520,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10942340/,"LLM-Driven, Self-Improving Framework for Security Test Automation: Leveraging Karate DSL for Augmented API Resilience","['Emil Marian Pasca', 'Daniela Delinschi', 'Rudolf Erdei', 'Oliviu Matei']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10942340,"Modern software architectures heavily rely on APIs, yet face significant security challenges, particularly with Broken Object Level Authorization (BOLA) vulnerabilities, which remain the most critical API security risk according to OWASP. This paper introduces Karate-BOLA-Guard, an innovative framework leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques to automate security-focused test case generation for APIs. Our approach integrates vector databases for context retrieval, multiple LLM models for test generation, and observability tools for process monitoring. Initial experiments were carried out on three deliberately vulnerable APIs (VAmPI, Crapi, and OWASP Juice Shop), with subsequent validation on fifteen additional production APIs spanning diverse domains including social media, version control systems, financial services, and transportation services. Our evaluation metrics show Llama 3 8B achieving consistent performance (Accuracy: 3.1-3.4, Interoperability: 3.7-4.3) with an average processing time of 143.76 seconds on GPU. Performance analysis revealed significant GPU acceleration benefits, with 20-25x improvement over CPU processing times. Smaller models demonstrated efficient processing, with Phi-3 Mini averaging 69.58 seconds and Mistral 72.14 seconds, while maintaining acceptable accuracy scores. Token utilization patterns showed Llama 3 8B using an average of 36,591 tokens per session, compared to Mistral’s 25,225 and Phi-3 Mini’s 31,007. Our framework’s effectiveness varied across APIs, with notably strong performance in complex platforms (Instagram: A = 4.3, I = 4.4) while maintaining consistent functionality in simpler implementations (VAmPI: A = 3.6, I = 4.3). The iterative refinement process, evaluated through comprehensive metrics including Accuracy (A), Complexity (C), and Interoperability (I), represents a significant advancement in automated API security testing, offering an efficient, accurate, and adapt...",https://doi.org/10.1109/ACCESS.2025.3554960,
https://ieeexplore.ieee.org/document/10764966/,Magneto: A Step-Wise Approach to Exploit Vulnerabilities in Dependent Libraries via LLM-Empowered Directed Fuzzing,"['Zhuotong Zhou', 'Yongzhuo Yang', 'Susheng Wu', 'Yiheng Huang', 'Bihuan Chen', 'Xin Peng']",,"The wide adoption of open source third-party libraries can propagate vulnerabilities that originally exist in third-party libraries through dependency chains to downstream projects. To mitigate this security risk, vulnerability exploitation analysis has been proposed to further reduce false positives of vulnerability reachability analysis. However, existing approaches work less effectively when the vulnerable function of the vulnerable library is indirectly invoked by a client project through a call chain of multiple steps.To address this problem, we propose a step-wise approach, named Magneto, to exploit vulnerabilities in dependent libraries of a client project through LLM-empowered directed fuzzing. Its core idea is to decompose the directed fuzzing for the whole call chain (from the client project to the vulnerable function) into a series of stepwise directed fuzzing for each step of the call chain. To empower directed fuzzing, it leverages LLM to facilitate the initial seed generation. Our evaluation has demonstrated the effectiveness of Magneto over the state-of-the-art; i.e., Magneto achieves an improvement of at least 75.6% in successfully exploiting the vulnerability.CCS CONCEPTS • Security and privacy → Vulnerability management.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10599579/,MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation,"['Guanyu Wang', 'Yuekang Li', 'Yi Liu', 'Gelei Deng', 'Tianlin Li', 'Guosheng Xu']",,"Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation. This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM -augmented generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissim-ilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world matching scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51% on our tests compared to the original datasets, em-phasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM -augmented applications.",https://doi.org/10.1145/3650105.3652297,14-14 April 2024
https://ieeexplore.ieee.org/document/10973230/,Action-State Testing—A Model for Test Design Automation,"['István Forgács', 'Attila Kovács']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10973230,"Model-based testing (MBT) is essential in software testing, offering automation, comprehensive coverage, and defect prevention. It uses abstract models to automatically design and generate test cases, representing the expected system behaviour, including states, transitions, inputs, and outputs. This paper explores the action-state testing modelling technique, originally introduced by the authors in Forgács and Kovács (2000). In this approach, a model step comprises an action (input), one or more responses (outputs), and an optional state. The steps can be arranged sequentially, or they may be forked and joined. Sequential steps appear within the same test case. Forked steps are distributed across different test cases. The joined steps also belong to separate test cases. In addition, the graphical model can be constructed using a text editor. This paper builds upon the concept by establishing its theoretical foundation. We demonstrate how the action-state model eliminates the need for guard conditions and coding, maintains a concise and manageable structure, and seamlessly incorporates outputs, ultimately enhancing testing efficiency. Additionally, we provide guidelines for adding new states and empirically validate the benefits of action-state testing over alternative techniques, achieving a 100% defect detection percentage (DDP). This paper marks the first instalment of the author’s Test Design Trilogy, dedicated to refining and unifying various test design techniques.",https://doi.org/10.1109/ACCESS.2025.3563337,
https://ieeexplore.ieee.org/document/11028238/,Exploring Large Language Models for Requirements on String Values,"['Aren A. Babikian', 'Boqi Chen', 'Gunter Mussbacher']",,"Behavior-driven development (BDD) enables collaboration among different stakeholders by employing a natural-language representation of system requirements and of test scenarios. These scenarios often involve constraints over string values, e.g. for the validity of email addresses, which are challenging to test comprehensively. Traditional methods like SMT solvers (e.g. Z3, Ostrich) handle constraints efficiently but produce unrealistic strings and require formal specifications that are often unavailable and expensive to compute. This paper explores the potential of large language models (LLMs) in generating realistic, constraint-satisfying strings for BDD. We propose an evaluation framework to assess LLMs’ ability to (1) generate consistent string values and (2) detect constraint inconsistencies. In our experiments, three LLMs are compared to state-of-the-art solvers using constraints from a software engineering course project. Results show that while solvers dominate in precision and recall, LLMs derive realistic strings more suitable for a requirements engineering context. With these trade-offs, we believe that, when formal constraints are available, a combined LLM-solver approach could offer a more effective solution.",https://doi.org/10.1109/MO2RE66661.2025.00009,27-27 April 2025
https://ieeexplore.ieee.org/document/10784391/,Test Case Migration from Monolith to Microservices Using Large Language Models,"['Hang-Wei Yeh', 'Shang-Pin Ma', 'Yi Chen']",,"Due to microservices' modularity, high scalability, and good fault tolerance, more and more software systems are transitioning from monolithic architectures to microservice architectures. In this migration process, the migration of test cases is a crucial step to ensure functional consistency and completeness before and after the migration, thereby maintaining the stability and reliability of the microservice system post-migration. However, despite numerous studies on architecture migration, there is still a lack of methods to efficiently convert test cases of monolithic systems into ones for the migrated microservices. Therefore, this study proposes a test migration approach based on Large Language Models (LLM), called LTM^3 (LLM-based Test Migration from Monolith to Microservices). During migration, LTM^3 identifies the correspondence between existing test cases and monolithic functions, the connections between monolithic functions and migrated microservices, and the dependencies between microservices. Subsequently, by utilizing LLM and appropriate prompting, the integration test cases of the monolithic system are transformed into contract test cases for each microservice. The experimental results showed that LTM^3 can effectively migrate most test cases, with only a tiny portion requiring manual adjustment.",https://doi.org/10.1109/ICEBE62490.2024.00014,11-13 October 2024
https://ieeexplore.ieee.org/document/11021674/,Evaluating Large Language Models Via Multi-Modal User Knowledge Graphs: A Comprehensive Assessment Framework,"['Pan Liu', 'Zizhao Chen', 'Yihao Li', 'W. Eric Wong']",,"Large language models (LLMs) have been widely adopted across various industries, but issues such as hallucinations, biases, and erroneous outputs frequently arise, compromising their reliability and safety. Objectively assessing how well an LLM interprets user queries is crucial for selecting the right model for practical problem-solving. This paper proposes an evaluation method that leverages user knowledge graphs to measure an LLM's comprehension of user input. Specifically, we construct both text-based and graphical test cases derived from a user knowledge graph, thereby enabling multi-modal assessment of the LLM's understanding. To implement our approach, a knowledge graph is first built from the user's input and then mutated to produce diverse test cases. A search-based testing method is then applied to evaluate the model's comprehension. We provide a case study demonstrating the framework. Our findings indicate that multi-modal test cases outperform purely text-based test cases in revealing the true understanding capability of LLMs. Among the eight models tested, DeepSeek and Doubao exhibit stronger comprehension than the remaining six.",https://doi.org/10.1109/ISSSR65654.2025.00047,12-13 April 2025
https://ieeexplore.ieee.org/document/11025920/,Using Large Language Models to Generate Concise and Understandable Test Case Summaries,"['Natanael Djajadi', 'Amirhossein Deljouyi', 'Andy Zaidman']",,"Software testing is essential, and automatic test case generation can be an important aid to software engineers. However, generated tests are sometimes difficult to understand. Test summarization approaches that provide an overview of what exactly is tested can provide help, but existing summarization approaches generate documentation that is lengthy and redundant. In this paper, we investigate whether large language models (LLMs) can be used to generate more concise, yet understandable summaries. In a small-scale user study with 11 participants, we obtained positive feedback on the LLM-generated summaries.",https://doi.org/10.1109/ICPC66645.2025.00040,27-28 April 2025
https://ieeexplore.ieee.org/document/11030993/,Enhancing Flutter App Development: Addressing Configuration and Compatibility Bugs Using Large Language Models,"['Supun Rajaguru', 'Lohara Chathumini', 'Samantha Kumara', 'Ashansa Wijerathna']",,"In today’s world, there are cross-platform frameworks are widely available, The frameworks like Flutter, allow the developer to develop an app from a single codebase. But there are some major issues like debugging and compatibility finding in Flutter framework. These issues lead to prolonged debugging and lesser app reliability. The proposed model will lead to overcome those mentioned challenges. The proposed model used advanced LLMs including GPT-4o, Claude Sonnet 3.5, and Gemini 2.0 Flash, combined with RAG capabilities. The data was collected from multiple sources like Stack Overflow, GitHub, and Flutter documentation. The dataset was cleaned and preprocessed by removing low-scoring answers, filtering incomplete questions, and applying text sanitization techniques to ensure structured and relevant inputs for analysis. A fixed set of prompt templates to 350 Stack Overflow questions across all the LLMs were applied to commence the evaluation process. The 25 most important questions were identified through verification via cosine similarity and precision correction between its responses. It was able to determine the two models’ efficacy in finding configuration bugs and addressing those bugs. The current work involves integrating an RAG pipeline and using a vector database for better retrieval and response generation. Early evidence hints at the enhanced accuracy/precision of an RAG-enhanced LLM when that LLM is a standalone model for better Flutter configuration. In conclusion this proposed model presents a modular framework to integrate RAG-enhanced LLMs for bug detection and fixing. The framework requires much lesser debugging efforts along with more reliability of the app. Additionally, it can also adopt the fast-evolving Flutter framework. By filling a significant gap in cross-platform development literature, it helps advance AI-assisted debugging and improve the development workflows of Flutter apps.",https://doi.org/10.1109/SCSE65633.2025.11030993,03-03 April 2025
https://ieeexplore.ieee.org/document/11028452/,Do Code LLMs Understand Design Patterns?,"['Zhenyu Pan', 'Xuefeng Song', 'Yunkun Wang', 'Rongyu Cao', 'Binhua Li', 'Yongbin Li']",,"Code Large Language Models (LLMs) demonstrate great versatility in adapting to various downstream tasks, including code generation and completion, as well as bug detection and fixing. However, Code LLMs often fail to capture existing coding standards, leading to the generation of code that conflicts with the required design patterns for a given project. As a result, developers must post-process to adapt the generated code to the project’s design norms. In this work, we empirically investigate the biases of Code LLMs in software development. Through carefully designed experiments, we assess the models’ understanding of design patterns across recognition, comprehension, and generation. Our findings reveal that biases in Code LLMs significantly affect the reliability of downstream tasks.",https://doi.org/10.1109/LLM4Code66737.2025.00031,03-03 May 2025
https://ieeexplore.ieee.org/document/10825785/,RAGFix: Enhancing LLM Code Repair Using RAG and Stack Overflow Posts,"['Elijah Mansur', 'Johnson Chen', 'Muhammad Anas Raza', 'Mohammad Wardat']",,"Identifying, localizing, and resolving bugs in software engineering is challenging and costly. Approaches to resolve software bugs range from Large Language Model (LLM) code analysis and repair, and automated code repair technology that aims to alleviate the technical burden of difficult to solve bugs. We propose RAGFix, which enhances LLM’s capabilities for bug localization and code repair using Retrieval Augmented Generation (RAG) based on dynamically collected Stack Overflow posts. These posts are searchable via a Question and Answer Knowledge Graph (KGQA). We evaluate our method on the HumanEvalFix benchmark for Python using relevant closed and open-source models. Our approach facilitates error resolution in Python coding problems by creating a searchable, embedded knowledge graph representation of bug and solution information from Stack Overflow, interlinking bugs, and solutions through semi-supervised graph construction methods. We use cosine similarity on embeddings based on LLM-synthesized summaries and algorithmic features describing the coding problem and potential solution to find relevant results that improve LLM in-context performance. Our results indicate that our system enhances small open-source models’ ability to effectively repair code, particularly where these models have less parametric knowledge about relevant coding problems and can leverage nonparametric knowledge to provide accurate, actionable fixes.",https://doi.org/10.1109/BigData62323.2024.10825785,15-18 December 2024
https://ieeexplore.ieee.org/document/11024256/,CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge Graph,"['Hanxiang Xu', 'Wei Ma', 'Ting Zhou', 'Yanjie Zhao', 'Kai Chen', 'Qiang Hu']",,"In recent years, the programming capabilities of large language models (LLMs) have garnered significant attention. Fuzz testing, a highly effective technique, plays a key role in enhancing software reliability and detecting vulnerabilities. However, traditional fuzz testing tools rely on manually crafted fuzz drivers, which can limit both testing efficiency and effectiveness. To address this challenge, we propose an automated fuzz testing method driven by a code knowledge graph and powered by an LLM-based intelligent agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a code generation task, leveraging the knowledge graph of the code repository to automate the generation process within the fuzzing loop, while continuously refining both the fuzz driver and input seeds. The code knowledge graph is constructed through interprocedural program analysis, where each node in the graph represents a code entity, such as a function or a file. The knowledge graph-enhanced CKGFuzzer not only effectively resolves compilation errors in fuzz drivers and generates input seeds tailored to specific API usage scenarios, but also analyzes fuzz driver crash reports, assisting developers in improving code quality. By querying the knowledge graph of the code repository and learning from API usage scenarios, we can better identify testing targets and understand the specific purpose of each fuzz driver. We evaluated our approach using eight open-source software projects. The experimental results indicate that CKGFuzzer achieved an average improvement of 8.73% in code coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer reduced the manual review workload in crash case analysis by 84.4% and successfully detected 11 real bugs (including nine previously unreported bugs) across the tested libraries. Our research enhances the overall performance of fuzz testing by refining fuzz driver generation strategies and input seed analysis, offering a more effect...",https://doi.org/10.1109/ICSE-Companion66252.2025.00079,27 April 2025 - 03 May 2025
https://ieeexplore.ieee.org/document/10765042/,Leveraging Large Language Model to Assist Detecting Rust Code Comment Inconsistency,"['Yichi Zhang', 'Zixi Liu', 'Yang Feng', 'Baowen Xu']",,"Rust is renowned for its robust memory safety capabilities, yet its distinctive memory management model poses substantial challenges in both writing and understanding programs. Within Rust source code, comments are employed to clearly delineate conditions that might cause panic behavior, thereby warning developers about potential hazards associated with specific operations. Therefore, comments are particularly crucial for documenting Rust’s program logic and design. Nevertheless, as modern software frequently undergoes updates and modifications, maintaining the accuracy and relevance of these comments becomes a labor-intensive endeavor.In this paper, inspired by the remarkable capabilities of Large Language Models (LLMs) in understanding software programs, we propose a code-comment inconsistency detection tool, namely RustC4, that combines program analysis and LLM-driven techniques to identify inconsistencies in code comments. RustC4 leverages LLMs’ ability to interpret natural language descriptions within code comments, facilitating the extraction of design constraints. Program analysis techniques are then employed to accurately verify the implementation of these constraints. To evaluate the effectiveness of RustC4, we construct a dataset from 12 large-scale real-world Rust projects. The experiment results demonstrate that RustC4 is effective in detecting 176 real inconsistent cases and 23 of them have been confirmed and fixed by developers by the time this paper was submitted.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11029833/,Boosting Static Resource Leak Detection via LLM-based Resource-Oriented Intention Inference,"['Chong Wang', 'Jianan Liu', 'Xin Peng', 'Yang Liu', 'Yiling Lou']",,"Resource leaks, caused by resources not being released after acquisition, often lead to performance issues and system crashes. Existing static detection techniques rely on mechanical matching of predefined resource acquisition/release APIs and null-checking conditions to find unreleased resources, suffering from both (1) false negatives caused by the incompleteness of predefined resource acquisition/release APIs and (2) false positives caused by the incompleteness of resource reachability validation identification. To overcome these challenges, we propose InferROI, a novel approach that leverages the exceptional code comprehension capability of large language models (LLMs) to directly infer resource-oriented intentions (acquisition, release, and reachability validation) in code. InferROI first prompts the LLM to infer involved intentions for a given code snippet, and then incorporates a two-stage static analysis approach to check control-flow paths for resource leak detection based on the inferred intentions. We evaluate the effectiveness of InferROI in both resource-oriented intention inference and resource leak detection. Experimental results on the DroidLeaks and JLeaks datasets demonstrate InferROI achieves promising bug detection rate (59.3% and 62.5%) and false alarm rate (18.6% and 19.5%). Compared to three industrial static detectors, InferROI detects 14~45 and 149~485 more bugs in DroidLeaks and JLeaks, respectively. When applied to real-world open-source projects, InferROI identifies 29 unknown resource leak bugs (verified by authors), with 7 of them being confirmed by developers. In addition, the results of an ablation study underscores the importance of combining LLM-based inference with static analysis. Finally, manual annotation indicated that InferROI achieved a precision of 74.6% and a recall of 81.8% in intention inference, covering more than 60% resource types involved in the datasets.",https://doi.org/10.1109/ICSE55347.2025.00131,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/11052804/,Testing Refactoring Engine via Historical Bug Report driven LLM,"['Haibo Wang', 'Zhuolin Xu', 'Shin Hwei Tan']",,"Refactoring is the process of restructuring existing code without changing its external behavior while improving its internal structure. Refactoring engines are integral components of modern Integrated Development Environments (IDEs) and can automate or semi-automate this process to enhance code readability, reduce complexity, and improve the maintainability of software products. Similar to traditional software systems such as compilers, refactoring engines may also contain bugs that can lead to unexpected behaviors. In this paper, we propose a novel approach called RETester, a LLM-based framework for automated refactoring engine testing. Specifically, by using input program structure templates extracted from historical bug reports and input program characteristics that are error-prone, we design chain-of-thought (CoT) prompts to perform refactoring-preserving transformations. The generated variants are then tested on the latest version of refactoring engines using differential testing. We evaluate RETester on two most popular modern refactoring engines (i.e., Eclipse, and IntelliJ IDEA). It successfully revealed 18 previously unknown bugs in the latest version of those refactoring engines, seven of them have been confirmed by their developers, and three have been fixed.",https://doi.org/10.1109/Forge66646.2025.00020,27-28 April 2025
https://ieeexplore.ieee.org/document/11029791/,The Seeds of the Future Sprout from History: Fuzzing for Unveiling Vulnerabilities in Prospective Deep-Learning Libraries,"['Zhiyuan Li', 'Jingzheng Wu', 'Xiang Ling', 'Tianyue Luo', 'Zhiqing Rui', 'Yanjun Wu']",,"The widespread application of large language models (LLMs) underscores the importance of deep learning (DL) technologies that rely on foundational DL libraries such as PyTorch and TensorFlow. Despite their robust features, these libraries face challenges with scalability and adaptation to rapid advancements in the LLM community. In response, tech giants like Apple and Huawei are developing their own DL libraries to enhance performance, increase scalability, and safeguard intellectual property. Ensuring the security of these libraries is crucial, with fuzzing being a vital solution. However, existing fuzzing frameworks struggle with target flexibility, effectively testing bug-prone API sequences, and leveraging the limited available information in new libraries. To address these limitations, we propose FUTURE, the first universal fuzzing framework tailored for newly introduced and prospective DL libraries. FUTURE leverages historical bug information from existing libraries and fine-tunes LLMs for specialized code generation. This strategy helps identify bugs in new libraries and uses insights from these libraries to enhance security in existing ones, creating a cycle from history to future and back. To evaluate FUTURE's effectiveness, we conduct comprehensive evaluations on three newly introduced DL libraries. Evaluation results demonstrate that FUTURE significantly outperforms existing fuzzers in bug detection, success rate of bug reproduction, validity rate of code generation, and API coverage. Notably, FUTURE has detected 148 bugs across 452 targeted APIs, including 142 previously unknown bugs. Among these, 10 have been assigned CVE IDs. Additionally, FUTURE detects 7 bugs in PyTorch, demonstrating its ability to enhance security in existing libraries in reverse.",https://doi.org/10.1109/ICSE55347.2025.00132,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/11028362/,Are Large Language Models Memorizing Bug Benchmarks?,"['Daniel Ramos', 'Claudia Mamede', 'Kush Jain', 'Paulo Canelas', 'Catarina Gamboa', 'Claire Le Goues']",,"Large Language Models (LLMs) have become integral to various software engineering tasks, including code generation, bug detection, and repair. To evaluate model performance in these domains, numerous bug benchmarks containing real-world bugs from software projects have been developed. However, a growing concern within the software engineering community is that these benchmarks may not reliably reflect true LLM performance due to the risk of data leakage. Despite this concern, limited research has been conducted to quantify the impact of potential leakage.In this paper, we systematically evaluate popular LLMs to assess their susceptibility to data leakage from widely used bug benchmarks. To identify potential leakage, we use multiple metrics, including a study of benchmark membership within commonly used training datasets, as well as analyses of negative log-likelihood and 5-gram accuracy. Our findings show that certain models, in particular codegen-multi, exhibit significant evidence of memorization in widely used benchmarks like Defects4J, while newer models trained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage. These results highlight the need for careful benchmark selection and the adoption of robust metrics to adequately assess models capabilities.",https://doi.org/10.1109/LLM4Code66737.2025.00005,03-03 May 2025
https://ieeexplore.ieee.org/document/10764990/,LLMs and Prompting for Unit Test Generation: A Large-Scale Evaluation,"['Wendkûuni C. Ouédraogo', 'Kader Kaboré', 'Yewei Song', 'Jacques Klein', 'Haoye Tian', 'Anil Koyuncu']",,"Unit testing, essential for identifying bugs, is often neglected due to time constraints. Automated test generation tools exist but typically lack readability and require developer intervention. Large Language Models (LLMs) like GPT and Mistral show potential in test generation, but their effectiveness remains unclear.This study evaluates four LLMs and five prompt engineering techniques, analyzing 216 300 tests for 690 Java classes from diverse datasets. We assess correctness, readability, coverage, and bug detection, comparing LLM-generated tests to EvoSuite. While LLMs show promise, improvements in correctness are needed. The study highlights both the strengths and limitations of LLMs, offering insights for future research.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11022958/,BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection,"['Shams Tarek', 'Dipayan Saha', 'Sujan Kumar Saha', 'Farimah Farahmandi']",,"The current landscape of system-on-chips (SoCs) security verification faces challenges due to manual, labor-intensive, and inflexible methodologies. These issues limit the scalability and effectiveness of security protocols, making bug detection at the Register-Transfer Level (RTL) difficult. This paper proposes a new framework named BugWhisperer that utilizes a specialized, fine-tuned Large Language Model (LLM) to address these challenges. By enhancing the LLM’s hardware security knowledge and leveraging its capabilities for text inference and knowledge transfer, this approach automates and improves the adaptability and reusability of the verification process. We introduce an open-source, fine-tuned LLM specifically designed for detecting security vulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM effectively enhances the efficiency and flexibility of the security verification process. Additionally, we introduce a comprehensive hardware vulnerability database that supports this work and will further assist the research community in enhancing the security verification process.",https://doi.org/10.1109/VTS65138.2025.11022958,28-30 April 2025
https://ieeexplore.ieee.org/document/11042761/,Intent-driven Web UI Tests Repair with LLM,"['Yingjie Tao', 'Weiwei Wang', 'Junxia Guo']",,"As web applications are frequently updated, changes may introduce to web elements in new versions, causing test cases to fail. Consequently, automatic web test repair techniques are proposed to reduce the cost of regression testing. Most existing methods focus on finding the correct candidate elements or related attributes to fix the broken test case. However, when test case failures are caused by test flow changes or propagated breakages, those methods that focus solely on matching the failing element in the new version cannot work well. Through empirical analysis, we found that the test intent and the reasons that caused the test failure are useful in test case reparation. This paper proposes a novel intent-driven web test repair approach named LetTe, which first parses the test intent and failure reasons of failed web UI tests, and then guides the Large Language Model (LLM) to fix them via prompt design and fine-tuning. LetTe’s repair logic is to simulate that of a human expert. According to the test intent and reason for failure, “think about” the possible repair plan and then generate repair candidates through the corresponding chain-of-thought. We evaluate LetTe on 7 web applications collected from open-source websites as well as publicly available datasets. The experimental results show that our approach has a 75% correct repair rate, which is higher than all baseline methods.",https://doi.org/10.1109/AEMCSE65292.2025.11042761,09-11 May 2025
https://ieeexplore.ieee.org/document/10556117/,(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs,"['Wanqin Ma', 'Chenyang Yang', 'Christian Kästner']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10556117,"Large Language Models (LLMs) are increasingly integrated into software applications. Downstream application developers often access LLMs through APIs provided as a service. However, LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices, as evidenced by our case study on toxicity detection. Based on our case study, we emphasize the need for and re-examine the concept of regression testing for evolving LLM APIs. We argue that regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.CCS CONCEPTS•Software and its engineering → Software testing and debugging.",,14-15 April 2024
https://ieeexplore.ieee.org/document/10845786/,Addressing Technical Challenges in Large Language Model-Driven Educational Software System,"['Nacha Chondamrongkul', 'Georgi Hristov', 'Punnarumol Temdee']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10845786,"The integration of large language models (LLMs) into educational systems poses significant challenges across several key attributes, including integration, explainability, testability, and scalability. These challenges arise from the complexity of coordinating system components, difficulty interpreting LLM decision-making processes, and the need for reliable, consistent model outputs in varied educational scenarios. Additionally, ensuring scalability requires robust autoscaling mechanisms and suitable architecture design to handle fluctuating workloads. This paper tackles these challenges by proposing tactics to improve system integration, enhance explainability through metadata and an algorithm process, ensure response consistency via regression testing, and facilitate efficient autoscaling through an event-driven microservice architecture. The evaluation results highlight the effectiveness of these tactics, confirming both functional consistency and robust system performance under varying loads.",https://doi.org/10.1109/ACCESS.2025.3531380,
https://ieeexplore.ieee.org/document/10771408/,On Enhancing Root Cause Analysis with SQL Summaries for Failures in Database Workload Replays at SAP HANA,"['Neetha Jambigi', 'Joshua Hammesfahr', 'Moritz Mueller', 'Thomas Bach', 'Michael Felderer']",,"Capturing the workload of a database and replaying this workload for a new version of the database can be an effective approach for regression testing. However, false positive errors caused by many factors such as data privacy limitations, time dependency or non-determinism in multi-threaded environment can negatively impact the effectiveness. Therefore, we employ a machine learning based framework to automate the root cause analysis of failures found during replays. However, handling unseen novel issues not found in the training data is one general challenge of machine learning approaches with respect to generalizability of the learned model. We describe how we continue to address this challenge for more robust long-term solutions. From our experience, retraining with new failures is inadequate due to features overlapping across distinct root causes. Hence, we leverage a large language model (LLM) to analyze failed SQL statements and extract concise failure summaries as an additional feature to enhance the classification process. Our experiments show the F1-Macro score improved by 4.77% for our data. We consider our approach beneficial for providing end users with additional information to gain more insights into the found issues and to improve the assessment of the replay results.",https://doi.org/10.1109/ISSREW63542.2024.00052,28-31 October 2024
https://ieeexplore.ieee.org/document/10366620/,The Causal Reasoning Ability of Open Large Language Model: A Comprehensive and Exemplary Functional Testing,"['Shun-Hang Li', 'Gang Zhou', 'Zhi-Bo Li', 'Ji-Cang Lu', 'Ning-Bo Huang']",,"As the intelligent software, the development and application of large language models are extremely hot topics recently, bringing tremendous changes to general AI and software industry. Nonetheless, large language models, especially open source ones, incontrollably suffer from some potential software quality issues such as instability, inaccuracy, and insecurity, making software testing necessary. In this paper, we propose the first solution for functional testing of open large language models to check full-scene availability and conclude empirical principles for better steering large language models, particularly considering their black box and intelligence properties. Specifically, we focus on the model’s causal reasoning ability, which is the core of artificial intelligence but almost ignored by most previous work. First, for comprehensive evaluation, we deconstruct the causal reasoning capability into five dimensions and summary the forms of causal reasoning task as causality identification and causality matching. Then, rich datasets are introduced and further modified to generate test cases along with different ability dimensions and task forms to improve the testing integrity. Moreover, we explore the ability boundary of open large language models in two usage modes: prompting and lightweight fine-tuning. Our work conducts comprehensive functional testing on the causal reasoning ability of open large language models, establishes benchmarks, and derives empirical insights for practical usage. The proposed testing solution can be transferred to other similar evaluation tasks as a general framework for large language models or their derivations.",https://doi.org/10.1109/QRS60937.2023.00032,22-26 October 2023
https://ieeexplore.ieee.org/document/10638557/,Intent-Driven Mobile GUI Testing with Autonomous Large Language Model Agents,"['Juyeon Yoon', 'Robert Feldt', 'Shin Yoo']",,"GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51 % for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 547 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.",https://doi.org/10.1109/ICST60714.2024.00020,27-31 May 2024
https://ieeexplore.ieee.org/document/10795054/,,[],,,,06-11 October 2024
https://ieeexplore.ieee.org/document/10765004/,,[],,,,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10812696/,,[],,,,
https://ieeexplore.ieee.org/document/10988978/,,[],,,,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/11029843/,Automating a Complete Software Test Process Using LLMs: An Automotive Case Study,"['Shuai Wang', 'Yinan Yu', 'Robert Feldt', 'Dhasarathy Parthasarathy']",,"Vehicle API testing verifies whether the interactions between a vehicle's internal systems and external applications meet expectations, ensuring that users can access and control various vehicle functions and data. However, this task is inherently complex, requiring the alignment and coordination of API systems, communication protocols, and even vehicle simulation systems to develop valid test cases. In practical industrial scenarios, inconsistencies, ambiguities, and interde-pendencies across various documents and system specifications pose significant challenges. This paper presents a system designed for the automated testing of in-vehicle APIs. By clearly defining and segmenting the testing process, we enable Large Language Models (LLMs) to focus on specific tasks, ensuring a stable and controlled testing workflow. Experiments conducted on over 100 APIs demonstrate that our system effectively automates vehicle API testing. The results also confirm that LLMs can efficiently handle mundane tasks requiring human judgment, making them suitable for complete automation in similar industrial contexts.",https://doi.org/10.1109/ICSE55347.2025.00211,26 April 2025 - 06 May 2025
https://ieeexplore.ieee.org/document/10548840/,CrashTranslator: Automatically Reproducing Mobile Application Crashes Directly from Stack Trace,"['Yuchao Huang', 'Junjie Wang', 'Zhe Liu', 'Yawen Wang', 'Song Wang', 'Chunyang Chen']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10548840,"Crash reports are vital for software maintenance since they allow the developers to be informed of the problems encountered in the mobile application. Before fixing, developers need to reproduce the crash, which is an extremely time-consuming and tedious task. Existing studies conducted the automatic crash reproduction with the natural language described reproducing steps. Yet we find a non-neglectable portion of crash reports only contain the stack trace when the crash occurs. Such stack-trace-only crashes merely reveal the last GUI page when the crash occurs, and lack step-by-step guidance. Developers tend to spend more effort in understanding the problem and reproducing the crash, and existing techniques cannot work on this, thus calling for a greater need for automatic support. This paper proposes an approach named CrashTranslator to automatically reproduce mobile application crashes directly from the stack trace. It accomplishes this by leveraging a pre-trained Large Language Model to predict the exploration steps for triggering the crash, and designing a reinforcement learning based technique to mitigate the inaccurate prediction and guide the search holistically. We evaluate CrashTranslator on 75 crash reports involving 58 popular Android apps, and it successfully reproduces 61.3% of the crashes, outperforming the state-of-the-art baselines by 109% to 206%. Besides, the average reproducing time is 68.7 seconds, out-performing the baselines by 302% to 1611%. We also evaluate the usefulness of CrashTranslator with promising results.",https://doi.org/10.1145/3597503.3623298,14-20 April 2024
https://ieeexplore.ieee.org/document/10740182/,RESTLess: Enhancing State-of-the-Art REST API Fuzzing With LLMs in Cloud Service Computing,"['Tao Zheng', 'Jiang Shao', 'Jinqiao Dai', 'Shuyu Jiang', 'Xingshu Chen', 'Changxiang Shen']",,"REST API Fuzzing is an emerging approach for automated vulnerability detection in cloud services. However, existing SOTA fuzzers face challenges in generating lengthy sequences comprising high-semantic requests, so that they may hardly trigger hard-to-reach states within a cloud service. To overcome this problem, we propose RESTLess, a flexible and efficient approach with hybrid optimization strategies for REST API fuzzing enhancement. Specifically, to pass the cloud gateway syntax semantic checking, we construct a dataset of valid parameters of REST API with Large Language Model named RTSet, then utilize it to develop an efficient REST API specification semantic enhancement approach. To detect vulnerability hidden under complex API operations, we design a flexible parameter rendering order optimization algorithm to increase the length and type of request sequences. Evaluation results highlight that RESTLess manifests noteworthy enhancements in the semantic quality of generated sequences in comparison to existing tools, thereby augmenting their capabilities in detecting vulnerabilities effectively. We also apply RESTLess to nine real-world cloud service such as Microsoft Azure, Amazon Web Services, Google Cloud, etc., and detecte 38 vulnerabilities, of which 16 have been confirmed and fixed by the relevant vendors.",https://doi.org/10.1109/TSC.2024.3489441,
https://ieeexplore.ieee.org/document/10391027/,AI-Powered Software Testing: The Impact of Large Language Models on Testing Methodologies,"['Vahit Bayrı', 'Ece Demirel']",,"Software testing is a crucial aspect of the software development lifecycle, ensuring the delivery of high-quality, reliable, and secure software systems. With the advancements in Artificial Intelligence (AI) and Natural Language Processing (NLP), Large Language Models (LLMs) have emerged as powerful tools capable of understanding and processing natural language texts easly. This article investigates the application of AI-based software testing, with a specific focus on the impact of LLMs in traditional testing methodologies. Through a comprehensive review of relevant literature and SeturDigital’s 25 year testing experience, this article explores the potential benefits, challenges, and prospects of integrating LLMs into software testing.",https://doi.org/10.1109/IISEC59749.2023.10391027,21-22 December 2023
https://ieeexplore.ieee.org/document/10734644/,Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs,"['Shengbei Jiang', 'Jiabao Zhang', 'Wei Chen', 'Bo Wang', 'Jianyi Zhou', 'Jie M. Zhang']",,"Automated debugging is an emerging research field that aims to automatically find and repair bugs. In this field, Fault Localization (FL) and Automated Program Repair (APR) gain the most research efforts. Most recently, researchers have adopted pre-trained Large Language Models (LLMs) to facilitate FL and APR and their results are promising. However, the LLMs they used either vanished (such as Codex) or outdated (such as early versions of GPT). In this paper, we evaluate the performance of recent commercial closed-source general-purpose LLMs on FL and APR, i.e., ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0. We select three popular LLMs and evaluate them on 120 real-world Java bugs from the benchmark Defects4J. For FL and APR, we designed three kinds of prompts for each, considering different kinds of information. The results show that these LLMs could successfully locate 53.3% and correctly fix 12.5% of these bugs.CCS CONCEPTS• Software and its engineering → Search-based software engineering; Software testing and debugging.",,20-20 April 2024
https://ieeexplore.ieee.org/document/10298349/,From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining,"['Xiaoxue Ren', 'Xinyuan Ye', 'Dehai Zhao', 'Zhenchang Xing', 'Xiaohu Yang']",,"Large Language Models (LLMs) have shown promising results in automatic code generation by improving coding efficiency to a certain extent. However, generating high-quality and reliable code remains a formidable task because of LLMs' lack of good programming practice, especially in exception handling. In this paper, we first conduct an empirical study and summarize three crucial challenges of LLMs in exception handling, i.e., incomplete exception handling, incorrect exception handling and abuse of try-catch. We then try prompts with different granularities to address such challenges, finding fine-grained knowledge-driven prompts works best. Based on our empirical study, we propose a novel Knowledge-driven Prompt Chaining-based code generation approach, name KPC, which decomposes code generation into an AI chain with iterative check-rewrite steps and chains fine-grained knowledge-driven prompts to assist LLMs in considering exception-handling specifications. We evaluate our KPC-based approach with 3,079 code generation tasks extracted from the Java official API documentation. Extensive experimental results demonstrate that the KPC-based approach has considerable potential to ameliorate the quality of code generated by LLMs. It achieves this through proficiently managing exceptions and obtaining remarkable enhancements of 109.86% and 578.57% with static evaluation methods, as well as a reduction of 18 runtime bugs in the sampled dataset with dynamic validation.",https://doi.org/10.1109/ASE56229.2023.00143,11-15 September 2023
https://ieeexplore.ieee.org/document/10615269/,Test Code Generation for Telecom Software Systems Using Two-Stage Generative Model,"['Mohamad Nabeel', 'Doumitrou Daniil Nimara', 'Tahar Zanouda']",,"In recent years, the evolution of Telecom towards achieving intelligent, autonomous, and open networks has led to an increasingly complex Telecom Software system, supporting various heterogeneous deployment scenarios, with multi-standard and multi-vendor support. As a result, it becomes a challenge for large-scale Telecom software companies to develop and test software for all deployment scenarios. To address these challenges, we propose a framework for Automated Test Generation for large-scale Telecom Software systems. We begin by generating Test Case Input data for test scenarios observed using a time-series Generative model trained on historical Telecom Network data during field trials. Additionally, the time-series Generative model helps in preserving the privacy of Telecom data. The generated time-series software performance data are then utilized with test descriptions written in natural language to generate Test Script using the Generative Large Language Model. Our comprehensive experiments on public datasets and Telecom datasets obtained from operational Telecom Networks demonstrate that the framework can effectively generate comprehensive test case data input and useful test code.",https://doi.org/10.1109/ICCWorkshops59551.2024.10615269,09-13 June 2024
https://ieeexplore.ieee.org/document/10903339/,Exploring Testing Methods for Large Language Models,"['Timothy Elvira', 'Tyler Thomas Procko', 'Lynn Vonderhaar', 'Omar Ochoa']",,"Large Language Models (LLMs) are extensive aggregations of human language, designed to understand and generate sophisticated text. LLMs are becoming ubiquitous in a range of applications, from social media to code generation. With their immense size, LLMs face scalability challenges, making testing methods particularly difficult to implement effectively. Traditional machine learning and software testing methods, derived and adapted for LLMs, test these models to a point; however, they still struggle to accurately capture the full complexity of model behavior. This paper aims to capture the current efforts and techniques in testing LLMs, specifically focusing on stress testing, mutation testing, regression testing, metamorphic testing, and adversarial testing. This survey focuses on how traditional testing methods must be adapted to fit the needs of LLMs. Furthermore, while this area is fairly novel, there are still gaps in the literature that have been identified for future research.",https://doi.org/10.1109/ICMLA61862.2024.00177,18-20 December 2024
https://ieeexplore.ieee.org/document/10684633/,Symbolic Execution with Test Cases Generated by Large Language Models,"['Jiahe Xu', 'Jingwei Xu', 'Taolue Chen', 'Xiaoxing Ma']",,"Symbolic execution is a powerful program analysis technique. External environment construction and internal path explosion are two long-standing problems which may affect the effectiveness and performance of symbolic execution on complex programs. The intrinsic challenge is to achieve a sufficient understanding of the program context to construct a set of execution environments which can guide the selection of symbolic states. In this paper, we propose a novel program-context-guided symbolic execution framework LangSym based on program’s instruction/user manual. Leveraging the capabilities of natural language understanding and code generation in large language models (LLMs), LangSym can automatically extract the knowledge related to the functionality of the program, and generate adequate test cases and the corresponding environments as the prior knowledge for symbolic execution. We instantiate LangSym in KLEE, a widely adopted symbolic execution engine, to build a pipeline that could automatically leverage LLMs to boost the symbolic execution. We evaluate LangSym on almost all GNU Coreutils programs and considerable large-scale programs, showing that LangSym outperforms the existing strategies in KLEE with at least a 10% increase for line coverage.",https://doi.org/10.1109/QRS62785.2024.00031,01-05 July 2024
https://ieeexplore.ieee.org/document/10638618/,KAT: Dependency-Aware Automated API Testing with Large Language Models,"['Tri Le', 'Thien Tran', 'Duy Cao', 'Vy Le', 'Tien N. Nguyen', 'Vu Nguyen']",,"API testing has increasing demands for software companies. Prior API testing tools were aware of certain types of dependencies that needed to be concise between operations and parameters. However, their approaches, which are mostly done manually or using heuristic-based algorithms, have limitations due to the complexity of these dependencies. In this paper, we present KAT (Katalon API Testing), a novel AI -driven approach that leverages the large language model GPT in conjunction with advanced prompting techniques to autonomously generate test cases to validate RESTful APIs. Our comprehensive strategy encompasses various processes to construct an operation depen-dency graph from an OpenAPI specification and to generate test scripts, constraint validation scripts, test cases, and test data. Our evaluation of KAT using 12 real-world RESTful services shows that it can improve test coverage, detect more undocumented status codes, and reduce false positives in these services in comparison with a state-of-the-art automated test generation tool. These results indicate the effectiveness of using the large language model for generating test scripts and data for API testing.",https://doi.org/10.1109/ICST60714.2024.00017,27-31 May 2024
https://ieeexplore.ieee.org/document/10609742/,Assessing Evaluation Metrics for Neural Test Oracle Generation,"['Jiho Shin', 'Hadi Hemmati', 'Moshi Wei', 'Song Wang']",,"Recently, deep learning models have shown promising results in test oracle generation. Neural Oracle Generation (NOG) models are commonly evaluated using static (automatic) metrics which are mainly based on textual similarity of the output, e.g. BLEU, ROUGE-L, METEOR, and Accuracy. However, these textual similarity metrics may not reflect the testing effectiveness of the generated oracle within a test suite, which is often measured by dynamic (execution-based) test adequacy metrics such as code coverage and mutation score. In this work, we revisit existing oracle generation studies plus gpt-3.5 to empirically investigate the current standing of their performance in textual similarity and test adequacy metrics. Specifically, we train and run four state-of-the-art test oracle generation models on seven textual similarity and two test adequacy metrics for our analysis. We apply two different correlation analyses between these two different sets of metrics. Surprisingly, we found no significant correlation between the textual similarity metrics and test adequacy metrics. For instance, gpt-3.5 on the jackrabbit-oak project had the highest performance on all seven textual similarity metrics among the studied NOGs. However, it had the lowest test adequacy metrics compared to all the studied NOGs. We further conducted a qualitative analysis to explore the reasons behind our observations. We found that oracles with high textual similarity metrics but low test adequacy metrics tend to have complex or multiple chained method invocations within the oracle's parameters, making them hard for the model to generate completely, affecting the test adequacy metrics. On the other hand, oracles with low textual similarity metrics but high test adequacy metrics tend to have to call different assertion types or a different method that functions similarly to the ones in the ground truth. Overall, this work complements prior studies on test oracle generation with an extensive performance ev...",https://doi.org/10.1109/TSE.2024.3433463,
https://ieeexplore.ieee.org/document/10764870/,AgoneTest: Automated creation and assessment of Unit tests leveraging Large Language Models,"['Andrea Lops', 'Fedelucio Narducci', 'Azzurra Ragone', 'Michelantonio Trizio']",,"Software correctness is crucial, with unit testing playing an indispensable role in the software development lifecycle. However, creating unit tests is time-consuming and costly, underlining the need for automation. Leveraging Large Language Models (LLMs) for unit test generation is a promising solution, but existing studies focus on simple, small-scale scenarios, leaving a gap in understanding LLMs’ performance in real-world applications, particularly regarding integration and assessment efficacy at scale. Here, we present AgoneTest, a system focused on automatically generating and evaluating complex class-level test suites. Our contributions include a scalable automated system, a newly developed dataset for rigorous evaluation, and a detailed methodology for test quality assessment.CCSCONCEPTS• Software and its engineering → Automatic programming; Software testing and debugging.",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/11025195/,"Exploration of Course Practice on the Integration of AI Language Model and Ideological and Political Education : The Course of ""Software Testing"" as an Example","['Chang Liu', 'Yan Jiang', 'Dongxia Zheng']",,"With the rapid development of information technology, artificial intelligence technology, and digital technology, cutting-edge technologies such as big data, intelligent algorithms, cloud computing, the Internet of Things, and blockchain are gradually penetrating into the field of education, promoting the modernization and digitization of education. In this context, software testing courses, as important courses in computer science and technology and software engineering majors, are facing new challenges and opportunities. In response to the mismatch between the curriculum teaching system and the new demands of the artificial intelligence era, it is proposed to focus on artificial intelligence technology, optimize the curriculum system from multiple dimensions, introduce AI language models to drive teaching, and pay attention to the deep integration of ideological and political education in the curriculum. This article demonstrates how to integrate ideological and political elements into software testing courses through specific cases. Practice has proven that AI driven courses can better stimulate students' interest in learning, help improve their ideological level, technical ability, and industry competitiveness.",https://doi.org/10.1109/ICISE-IE64355.2024.11025195,20-22 December 2024
https://ieeexplore.ieee.org/document/10298360/,Towards Autonomous Testing Agents via Conversational Large Language Models,"['Robert Feldt', 'Sungmin Kang', 'Juyeon Yoon', 'Shin Yoo']",,"Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized “hallucination” of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.",https://doi.org/10.1109/ASE56229.2023.00148,11-15 September 2023
https://ieeexplore.ieee.org/document/10638604/,Poster: gptCombFuzz: Combinatorial Oriented LLM Seed Generation for effective Fuzzing,"['Darshan Lohiya', 'Monika Rani Golla', 'Sangharatna Godboley', 'P. Radha Krishna']",,"The important contribution that large language models (LLMs) have made to the development of a new software testing era is the main objective of this proposed approach. It emphasizes the role that LLMs play in producing complex and diverse input seeds, which opens the way for efficient bug discovery. In the study we also introduce a systematic approach for combining various input values, employing the principles of Combinatorial testing using the PICT (Pairwise independent Combinatorial testing). By promoting a more varied set of inputs for thorough testing, PICT enhances the seed production process. Then we show how these different seeds may be easily included in the American Fuzzy Lop (AFL) tool, demonstrating how AFL can effectively use them to find and detect software flaws. This integrated technique offers a powerful yet straightforward approach to software Quality.",https://doi.org/10.1109/ICST60714.2024.00048,27-31 May 2024
https://ieeexplore.ieee.org/document/10962470/,,[],,,,
https://ieeexplore.ieee.org/document/10430067/,,[],,,,
https://ieeexplore.ieee.org/document/11025799/,,[],,,,
https://ieeexplore.ieee.org/document/11023907/,,[],,,,
https://ieeexplore.ieee.org/document/10675912/,,[],,,https://doi.org/10.1109/ICSTW60967.2024.00032,27-31 May 2024
https://ieeexplore.ieee.org/document/10719045/,,[],,,https://doi.org/10.1109/ICSESS62520.2024.10719045,13-14 September 2024
https://ieeexplore.ieee.org/document/10638599/,,[],,,https://doi.org/10.1109/ICST60714.2024.00019,27-31 May 2024
https://ieeexplore.ieee.org/document/10765066/,,[],,,,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10638611/,Improving Patch Correctness Analysis via Random Testing and Large Language Models,"['Facundo Molina', 'Juan Manuel Copia', 'Alessandra Gorla']",,"Patch correctness assessment represents a crucial step in the patch validation process, with the potential to enhance the practical adoption of automated program repair (APR) techniques and substantially reduce validation costs. While some automated techniques have been proposed for assessing patch correctness, they primarily focus on either ranking patches based on their likelihood of being correct or classifying them as correct or incorrect without offering any further explanatory information. In this paper, we introduce FIXCHECK, a novel approach that combines random testing and large language models to automatically generate fault-revealing tests for potentially incorrect patches. To achieve this, FIXCHECK employs a two-fold process: Firstly, a random testing procedure generates a comprehensive set of test cases. Secondly, a large language model is utilized to derive meaningful assertions for each test case. Additionally, FIXCHECK incorporates a selection and prioritization mechanism, which evaluates the generated tests executed on the patched program and discards or ranks them based on their likelihood of revealing faults in the patch. To assess the effectiveness of our approach, we conducted evaluations on a benchmark comprising 160 patches, encompassing both patches created by developers and patches generated by APR tools. The results demonstrate that FIXCHECK effectively generates fault-revealing tests for 62 % of incorrect patches written by developers, with a high level of confidence. Furthermore, it complements existing patch correctness assessment techniques by providing fault-revealing tests for up to 50% of the incorrect patches identified by state-of-the-art techniques.",https://doi.org/10.1109/ICST60714.2024.00036,27-31 May 2024
https://ieeexplore.ieee.org/document/10123585/,Large Language Models: The Next Frontier for Variable Discovery within Metamorphic Testing?,"['Christos Tsigkanos', 'Pooja Rani', 'Sebastian Müller', 'Timo Kehrer']",,"Metamorphic testing involves reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables. A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation, a strategy particularly fitting to testing scientific software. However, such software typically has large input-output spaces, and the fundamental prerequisite – extracting variables of interest – is an arduous and non-scalable process when performed manually. To this end, we devise a workflow around an autoregressive transformer-based Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software. Our end-to-end approach, besides a prompt specification consisting of few-shot examples by a human user, is fully automated, in contrast to current practice requiring human intervention. We showcase our LLM workflow over a real case, and compare variables extracted to ground truth manually labelled by experts. Our preliminary results show that our LLM-based workflow achieves an accuracy of 0.87, while successfully deriving 61.8% of variables as partial matches and 34.7% as exact matches.",https://doi.org/10.1109/SANER56733.2023.00070,21-24 March 2023
https://ieeexplore.ieee.org/document/10556182/,Seven Failure Points When Engineering a Retrieval Augmented Generation System,"['Scott Barnett', 'Stefanus Kurniawan', 'Srikanth Thudumu', 'Zach Brannelly', 'Mohamed Abdelrazek']",,"Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.CCS CONCEPTS• Software and its engineering → Empirical software validation.",,14-15 April 2024
https://ieeexplore.ieee.org/document/10962487/,Quality Assurance for LLM-RAG Systems: Empirical Insights from Tourism Application Testing,"['Bestoun S. Ahmed', 'Ludwig Otto Baader', 'Firas Bayram', 'Siri Jagstedt', 'Peter Magnusson']",,"This paper presents a comprehensive framework for testing and evaluating quality characteristics of Large Language Model (LLM) systems enhanced with Retrieval-Augmented Generation (RAG) in tourism applications. Through systematic empirical evaluation of three different LLM variants across multiple parameter configurations, we demonstrate the effectiveness of our testing methodology in assessing both functional correctness and extra-functional properties. Our framework implements 17 distinct metrics that encompass syntactic analysis, semantic evaluation, and behavioral evaluation through LLM judges. The study reveals significant information about how different architectural choices and parameter configurations affect system performance, particularly highlighting the impact of temperature and top-p parameters on response quality. The tests were carried out on a tourism recommendation system for the Varmland region in Sweden, utilizing standard and RAG-enhanced configurations. The results indicate that the newer LLM versions show modest improvements in performance metrics, though the differences are more pronounced in response length and complexity rather than in semantic quality. The research contributes practical insights for implementing robust testing practices in LLM-RAG systems, providing valuable guidance to organizations deploying these architectures in production environments.",https://doi.org/10.1109/ICSTW64639.2025.10962487,31 March 2025 - 04 April 2025
https://ieeexplore.ieee.org/document/10796866/,Automated Test Case Generation for Satellite FRD Using NLP and Large Language Model,"['Shakthi S', 'Pratibha Srivastava', 'Ravi Kumar L', 'SG Prasad']",,"In recent times, the research on the use of Large Language Models (LLMs) for developing software applications has grown exponentially. Generating test cases for satellite Functional Requirement Documents (FRDs) pose a significant challenge due to their complex nature, requiring intricate analysis. Manual methods are time-consuming and error-prone, prompting the need for automated solutions or semi-automated solutions. This work proposes a novel approach to automate test case generation from FRDs using LLMs and Natural Language Processing (NLP). By harnessing the capabilities of LLMs, our system extracts and interprets complex variables and equations, facilitating the automated creation of comprehensive test cases. This approach aims to streamline the satellite testing process, improving efficiency and accuracy while reducing the burden on human analysts. We generate a custom dataset of 10 samples and then benchmark 4 LLMs on the dataset. We open-source the complete codebase for implementation and for further research.",https://doi.org/10.1109/ICECCME62383.2024.10796866,04-06 November 2024
https://ieeexplore.ieee.org/document/10500073/,Requirements Verification Through the Analysis of Source Code by Large Language Models,"['Juan Ortiz Couder', 'Dawson Gomez', 'Omar Ochoa']",,"In the most recent years, Large Language Models (LLMs) have gained popularity and have been accepted and used in different domains due to their ability to understand and generate written language. LLMs allow us to analyze large amounts of data in a few moments, yet they are also extremely simple to use, making them a very powerful assistive tool that can aid in a wide range of tasks; from planning a family trip, to aid during the development process of a huge system. For software developers, LLMs have been mostly used for code generation, explanation, or optimization. Software verification is a crucial part of software development as it is the process of ensuring that a system meets specific requirements. Requirements specifications play a pivotal role in software verification as they define what a system should do. In this paper we propose the use of LLMs for code verification through the analysis of requirements specifications. We prove that LLMs, such as GPT-3.5, can verify a list of requirements through a given code and evaluate why the requirements have or have not been met.",https://doi.org/10.1109/SoutheastCon52093.2024.10500073,15-24 March 2024
https://ieeexplore.ieee.org/document/10298442/,SMT Solver Validation Empowered by Large Pre-Trained Language Models,"['Maolin Sun', 'Yibiao Yang', 'Yang Wang', 'Ming Wen', 'Haoxiang Jia', 'Yuming Zhou']",,"SMT solvers are utilized to check the satisfiability of logic formulas and have been applied in various crucial domains, including software verification, test case generation, and program synthesis. However, bugs hidden in SMT solvers can lead to severe consequences, causing erroneous results in these domains. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Despite several testing approaches proposed for SMT solvers, generating effective test formulas to comprehensively test SMT solvers remains a challenge. To address this challenge, in this study, we propose to port large language models (LLMs) to generate SMT formulas for fuzzing solvers. Specifically, the study presents a novel retrain-finetune pipeline to unleash the potential of language models to generate effective SMT formulas and improve their generation performance through data augmentation. We implemented our approach as a practical fuzzing tool, named LasT,and then extensively tested the state-of-the-art SMT solvers, namely Z3, cvc5, and Bitwuzla. To date, Last has successfully uncovered 65 genuine bugs for the solvers, of which 45 have been fixed by the developers.",https://doi.org/10.1109/ASE56229.2023.00180,11-15 September 2023
https://ieeexplore.ieee.org/document/10605166/,AutoTest: Evolutionary Code Solution Selection with Test Cases,"['Zhihua Duan', 'Jialin Wang']",,"With the development of code generation techniques, selecting the correct code solution from multiple candidate solutions has become a crucial task. This study proposes AutoTest, a novel technique that combines automated test case generation with code solution execution to optimize the selection process using an evolutionary genetic algorithm. Firstly, AutoTest utilizes large pre-trained language models such as codegen-16B, code-davinci-002, and incoder-6B to provide code solutions and their corresponding test cases. Then, by executing the code solutions and evaluating their performance on the test cases, a consensus set is formed. Fine-grained ranking is achieved through the selection, mutation, and crossover mechanisms based on the evolutionary genetic algorithm, with the adjustment of alpha and beta parameters. Finally, the best code solution is chosen. AutoTest demonstrates significant performance improvements on the HumanEval benchmark test. The HumanEval dataset consists of 164 programming problems, and AutoTest achieves approximately a 10% improvement over the baseline method in terms of pass@1 score.",https://doi.org/10.1109/CSCloud62866.2024.00030,28-30 June 2024
https://ieeexplore.ieee.org/document/10659742/,Leveraging Large Language Model for Automatic Patch Correctness Assessment,"['Xin Zhou', 'Bowen Xu', 'Kisub Kim', 'DongGyun Han', 'Hung Huu Nguyen', 'Thanh Le-Cong']",,"Automated Program Repair (APR) techniques have shown more and more promising results in fixing real-world bugs. Despite the effectiveness, APR techniques still face an overfitting problem: a generated patch can be incorrect although it passes all tests. It is time-consuming to manually evaluate the correctness of generated patches that can pass all available test cases. To address this problem, many approaches have been proposed to automatically assess the correctness of patches generated by APR techniques. These approaches are mainly evaluated within the cross-validation setting. However, for patches generated by a new or unseen APR tool, users are implicitly required to manually label a significant portion of these patches (e.g., 90% in 10-fold cross-validation) in the cross-validation setting before inferring the remaining patches (e.g., 10% in 10-fold cross-validation). To mitigate the issue, in this study, we propose LLM4PatchCorrect, the patch correctness assessment by adopting a large language model for code. Specifically, for patches generated by a new or unseen APR tool, LLM4PatchCorrect does not need labeled patches of this new or unseen APR tool for training but directly queries the large language model for code to get predictions on the correctness labels without training. In this way, LLM4PatchCorrect can reduce the manual labeling effort when building a model to automatically assess the correctness of generated patches of new APR tools. To provide knowledge regarding the automatic patch correctness assessment (APCA) task to the large language model for code, LLM4PatchCorrect leverages bug descriptions, execution traces, failing test cases, test coverage, and labeled patches generated by existing APR tools, before deciding the correctness of the unlabeled patches of a new or unseen APR tool. Additionally, LLM4PatchCorrect prioritizes labeled patches from existing APR tools that exhibit semantic similarity to those generated by new APR tools, enhancing t...",https://doi.org/10.1109/TSE.2024.3452252,
https://ieeexplore.ieee.org/document/10765035/,JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models,"['Jialun Cao', 'Zhiyong Chen', 'Jiarong Wu', 'Shing-Chi Cheung', 'Chang Xu']",,"Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs’ capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8% of benchmarks involve Python, while only 5 benchmarks involve Java, resulting in an insufficient understanding of LLMs’ capability to generate Java code. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills (e.g., variables, operators, and control structures), while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism). Considering the prevalence of these advanced features in real-world Java project development, constructing benchmarks to test LLMs on handling OOP features is necessary.To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM’s capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 48.24% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an idea...",,27 October 2024 - 01 November 2024
https://ieeexplore.ieee.org/document/10967319/,A Study of Using Multimodal LLMs for Non-Crash Functional Bug Detection in Android Apps,"['Bangyan Ju', 'Jin Yang', 'Tingting Yu', 'Tamerlan Abdullayev', 'Yuanyuan Wu', 'Dingbang Wang']",,"Numerous approaches employing various strategies have been developed to test the graphical user interfaces (GUIs) of mobile apps. However, traditional GUI testing techniques, such as random and model-based testing, primarily focus on generating test sequences that excel in achieving high code coverage but often fail to act as effective test oracles for noncrash functional (NCF) bug detection. To tackle these limitations, this study empirically investigates the capability of leveraging large language models (LLMs) to be test oracles to detect NCF bugs in Android apps. Our intuition is that the training corpora of LLMs, encompassing extensive mobile app usage and bug report descriptions, enable them with the domain knowledge relevant to NCF bug detection. We conducted a comprehensive empirical study to explore the effectiveness of LLMs as test oracles for detecting NCF bugs in Android apps on 71 welldocumented NCF bugs. The results demonstrated that LLMs achieve a 49% bug detection rate, outperforming existing tools for detecting NCF bugs in Android apps. Additionally, by leveraging LLMs to be test oracles, we successfully detected 24 previously unknown NCF bugs in 64 Android apps, with four of these bugs being confirmed or fixed. However, we also identified limitations of LLMs, primarily related to performance degradation, inherent randomness, and false positives. Our study highlights the potential of leveraging LLMs as test oracles for Android NCF bug detection and suggests directions for future research.",https://doi.org/10.1109/APSEC65559.2024.00017,03-06 December 2024
https://ieeexplore.ieee.org/document/10772249/,MoCo: Fuzzing Deep Learning Libraries via Assembling Code,"['Pin Ji', 'Yang Feng', 'Duo Wu', 'Lingyue Yan', 'Penglin Chen', 'Jia Liu']",,"The rapidly developing Deep Learning (DL) techniques have been applied in software systems of various types. However, they can also pose new safety threats with potentially serious consequences, especially in safety-critical domains. DL libraries serve as the underlying foundation for DL systems, and bugs in them can have unpredictable impacts that directly affect the behaviors of DL systems. Previous research on fuzzing DL libraries still has limitations in generating tests corresponding to crucial testing scenarios and constructing test oracles. In this paper, we propose MoCo, a novel fuzzing testing method for DL libraries via assembling code. The seed tests used by MoCo are code files that implement DL models, covering both model construction and training in the most common real-world application scenarios for DL libraries. MoCo first disassembles the seed code files to extract templates and code blocks, then applies code block mutation operators (e.g., API replacement, random generation, and boundary checking) to generate new code blocks that fit the template. To ensure the correctness of the code block mutation, we employ the Large Language Model to parse the official documents of DL libraries for information about the parameters and the constraints between them. By inserting context-appropriate code blocks into the template, MoCo can generate a tree of code files with intergenerational relations. According to the derivation relations in this tree, we construct the test oracle based on the execution state consistency and the calculation result consistency. Since the granularity of code assembly is controlled rather than randomly divergent, we can quickly pinpoint the lines of code where the bugs are located and the corresponding triggering conditions. We conduct a comprehensive experiment to evaluate the efficiency and effectiveness of MoCo using three widely-used DL libraries (i.e., TensorFlow, PyTorch, and Jittor). During the experiments, MoCo detects 77 new...",https://doi.org/10.1109/TSE.2024.3509975,
https://ieeexplore.ieee.org/document/10644000/,"PyBugHive: A Comprehensive Database of Manually Validated, Reproducible Python Bugs","['Gábor Antal', 'Norbert Vándor', 'István Kolláth', 'Balázs Mosolygó', 'Péter Hegedűs', 'Rudolf Ferenc']",https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10644000,"Python is currently the number one language in the TIOBE index and has been the second most popular language on GitHub for years. But so far, there are only a few bug databases that contain bugs for Python projects and even fewer in which bugs can be reproduced. In this paper, we present a manually curated database of reproducible Python bugs called PyBugHive. The initial version of PyBugHive is a benchmark of 149 real, manually validated bugs from 11 Python projects. Each entry in our database contains the summary of the bug report, the corresponding patch, and the test cases that expose the given bug. PyBugHive features a rich command line interface for accessing both the buggy and fixed versions of the programs and provides the abstraction for executing the corresponding test cases. The interface facilitates highly reproducible empirical research and tool comparisons in fields such as testing, automated program repair, or bug prediction. The usage of our database is demonstrated through a use case involving a large language model, GPT-3.5. First, we evaluated the bug detection capabilities of the model with the help of the bug repository. Using multiple prompts, we found out that GPT-3.5 was able to detect 67 out of 149 bugs (45%). Furthermore, we leveraged the constructed bug dataset in assessing the automatic program repair capabilities of GPT-3.5 by comparing the generated fixes with the real patches contained in the dataset. However, its performance was far worse in this task compared to bug detection, as it was able to fix only one of the detected issues.",https://doi.org/10.1109/ACCESS.2024.3449106,
https://ieeexplore.ieee.org/document/11029933/,Ranking Relevant Tests for Order-Dependent Flaky Tests,"['Shanto Rahman', 'Bala Naren Chanumolu', 'Suzzana Rafi', 'August Shi', 'Wing Lam']",,"One major challenge of regression testing are flaky tests, i.e., tests that may pass in one run but fail in another run for the same version of code. One prominent category of flaky tests is order-dependent (OD) flaky tests, which can pass or fail depending on the order in which the tests are run. To help developers debug and fix OD tests, prior work attempts to automatically find OD-relevant tests, which are tests that determine whether an OD test passes or fails, depending on whether the OD-relevant tests run before or after the OD test. Prior work found OD-relevant tests by running different tests before the OD test, without considering each test's likelihood of being OD-relevant tests. We propose RankF to rank tests in order of likelihood of being OD-relevant tests, finding the first OD-relevant test for a given OD test more quickly. We propose two ranking approaches, each requiring different information. Our first approach, RankFL
, relies on training a large-language model to analyze test code. Our second approach, RankFO
, relies on analyzing prior test-order execution information. We evaluate our approaches on 155 OD tests across 24 open-source projects. We compare RankF against baselines from prior work, where we find that RankF finds the first OD-relevant test for an OD test faster than the best baseline; depending on the type of OD-relevant test, RankF takes 9.4 to 14.1 seconds on median, compared to the baseline's 34.2 to 118.5 seconds on median.",https://doi.org/10.1109/ICSE55347.2025.00178,26 April 2025 - 06 May 2025
