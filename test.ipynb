{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d711032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from crawl_tools.acm_crawler import ACMCrawler\n",
    "from crawl_tools.arxiv_crawler import ArxivCrawler\n",
    "from crawl_tools.ieee_crawler import IEEECrawler\n",
    "from crawl_tools.mdpi_crawler import MDPICrawler\n",
    "from crawl_tools.science_direct_crawler import ScienceDirectCrawler\n",
    "from crawl_tools.springer_crawler import SpringerCrawler\n",
    "from filter_tools.content_analysis import ContentAnalyzer\n",
    "from filter_tools.download_papers import PaperDownloader\n",
    "from filter_tools.keywords_filter_paper import KeywordFilter\n",
    "\n",
    "# ======== GLOBAL CONFIGURATION VARIABLES ========\n",
    "# These variables can be modified to test specific parts of the pipeline\n",
    "\n",
    "# Directory structure\n",
    "BASE_OUTPUT_DIR = \"output\"\n",
    "CRAWL_SOURCE = \"arxiv\"  # Change this to test different crawlers\n",
    "INPUT_DIR = os.path.join(BASE_OUTPUT_DIR, CRAWL_SOURCE)\n",
    "FILTERED_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, f\"filtered_{CRAWL_SOURCE}\")\n",
    "DOWNLOAD_DIR = os.path.join(FILTERED_OUTPUT_DIR, \"downloaded_papers\")\n",
    "ANALYSIS_DIR = os.path.join(FILTERED_OUTPUT_DIR, \"analysis\")\n",
    "SUMMARY_DIR = os.path.join(FILTERED_OUTPUT_DIR, \"summary\")\n",
    "\n",
    "# Ensure directories exist\n",
    "for directory in [INPUT_DIR, FILTERED_OUTPUT_DIR, DOWNLOAD_DIR, ANALYSIS_DIR, SUMMARY_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Keywords for crawling\n",
    "KEYWORD_SETS = {\n",
    "    'functional_testing': ['\"Functional Testing\"',\n",
    "                          '\"Software Testing\"',\n",
    "                          '\"Test Case Generation\"',\n",
    "                          '\"Test Data Generation\"',\n",
    "                          '\"Test Automation Frameworks\"',\n",
    "                          '\"UI (User Interface) Testing\"',\n",
    "                          '\"API (Application Programming Interface) Testing\"',\n",
    "                          '\"Test Oracle Problem\"',\n",
    "                          '\"Test Coverage\"',\n",
    "                          '\"Test Maintenance\"',\n",
    "                          '\"Bug Detection\"',\n",
    "                          '\"Software Quality Assurance\" (SQA)',\n",
    "                          '\"Regression Testing\"'],\n",
    "    'llm': ['\"llm\"', '\"large language model\"']\n",
    "}\n",
    "\n",
    "# Keywords for filtering\n",
    "FILTER_KEYWORDS = [\n",
    "    \"software testing\", \"test automation\", \"llm\", \"large language model\",\n",
    "    \"ai agent\", \"prompt engineering\", \"test case generation\", \"functional testing\",\n",
    "    \"test oracle\", \"test coverage\", \"bug detection\", \"regression testing\"\n",
    "]\n",
    "\n",
    "# API keys for content analysis (leave empty to use environment variable)\n",
    "API_KEYS = []\n",
    "\n",
    "# Similarity thresholds for keyword matching\n",
    "SIMILARITY_THRESHOLDS = {\n",
    "    \"exact\": 100,\n",
    "    \"high\": 90,\n",
    "    \"medium\": 75,\n",
    "    \"low\": 65\n",
    "}\n",
    "\n",
    "# Paper download limits\n",
    "MAX_PAPERS_TO_DOWNLOAD = 20\n",
    "PAPERS_START_DATE = \"2024-01-01\"  # Focus on recent papers\n",
    "\n",
    "# Crawler configuration\n",
    "CRAWL_CONFIG = {\n",
    "    'headless': False,     \n",
    "    'max_threads': 4,          \n",
    "    'use_multithreading': True \n",
    "}\n",
    "\n",
    "# File paths for specific crawl outputs\n",
    "ALL_PAPERS_CSV = os.path.join(INPUT_DIR, f\"all_{CRAWL_SOURCE}_papers.csv\")\n",
    "SUMMARY_FILE = os.path.join(SUMMARY_DIR, \"functional_testing_llm_true.csv\")\n",
    "\n",
    "# Helper variables for simulation when cells aren't run\n",
    "filtered_papers_fallback = [f for f in os.listdir(INPUT_DIR) \n",
    "                          if f.endswith('.csv') and f.startswith('crawl_by_')] if os.path.exists(INPUT_DIR) else []\n",
    "filtered_papers_fallback = [os.path.join(INPUT_DIR, f) for f in filtered_papers_fallback]\n",
    "\n",
    "# Helper function to check and get variables without errors\n",
    "def get_var(var_name, default=None):\n",
    "    \"\"\"Helper function to safely get variables\"\"\"\n",
    "    if var_name in globals():\n",
    "        return globals()[var_name]\n",
    "    else:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d07b5695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã ƒê√£ c·∫•u h√¨nh 6 crawler tools\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"crawl_results\"\n",
    "max_threads = 4\n",
    "headless = False\n",
    "keyword_sets = {\n",
    "    'functional testing': ['\"Functional Testing\"',\n",
    "'\"Software Testing\"',\n",
    "'\"Test Case Generation\"',\n",
    "'\"Test Data Generation\"',\n",
    "'\"Test Automation Frameworks\"',\n",
    "'\"UI (User Interface) Testing\"',\n",
    "'\"API (Application Programming Interface) Testing\"',\n",
    "'\"Test Oracle Problem\"',\n",
    "'\"Test Coverage\"',\n",
    "'\"Test Maintenance\"',\n",
    "'\"Bug Detection\"',\n",
    "'\"Software Quality Assurance\" (SQA)',\n",
    "'\"Regression Testing\"'],\n",
    "    'llm': ['\"llm\"', '\"large language model\"']\n",
    "}\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "CRAWL_CONFIG = {\n",
    "    'headless': False,     \n",
    "    'max_threads': 4,          \n",
    "    'use_multithreading': True \n",
    "}\n",
    "CRAWLERS = {\n",
    "    'acm': {\n",
    "        'output_dir': 'output/acm'\n",
    "    },\n",
    "    'arxiv': {\n",
    "        'output_dir': 'output/arxiv'\n",
    "    },\n",
    "    'ieee': {\n",
    "        'output_dir': 'output/ieee'\n",
    "    },\n",
    "    'mdpi': {\n",
    "        'output_dir': 'output/mdpi'\n",
    "    },\n",
    "    'science_direct': {\n",
    "        'output_dir': 'output/science_direct'\n",
    "    },\n",
    "    'springer': {\n",
    "        'output_dir': 'output/springer'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìã ƒê√£ c·∫•u h√¨nh {len(CRAWLERS)} crawler tools\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4bde3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ArxivCrawler(\n",
    "            headless=CRAWL_CONFIG['headless'],\n",
    "            output_dir=CRAWLERS['arxiv']['output_dir'],\n",
    "            max_threads=CRAWL_CONFIG['max_threads'],\n",
    "            keyword_sets=keyword_sets\n",
    "        ) as crawler:\n",
    "            results = crawler.crawl_complete(use_multithreading=CRAWL_CONFIG['use_multithreading'])\n",
    "            print(f\"üéâ Crawling completed! Results: {len(results)} keyword searches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ ACM Digital Library Crawler (v·ªõi x·ª≠ l√Ω cookie t·ª± ƒë·ªông)\n",
    "print(\"üöÄ Starting ACM Digital Library crawling...\")\n",
    "\n",
    "with ACMCrawler(\n",
    "    headless=CRAWL_CONFIG['headless'],\n",
    "    output_dir=CRAWLERS['acm']['output_dir'],\n",
    "    max_threads=CRAWL_CONFIG['max_threads'],\n",
    "    keyword_sets=keyword_sets\n",
    ") as crawler:\n",
    "    results = crawler.crawl_complete(use_multithreading=CRAWL_CONFIG['use_multithreading'])\n",
    "    print(f\"üéâ ACM Crawling completed! Results: {len(results)} keyword searches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5dbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ IEEE Xplore Crawler\n",
    "print(\"üöÄ Starting IEEE Xplore crawling...\")\n",
    "\n",
    "with IEEECrawler(\n",
    "    headless=CRAWL_CONFIG['headless'],\n",
    "    output_dir=CRAWLERS['ieee']['output_dir'],\n",
    "    max_threads=CRAWL_CONFIG['max_threads'],\n",
    "    keyword_sets=keyword_sets\n",
    ") as crawler:\n",
    "    results = crawler.crawl_complete(use_multithreading=CRAWL_CONFIG['use_multithreading'])\n",
    "    print(f\"üéâ IEEE Crawling completed! Results: {len(results)} keyword searches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63563a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Science Direct Crawler\n",
    "print(\"üöÄ Starting Science Direct crawling...\")\n",
    "\n",
    "with ScienceDirectCrawler(\n",
    "    headless=CRAWL_CONFIG['headless'],\n",
    "    output_dir=CRAWLERS['science_direct']['output_dir'],\n",
    "    max_threads=CRAWL_CONFIG['max_threads'],\n",
    "    keyword_sets=keyword_sets\n",
    ") as crawler:\n",
    "    results = crawler.crawl_complete(use_multithreading=CRAWL_CONFIG['use_multithreading'])\n",
    "    print(f\"üéâ Science Direct Crawling completed! Results: {len(results)} keyword searches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dadc37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Springer Crawler (v·ªõi x·ª≠ l√Ω cookie t·ª± ƒë·ªông)\n",
    "print(\"üöÄ Starting Springer crawling...\")\n",
    "\n",
    "with SpringerCrawler(\n",
    "    headless=CRAWL_CONFIG['headless'],\n",
    "    output_dir=CRAWLERS['springer']['output_dir'],\n",
    "    max_threads=CRAWL_CONFIG['max_threads'],\n",
    "    keyword_sets=keyword_sets\n",
    ") as crawler:\n",
    "    results = crawler.crawl_complete(use_multithreading=CRAWL_CONFIG['use_multithreading'])\n",
    "    print(f\"üéâ Springer Crawling completed! Results: {len(results)} keyword searches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Using KeywordFilter to filter papers by keywords\n",
    "print(\"üîé Starting keyword filtering...\")\n",
    "\n",
    "# Use our global configuration variables\n",
    "print(f\"Input Directory: {INPUT_DIR}\")\n",
    "print(f\"Output Directory: {FILTERED_OUTPUT_DIR}\")\n",
    "\n",
    "# Initialize the keyword filter with the configured directories\n",
    "keyword_filter = KeywordFilter(\n",
    "    input_dir=INPUT_DIR,\n",
    "    output_dir=FILTERED_OUTPUT_DIR\n",
    ")\n",
    "\n",
    "# Set similarity thresholds from our configuration\n",
    "keyword_filter.set_similarity_thresholds(\n",
    "    exact=SIMILARITY_THRESHOLDS[\"exact\"],\n",
    "    high=SIMILARITY_THRESHOLDS[\"high\"], \n",
    "    medium=SIMILARITY_THRESHOLDS[\"medium\"],\n",
    "    low=SIMILARITY_THRESHOLDS[\"low\"]\n",
    ")\n",
    "\n",
    "# Show the keywords we're using\n",
    "print(f\"Using {len(FILTER_KEYWORDS)} keywords for filtering:\")\n",
    "print(f\"- {', '.join(FILTER_KEYWORDS[:5])}... and {len(FILTER_KEYWORDS)-5} more\")\n",
    "\n",
    "# Run the filtering - this is the main function we're testing\n",
    "try:\n",
    "    # If the input directory doesn't exist or is empty, we'll simulate the output\n",
    "    if not os.path.exists(INPUT_DIR) or len(os.listdir(INPUT_DIR)) == 0:\n",
    "        print(f\"‚ö†Ô∏è Input directory {INPUT_DIR} doesn't exist or is empty. Using simulated data.\")\n",
    "        filtered_papers = []\n",
    "    else:\n",
    "        # Apply filtering with configurable minimum occurrences\n",
    "        filtered_papers = keyword_filter.filter_papers(\n",
    "            keywords=FILTER_KEYWORDS, \n",
    "            min_keyword_occurrences=1\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Filtered {len(filtered_papers)} papers containing relevant keywords\")\n",
    "    \n",
    "    # Also process all papers if the file exists\n",
    "    if os.path.exists(ALL_PAPERS_CSV):\n",
    "        print(f\"\\nüîç Running analysis on all papers dataset: {ALL_PAPERS_CSV}\")\n",
    "        \n",
    "        # Process the entire dataset\n",
    "        summaries = keyword_filter.run_pipeline(\n",
    "            input_csv=ALL_PAPERS_CSV,\n",
    "            output_dir=SUMMARY_DIR,\n",
    "            min_keyword_occurrences=1\n",
    "        )\n",
    "        \n",
    "        print(f\"üìä Complete analysis saved to {SUMMARY_DIR}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è All papers CSV not found at {ALL_PAPERS_CSV}\")\n",
    "        print(\"You can run the crawler first or manually place a CSV file there.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during filtering: {e}\")\n",
    "    # Use our fallback values\n",
    "    print(\"Using fallback filtered papers...\")\n",
    "    filtered_papers = filtered_papers_fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13661f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Starting paper download process...\n",
      "Download Directory: output\\filtered_arxiv\\downloaded_papers\n",
      "Using 21 filtered paper files from previous step\n",
      "Loaded 1500 papers from crawl_by_large_language_model.csv\n",
      "Loaded 38 papers from crawl_by_large_language_model_AND_Bug_Detection.csv\n",
      "Loaded 8 papers from crawl_by_large_language_model_AND_Functional_Testing.csv\n",
      "Loaded 7 papers from crawl_by_large_language_model_AND_Regression_Testing.csv\n",
      "Loaded 2 papers from crawl_by_large_language_model_AND_Software_Quality_Assurance_(SQA).csv\n",
      "Loaded 74 papers from crawl_by_large_language_model_AND_Software_Testing.csv\n",
      "Loaded 55 papers from crawl_by_large_language_model_AND_Test_Case_Generation.csv\n",
      "Loaded 28 papers from crawl_by_large_language_model_AND_Test_Coverage.csv\n",
      "Loaded 3 papers from crawl_by_large_language_model_AND_Test_Data_Generation.csv\n",
      "Loaded 1 papers from crawl_by_large_language_model_AND_Test_Maintenance.csv\n",
      "Loaded 350 papers from crawl_by_llm.csv\n",
      "Loaded 46 papers from crawl_by_llm_AND_Bug_Detection.csv\n",
      "Loaded 5 papers from crawl_by_llm_AND_Functional_Testing.csv\n",
      "Loaded 14 papers from crawl_by_llm_AND_Regression_Testing.csv\n",
      "Loaded 2 papers from crawl_by_llm_AND_Software_Quality_Assurance_(SQA).csv\n",
      "Loaded 78 papers from crawl_by_llm_AND_Software_Testing.csv\n",
      "Loaded 61 papers from crawl_by_llm_AND_Test_Case_Generation.csv\n",
      "Loaded 27 papers from crawl_by_llm_AND_Test_Coverage.csv\n",
      "Loaded 3 papers from crawl_by_llm_AND_Test_Data_Generation.csv\n",
      "Loaded 1 papers from crawl_by_llm_AND_Test_Maintenance.csv\n",
      "Loaded 2 papers from crawl_by_llm_AND_Test_Oracle_Problem.csv\n",
      "Filtered to 0 papers published after 2024-01-01\n",
      "‚ö†Ô∏è No papers to download after filtering\n",
      "\n",
      "Attempting to use summary file instead...\n",
      "Found summary file: output\\filtered_arxiv\\summary\\functional_testing_llm_true.csv\n",
      "Loaded 1764 papers from functional_testing_llm_true.csv\n",
      "Filtered to 0 papers published after 2024-01-01\n",
      "‚ö†Ô∏è No papers to download after filtering\n",
      "\n",
      "üìù Total downloaded papers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giang\\AppData\\Local\\Temp\\ipykernel_4728\\878636003.py:46: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  unique_df['submitted'] = pd.to_datetime(unique_df['submitted'], errors='coerce')\n",
      "C:\\Users\\giang\\AppData\\Local\\Temp\\ipykernel_4728\\878636003.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_df['submitted'] = pd.to_datetime(unique_df['submitted'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# üì• Downloading papers for further analysis\n",
    "print(\"üì• Starting paper download process...\")\n",
    "\n",
    "print(f\"Download Directory: {DOWNLOAD_DIR}\")\n",
    "\n",
    "# Initialize the paper downloader with the output directory\n",
    "downloader = PaperDownloader(output_dir=DOWNLOAD_DIR)\n",
    "\n",
    "# Get filtered papers from previous step or use fallback\n",
    "filtered_papers_to_use = get_var('filtered_papers', filtered_papers_fallback)\n",
    "\n",
    "def download_from_csv_files(csv_files):\n",
    "    \"\"\"Helper function to download papers from multiple CSV files\"\"\"\n",
    "    if not csv_files or len(csv_files) == 0:\n",
    "        print(\"‚ö†Ô∏è No CSV files provided for downloading\")\n",
    "        return []\n",
    "        \n",
    "    filtered_dfs = []\n",
    "    for paper_path in csv_files:\n",
    "        try:\n",
    "            if os.path.exists(paper_path):\n",
    "                paper_df = pd.read_csv(paper_path)\n",
    "                filtered_dfs.append(paper_df)\n",
    "                print(f\"Loaded {len(paper_df)} papers from {os.path.basename(paper_path)}\")\n",
    "            else:\n",
    "                print(f\"File not found: {paper_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {paper_path}: {e}\")\n",
    "    \n",
    "    downloaded_results = []\n",
    "    if filtered_dfs:\n",
    "        try:\n",
    "            # Combine all DataFrames\n",
    "            combined_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "            \n",
    "            # Remove duplicates based on pdf_link\n",
    "            if 'pdf_link' in combined_df.columns:\n",
    "                # Get papers with valid PDF links\n",
    "                valid_links_df = combined_df[combined_df['pdf_link'].notna()]\n",
    "                unique_df = valid_links_df.drop_duplicates(subset=['pdf_link'])\n",
    "                \n",
    "                if len(unique_df) > 0:\n",
    "                    # Apply date filtering if 'submitted' column exists\n",
    "                    if 'submitted' in unique_df.columns:\n",
    "                        try:\n",
    "                            unique_df['submitted'] = pd.to_datetime(unique_df['submitted'], errors='coerce')\n",
    "                            unique_df = unique_df.sort_values(by='submitted', ascending=False)\n",
    "                            unique_df = unique_df[unique_df['submitted'] >= pd.to_datetime(PAPERS_START_DATE)]\n",
    "                            print(f\"Filtered to {len(unique_df)} papers published after {PAPERS_START_DATE}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Date filtering error: {e}\")\n",
    "                \n",
    "                    # Limit to avoid too many downloads\n",
    "                    if len(unique_df) > MAX_PAPERS_TO_DOWNLOAD:\n",
    "                        print(f\"Limiting to {MAX_PAPERS_TO_DOWNLOAD} most recent papers\")\n",
    "                        unique_df = unique_df.head(MAX_PAPERS_TO_DOWNLOAD)\n",
    "                        \n",
    "                    # Download the papers\n",
    "                    if len(unique_df) > 0:\n",
    "                        print(f\"Starting download of {len(unique_df)} papers...\")\n",
    "                        downloaded_results = downloader.download_papers(unique_df, pdf_link_column='pdf_link')\n",
    "                        print(f\"üìö Downloaded {len(downloaded_results)} papers\")\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è No papers to download after filtering\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è No unique PDF links found\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No 'pdf_link' column found in papers\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing DataFrames: {e}\")\n",
    "    return downloaded_results\n",
    "\n",
    "try:\n",
    "    # First try to use filtered papers from previous step\n",
    "    if filtered_papers_to_use and len(filtered_papers_to_use) > 0:\n",
    "        print(f\"Using {len(filtered_papers_to_use)} filtered paper files from previous step\")\n",
    "        downloaded_papers = download_from_csv_files(filtered_papers_to_use)\n",
    "        \n",
    "        # If we didn't get any downloads, try the summary file\n",
    "        if len(downloaded_papers) == 0:\n",
    "            print(f\"\\nAttempting to use summary file instead...\")\n",
    "            if os.path.exists(SUMMARY_FILE):\n",
    "                print(f\"Found summary file: {SUMMARY_FILE}\")\n",
    "                downloaded_papers = download_from_csv_files([SUMMARY_FILE])\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Summary file not found at: {SUMMARY_FILE}\")\n",
    "    else:\n",
    "        print(\"No filtered papers available from previous step\")\n",
    "        \n",
    "        # Try using the summary file if it exists\n",
    "        if os.path.exists(SUMMARY_FILE):\n",
    "            print(f\"Using summary file: {SUMMARY_FILE}\")\n",
    "            downloaded_papers = download_from_csv_files([SUMMARY_FILE])\n",
    "        # Otherwise try using any CSV files in the input directory\n",
    "        elif os.path.exists(INPUT_DIR):\n",
    "            csv_files = [os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR) \n",
    "                        if f.endswith('.csv') and 'crawl_by_' in f]\n",
    "            if csv_files:\n",
    "                print(f\"Using {len(csv_files)} CSV files found in {INPUT_DIR}\")\n",
    "                downloaded_papers = download_from_csv_files(csv_files)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No suitable CSV files found in {INPUT_DIR}\")\n",
    "                downloaded_papers = []\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No input sources available. Please run keyword filtering first or manually place CSV files.\")\n",
    "            downloaded_papers = []\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during paper download: {e}\")\n",
    "    downloaded_papers = []\n",
    "\n",
    "# Save the variable for use in the next cell\n",
    "print(f\"\\nüìù Total downloaded papers: {len(downloaded_papers)}\")\n",
    "if len(downloaded_papers) > 0:\n",
    "    # Check if any PDFs were actually saved to disk\n",
    "    pdf_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.lower().endswith('.pdf')] if os.path.exists(DOWNLOAD_DIR) else []\n",
    "    print(f\"üìÅ PDF files in download directory: {len(pdf_files)}\")\n",
    "    \n",
    "    # If no PDFs are in the directory, we need to create a dummy example for testing\n",
    "    if len(pdf_files) == 0 and len(downloaded_papers) > 0:\n",
    "        print(\"‚ö†Ô∏è No PDF files found in download directory despite successful downloads.\")\n",
    "        print(\"This might be due to download errors or permission issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d646cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Starting content analysis...\n",
      "Analysis Directory: output\\filtered_arxiv\\analysis\n",
      "‚ùå Error during content analysis: cannot access local variable 'os' where it is not associated with a value\n"
     ]
    }
   ],
   "source": [
    "# üìä Analyzing paper content\n",
    "print(\"üî¨ Starting content analysis...\")\n",
    "print(f\"Analysis Directory: {ANALYSIS_DIR}\")\n",
    "\n",
    "# Get reference to downloaded papers or create empty list for testing\n",
    "downloaded_papers_to_use = get_var('downloaded_papers', [])\n",
    "\n",
    "def analyze_pdf_directory(pdf_dir, output_dir, use_api=True):\n",
    "    \"\"\"Helper function to analyze PDFs in a directory with fallback options\"\"\"\n",
    "    results = {'analyzed_files': [], 'analysis_data': []}\n",
    "    \n",
    "    # Check if directory exists and contains PDF files\n",
    "    if not os.path.exists(pdf_dir):\n",
    "        print(f\"‚ö†Ô∏è PDF directory not found: {pdf_dir}\")\n",
    "        return results\n",
    "        \n",
    "    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        print(f\"‚ö†Ô∏è No PDF files found in {pdf_dir}\")\n",
    "        return results\n",
    "        \n",
    "    print(f\"Found {len(pdf_files)} PDF files to analyze\")\n",
    "    \n",
    "    # Check if we should try to use the Gemini API\n",
    "    if use_api:\n",
    "        try:\n",
    "            # Check if the google.genai module is available\n",
    "            import importlib.util\n",
    "            genai_spec = importlib.util.find_spec(\"google.genai\")\n",
    "            has_genai = genai_spec is not None\n",
    "            \n",
    "            if has_genai:\n",
    "                # Try to get API key from config or environment\n",
    "                api_keys_to_use = API_KEYS\n",
    "                if not api_keys_to_use:\n",
    "                    import os\n",
    "                    env_key = os.environ.get('GOOGLE_API_KEY')\n",
    "                    if env_key:\n",
    "                        api_keys_to_use = [env_key]\n",
    "                        print(\"Using API key from GOOGLE_API_KEY environment variable\")\n",
    "                \n",
    "                if api_keys_to_use:\n",
    "                    # Initialize analyzer and process files\n",
    "                    print(\"Using Gemini API for content analysis\")\n",
    "                    analyzer = ContentAnalyzer(api_keys=api_keys_to_use)\n",
    "                    results['analyzed_files'] = analyzer.process_directory(pdf_dir)\n",
    "                    \n",
    "                    # If successful, extract data from the JSON files\n",
    "                    if results['analyzed_files']:\n",
    "                        print(f\"‚úÖ Successfully analyzed {len(results['analyzed_files'])} papers with Gemini API\")\n",
    "                        \n",
    "                        for json_file in results['analyzed_files']:\n",
    "                            try:\n",
    "                                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                                    data = json.load(f)\n",
    "                                    paper_info = {\n",
    "                                        'title': data.get('paper_identification', {}).get('title', 'Unknown'),\n",
    "                                        'authors': data.get('paper_identification', {}).get('authors', 'Unknown'),\n",
    "                                        'publication': data.get('paper_identification', {}).get('publication_venue_year', 'Unknown'),\n",
    "                                        'problem': data.get('abstract_analysis', {}).get('problem_statement', {}).get('analysis', ''),\n",
    "                                        'methodology': data.get('methodology_analysis', {}).get('ai_techniques_used', {}).get('analysis', ''),\n",
    "                                        'results': data.get('results_analysis', {}).get('quantitative_results', {}).get('analysis', '')\n",
    "                                    }\n",
    "                                    results['analysis_data'].append(paper_info)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {json_file}: {e}\")\n",
    "                        \n",
    "                        return results\n",
    "            \n",
    "            print(\"Gemini API not available or not configured. Falling back to basic extraction.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with Gemini API: {e}\")\n",
    "            print(\"Falling back to basic extraction\")\n",
    "    \n",
    "    # Basic extraction fallback\n",
    "    try:\n",
    "        import PyPDF2\n",
    "        print(f\"Performing basic extraction from {len(pdf_files)} PDF files\")\n",
    "        \n",
    "        for pdf_file in pdf_files:\n",
    "            pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "            try:\n",
    "                with open(pdf_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    \n",
    "                    # Get text from first page\n",
    "                    text = \"\"\n",
    "                    if reader.pages:\n",
    "                        text = reader.pages[0].extract_text() or \"\"\n",
    "                    \n",
    "                    # Try to extract title and other info from text\n",
    "                    lines = text.split('\\n')\n",
    "                    probable_title = lines[0] if lines else pdf_file\n",
    "                    \n",
    "                    file_info = {\n",
    "                        'title': probable_title[:100],\n",
    "                        'filename': pdf_file,\n",
    "                        'authors': ' '.join(lines[1:3]) if len(lines) > 1 else 'Unknown',\n",
    "                        'publication': 'Unknown',\n",
    "                        'pages': len(reader.pages),\n",
    "                        'problem': text[:200] + \"...\" if len(text) > 200 else text,\n",
    "                        'methodology': 'Basic extraction only',\n",
    "                        'results': 'Basic extraction only'\n",
    "                    }\n",
    "                    results['analysis_data'].append(file_info)\n",
    "                    # Save the extraction path\n",
    "                    results['analyzed_files'].append(pdf_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting from {pdf_file}: {e}\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è PyPDF2 module not found. Cannot perform basic extraction.\")\n",
    "        print(\"To install: pip install PyPDF2\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "try:\n",
    "    # First check if we have actual downloaded papers from the previous step\n",
    "    if downloaded_papers_to_use and len(downloaded_papers_to_use) > 0:\n",
    "        print(f\"Using {len(downloaded_papers_to_use)} downloaded papers from previous step\")\n",
    "    \n",
    "    # Try to analyze PDFs in the download directory\n",
    "    if os.path.exists(DOWNLOAD_DIR):\n",
    "        analysis_results = analyze_pdf_directory(DOWNLOAD_DIR, ANALYSIS_DIR)\n",
    "        \n",
    "        # Create summary dataframe if we have data\n",
    "        if analysis_results['analysis_data']:\n",
    "            analysis_data = analysis_results['analysis_data']\n",
    "            summary_df = pd.DataFrame(analysis_data)\n",
    "            print(f\"\\nüìà Analysis Summary:\")\n",
    "            print(f\"- Total papers analyzed: {len(summary_df)}\")\n",
    "            \n",
    "            # Save analysis results\n",
    "            summary_csv = os.path.join(ANALYSIS_DIR, \"analysis_summary.csv\")\n",
    "            summary_df.to_csv(summary_csv, index=False)\n",
    "            print(f\"üíæ Analysis summary saved to {summary_csv}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No analysis data was collected\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Download directory not found: {DOWNLOAD_DIR}\")\n",
    "        \n",
    "        # Try to create test data for demonstration\n",
    "        print(\"\\nüîç Creating demo analysis data for testing...\")\n",
    "        demo_data = [\n",
    "            {\n",
    "                'title': 'Example Paper 1: Using LLMs for Test Generation',\n",
    "                'authors': 'Jane Smith, John Doe',\n",
    "                'publication': 'Conference on AI Testing, 2023',\n",
    "                'problem': 'This paper addresses the challenge of automating test case generation using LLMs',\n",
    "                'methodology': 'The authors use a fine-tuned GPT model with prompt engineering techniques',\n",
    "                'results': 'The approach achieved 85% accuracy in generating valid test cases'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Example Paper 2: AI-based Test Oracle Generation',\n",
    "                'authors': 'Alex Johnson, Maria Garcia',\n",
    "                'publication': 'Journal of Software Testing, 2022',\n",
    "                'problem': 'Creating reliable test oracles remains challenging for complex software systems',\n",
    "                'methodology': 'The paper proposes a novel chain-of-thought approach for test oracle generation',\n",
    "                'results': 'The proposed method reduced false positives by 40% compared to baseline approaches'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Save demo data\n",
    "        demo_df = pd.DataFrame(demo_data)\n",
    "        demo_csv = os.path.join(ANALYSIS_DIR, \"demo_analysis.csv\")\n",
    "        demo_df.to_csv(demo_csv, index=False)\n",
    "        print(f\"üíæ Demo analysis data saved to {demo_csv}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during content analysis: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a348e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
